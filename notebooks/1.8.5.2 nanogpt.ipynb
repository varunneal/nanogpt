{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f894d5-2b23-46f6-8622-90ed61ff381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 1)) (2.1.2)\n",
      "Collecting tqdm (from -r ../requirements.txt (line 2))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 3)) (2.7.0+cu126)\n",
      "Collecting huggingface-hub (from -r ../requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch->-r ../requirements.txt (line 3)) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (2.32.4)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub->-r ../requirements.txt (line 4))\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->-r ../requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r ../requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (2025.8.3)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m203.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, hf-xet, huggingface-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [huggingface-hub] [huggingface-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.8 huggingface-hub-0.34.4 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu126\n",
      "Collecting torch==2.9.0.dev20250713+cu126\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.9.0.dev20250713%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.80)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.5.4.2)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.9 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/nvidia_nvshmem_cu12-3.3.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (1.11.1.6)\n",
      "Collecting pytorch-triton==3.4.0+gitae848267 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.4.0%2Bgitae848267-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.4.0+gitae848267->torch==2.9.0.dev20250713+cu126) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch==2.9.0.dev20250713+cu126) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.9.0.dev20250713+cu126) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.9.0.dev20250713%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl (822.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m822.6/822.6 MB\u001b[0m \u001b[31m157.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/nvidia_nvshmem_cu12-3.3.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 MB\u001b[0m \u001b[31m267.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.4.0%2Bgitae848267-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (154.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 MB\u001b[0m \u001b[31m201.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nvshmem-cu12, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0+cu126━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.0+cu126:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.0+cu1260m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torch]32m5/6\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.0+cu126 requires torch==2.7.0, but you have torch 2.9.0.dev20250713+cu126 which is incompatible.\n",
      "torchvision 0.22.0+cu126 requires torch==2.7.0, but you have torch 2.9.0.dev20250713+cu126 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.10.2.21 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvshmem-cu12-3.3.9 pytorch-triton-3.4.0+gitae848267 torch-2.9.0.dev20250713+cu126\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mName: torch\n",
      "Version: 2.9.0.dev20250713+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, pytorch-triton, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n",
      "fineweb_val_000000.bin: 100%|█████████████████| 200M/200M [00:00<00:00, 236MB/s]\n",
      "fineweb_train_000001.bin: 100%|███████████████| 200M/200M [00:00<00:00, 223MB/s]\n",
      "fineweb_train_000002.bin: 100%|███████████████| 200M/200M [00:00<00:00, 227MB/s]\n",
      "fineweb_train_000003.bin: 100%|███████████████| 200M/200M [00:00<00:00, 313MB/s]\n",
      "fineweb_train_000004.bin: 100%|███████████████| 200M/200M [00:00<00:00, 239MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip && pip install -r ../requirements.txt\n",
    "!pip install --pre \"torch==2.9.0.dev20250713+cu126\" --index-url https://download.pytorch.org/whl/nightly/cu126\n",
    "!pip show torch\n",
    "!pip install pytz\n",
    "!python ../data/cached_fineweb10B.py 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88f43-04ff-4317-9dbc-b7ae33cc69a2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62975aa6-f3b7-489a-9131-ea69089074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f91470-f6e0-431d-a490-23c80f3c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a879e98-a216-4626-a4ea-8db22d99df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432bad-bbd3-4853-9a8d-2b11d8feda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969746f-cc86-4121-a01e-19868c2a6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c63379b-7f98-4b29-aa53-31af47bc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f8de9-ae0d-4835-8897-b8def61c248b",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51675374-ca07-4849-89a2-f0cf8b130c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "timestamp = datetime.now(eastern).strftime(\"%H:%M-%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc86ec8-99e5-4389-96ed-3a6101591613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/03:09-2025-08-25.txt\n"
     ]
    }
   ],
   "source": [
    "# begin logging\n",
    "logfile = None\n",
    "\n",
    "run_id = os.environ.get(\"NB_BASE\",uuid.uuid4())\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logfile = f\"logs/{timestamp}.txt\"\n",
    "print(logfile)\n",
    "\n",
    "\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a587fb-7ff8-46de-82cd-3c98b8ea96ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]',\n",
       " '2.9.0.dev20250713+cu126')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version, torch.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ccb5-4d25-4918-aeb5-1b5a1f44a0fe",
   "metadata": {},
   "source": [
    "## Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1dd716-9d1b-4086-ae9c-3cf032ec04a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.compile ## ns\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f0f206-d925-4920-8f53-681265a44628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # Efficient systems-wise implementation of step developed by @YouJiacheng,\n",
    "        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,\n",
    "        # @ryanyang0, and @vagrawal.\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                if base_i + rank < len(params):\n",
    "                    grad = params[base_i + rank].grad\n",
    "                # This gives strange dynamo warnings\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * world_size\n",
    "            momentum = group[\"momentum\"]\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                if base_i + rank < len(params):\n",
    "                    p = params[base_i + rank]\n",
    "                    grad = p.grad\n",
    "                    eff_lr = group[\"lr\"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, \"lr_mul\", 1.0)\n",
    "                    eff_weight_decay = group[\"lr\"] * group[\"weight_decay\"] * getattr(p, \"wd_mul\", 1.0)\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
    "                    momentum_buffer = state[\"momentum_buffer\"]\n",
    "                    p.mul_(1 - eff_weight_decay)\n",
    "                    momentum_buffer.lerp_(grad, 1 - momentum)\n",
    "                    grad = grad.lerp_(momentum_buffer, momentum)\n",
    "                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)\n",
    "                    p.add_(other=v, alpha=-eff_lr)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbc0f81-97ec-4ed7-99df-78f34bada062",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DistAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "        # DistributedAdam implementation by @vagrawal\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78ffb1-753b-4a6f-a57f-81781873c084",
   "metadata": {},
   "source": [
    "## Custom Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e2360d-f65a-451b-89c9-9020d252e665",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x.new_tensor(x_s, dtype=torch.float32),\n",
    "            scale_b=x.new_tensor(w_s, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)\n",
    "        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)\n",
    "        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)\n",
    "        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=grad_inv_s,\n",
    "            scale_b=w_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=x_inv_s,\n",
    "            scale_b=grad_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "        return grad_x, grad_w\n",
    "\n",
    "    return impl(g, x_f8, w_f8)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)\n",
    "\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    return grad_x, grad_w, None, None, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c129e66-be79-4122-857d-fe205176d0c6",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ec017e-065d-4f43-b092-9163c3aaf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43c37-4d2a-4fd6-bd58-63fcd1b5a54a",
   "metadata": {},
   "source": [
    "### CastedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4934dccb-7f32-442d-a7b8-4cc8d666d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastedLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.use_fp8 = use_fp8\n",
    "        self.x_s = x_s\n",
    "        self.w_s = w_s\n",
    "        self.grad_s = grad_s\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        assert x.size(-1) == self.in_features\n",
    "        if self.use_fp8 and self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65922453-7b13-425f-a888-899f53d243ea",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c79cc0a-7a69-4d76-88a0-906ee65da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim // 4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim // 4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a536ae-14b2-4b5c-b0ee-f2ca0ad7045a",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83a21056-70ee-4b77-a2b8-8c33f427bbb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        self.attn_scale = 0.12\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):\n",
    "        B, T = x.size(0), x.size(1)  # batch size, sequence length\n",
    "\n",
    "        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads,\n",
    "                                                                             self.head_dim).chunk(3, dim=-2)\n",
    "        q, k = norm(q), norm(k)  # QK norm @Grad62304977\n",
    "        q, k = self.rotary(q), self.rotary(k)\n",
    "        if ve is not None:\n",
    "            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)  # @KoszarskyB & @Grad62304977\n",
    "        else:  # skip mid-layers token value embeddings by @YouJiacheng\n",
    "            v = lambdas[0] * v\n",
    "        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask,\n",
    "                           scale=self.attn_scale).transpose(1, 2)\n",
    "        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6828-b90a-47c6-9759-73bbab68521c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992d3a44-eca9-4645-b9b9-b678cd76fa19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = CastedLinear(dim, hdim)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(\n",
    "            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa25d-96b7-4026-904d-e9b7f6e7ce7a",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c18407-3a67-4e22-9f5f-7717fb3cffcf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor,\n",
    "                block_mask: BlockMask):\n",
    "\n",
    "        x = lambdas[0] * x + lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db37f6-e22c-4a8c-9d02-8283ba4961b7",
   "metadata": {},
   "source": [
    "### Create blockmasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77098330-ee81-44a3-ae1f-947a4f04060c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_blockmasks(seq_len: int,\n",
    "                      sliding_window_num_blocks: int,\n",
    "                      BLOCK_SIZE: int = 128,\n",
    "                      device: torch.device = \"cuda\") -> BlockMask:\n",
    "    assert seq_len % BLOCK_SIZE == 0, \"seq_len must be a multiple of BLOCK_SIZE\"\n",
    "\n",
    "    Q = seq_len // BLOCK_SIZE\n",
    "    i32 = torch.int32\n",
    "\n",
    "    # Block coordinates\n",
    "    q = torch.arange(Q, device=device, dtype=i32).view(-1, 1)  # [Q,1]\n",
    "    k = torch.arange(Q, device=device, dtype=i32).view(1, -1)  # [1,K]\n",
    "\n",
    "    \n",
    "    # Partial (current block): counts=1, indices first column = q\n",
    "    partial_idx = torch.zeros((Q, Q), dtype=i32, device=device)   # [Q,K]\n",
    "    partial_idx[:, 0] = q.squeeze(1)\n",
    "    partial_cnt = torch.ones((Q,), dtype=i32, device=device)      # [Q]\n",
    "\n",
    "    # Full (previous blocks), nearest-first prefix: [q-1, q-2, ..., 0]\n",
    "    full_idx = (q - 1 - k) % Q                                    # [Q,K]\n",
    "    full_cnt = torch.arange(Q, dtype=i32, device=device)          # [Q] = number of previous blocks\n",
    "\n",
    "    # Add (B,H) broadcast dims\n",
    "    partial_kv_indices    = partial_idx[None, None].contiguous()  # [1,1,Q,K]\n",
    "    full_kv_indices       = full_idx[None, None].contiguous()     # [1,1,Q,K]\n",
    "    partial_kv_num_blocks = partial_cnt[None, None].contiguous()  # [1,1,Q]\n",
    "    full_kv_num_blocks    = full_cnt[None, None].contiguous()     # [1,1,Q]\n",
    "\n",
    "    # Token-level causal within the current (partial) block\n",
    "    def causal_mask_mod(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "\n",
    "    # Windowing (ints → tensor ops via broadcasting)\n",
    "    W = max(sliding_window_num_blocks, 1)\n",
    "    max_full = max(W - 1, 0)                                      # int\n",
    "    full_num = torch.clamp_max(full_kv_num_blocks, max_full)      # [1,1,Q]\n",
    "    remain_for_partial = torch.clamp_min(W - full_num, 1)         # [1,1,Q]\n",
    "    partial_num = torch.minimum(partial_kv_num_blocks, remain_for_partial)\n",
    "\n",
    "    return BlockMask.from_kv_blocks(\n",
    "        partial_num, partial_kv_indices,\n",
    "        full_num,    full_kv_indices,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        mask_mod=causal_mask_mod,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0c5feb-2400-4ece-9bfd-1435ca4eb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "@lru_cache(1)\n",
    "def get_blockmask(seq_len, n_windows):\n",
    "    if (seq_len, n_windows) not in masks:\n",
    "        bm = create_blockmasks(seq_len, n_windows)\n",
    "        masks[(seq_len, n_windows)] = bm\n",
    "    return masks[(seq_len, n_windows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e0737-3fe1-49c5-af57-54d677f9b5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5beb6881-789d-45e3-bc87-f9fad2f58cfd",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9eacdf-a884-417f-a89d-158d72ac8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "\n",
    "        # self.lm_head = CastedLinear(model_dim, vocab_size)\n",
    "        self.lm_head = CastedLinear(model_dim, \n",
    "                                    vocab_size, \n",
    "                                    use_fp8=True, \n",
    "                                    x_s=(model_dim ** 0.5) / 448, \n",
    "                                    w_s=24 / 448,\n",
    "                                    grad_s=1 / 448\n",
    "                                   )\n",
    "        self.lm_head.weight.detach().zero_()  # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        pad = (-num_layers * 5) % dist.get_world_size()\n",
    "        self.scalars = nn.Parameter(torch.cat([\n",
    "            torch.ones(num_layers),  # skip_weights\n",
    "            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],  # block lambdas\n",
    "            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],  # SA lambdas\n",
    "            torch.ones(pad),\n",
    "        ]))\n",
    "        # set learning rates\n",
    "        for param in self.embed.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        for param in self.value_embeds.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        self.lm_head.weight.lr_mul = 27.5\n",
    "        self.scalars.lr_mul = 5.0\n",
    "\n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor, long_bm: BlockMask, short_bm: BlockMask):\n",
    "        assert input_seq.ndim == 2\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm,\n",
    "                       short_bm, long_bm]\n",
    "        assert len(block_masks) == len(self.blocks)\n",
    "        \n",
    "        x = x0 = norm(self.embed(input_seq))  # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        skip_weights = self.scalars[:(len(self.blocks) // 2)]\n",
    "        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)\n",
    "        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)\n",
    "\n",
    "        n = len(self.blocks) // 2\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "\n",
    "        logits = self.lm_head(x).float()\n",
    "\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1) ** 0.5))\n",
    "        # loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq,\n",
    "        #                        reduction=\"sum\" if self.training else \"mean\")\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq.view(-1),\n",
    "                       reduction=\"sum\" if self.training else \"mean\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18b470-51e6-466a-a90b-20606dcac5dc",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1f0904-4288-4559-b8ca-20f6084d5bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "024a9576-adb5-4d3d-a03c-b045d2984846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EOSBatchFinder:\n",
    "    def __init__(self, tokens: Tensor, world_size: int = 1, eos_id: int = 50256):\n",
    "        # Precompute EOS positions once per shard \n",
    "        self.eos_idx = (tokens == eos_id).nonzero(as_tuple=True)[0].to(torch.int64).cpu()\n",
    "        self.i = 0      # pointer into eos_idx (start EOS for next step)\n",
    "        self.pos = 0    # logical stream position within this shard\n",
    "        self.world_size = world_size\n",
    "\n",
    "    def seek(self, pos: int):\n",
    "        # Set pointer to the first EOS >= pos\n",
    "        self.i = int(torch.searchsorted(self.eos_idx, pos))\n",
    "        if self.i >= int(self.eos_idx.numel()):\n",
    "            raise StopIteration(\"Seek past last EOS.\")\n",
    "        self.pos = pos\n",
    "\n",
    "    def next_batch(self, batch_size_local: int, seq_len: int):\n",
    "        n = int(self.eos_idx.numel())\n",
    "        if self.i >= n:\n",
    "            raise StopIteration(\"No more EOS in this shard.\")\n",
    "\n",
    "        starts = [[] for _ in range(self.world_size)]\n",
    "        idx = self.i\n",
    "        cur = int(self.eos_idx[idx])  # EOS that ends the \"previous\" document; next doc starts at cur+1\n",
    "\n",
    "        for r in range(self.world_size):\n",
    "            for _ in range(batch_size_local):\n",
    "                start = cur + 1                            \n",
    "                target = start + seq_len                      # need seq_len tokens before next EOS\n",
    "                j = int(torch.searchsorted(self.eos_idx, target))\n",
    "                if j >= n:\n",
    "                    raise StopIteration(\"Insufficient EOS ahead; hit tail of shard.\")\n",
    "                starts[r].append(start)\n",
    "                idx = j\n",
    "                cur = int(self.eos_idx[idx])                  # next seq must also start at a new doc\n",
    "\n",
    "        advance = int(self.eos_idx[idx] - self.pos)           # move stream to the last end\n",
    "        self.pos += advance\n",
    "        self.i = idx\n",
    "        return starts, advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f671057-f8ff-4edf-bfc7-7ee221b11a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_data_generator(filename_pattern: str, batch_size: int, seq_len: int, align_to_bos: bool = True):\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    assert batch_size % world_size == 0 \n",
    "\n",
    "    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]\n",
    "\n",
    "    batch_size_local = (batch_size // world_size)\n",
    "    num_tokens_local = batch_size_local * seq_len\n",
    "    \n",
    "    num_tokens_global = batch_size * seq_len\n",
    "    \n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "\n",
    "    finder = EOSBatchFinder(tokens, world_size=world_size) if align_to_bos else None\n",
    "    if align_to_bos:\n",
    "        finder.seek(pos)\n",
    "\n",
    "    batch_span = num_tokens_global\n",
    "    _inputs, _targets = None, None\n",
    "    while True:\n",
    "        if pos + num_tokens_global + 1 >= len(tokens):\n",
    "            # Need to load new shard\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "            if align_to_bos:\n",
    "                finder = EOSBatchFinder(tokens, world_size=world_size)\n",
    "                finder.seek(pos)\n",
    "        if align_to_bos:\n",
    "            try:\n",
    "                batch_starts, batch_span = finder.next_batch(batch_size_local, seq_len)\n",
    "                start_idxs = batch_starts[rank]\n",
    "            except StopIteration:\n",
    "                # move to next shard on next loop\n",
    "                pos = len(tokens) + 1\n",
    "                continue\n",
    "\n",
    "            bufs = [tokens[s : s + seq_len + 1] for s in start_idxs]\n",
    "            buf = torch.stack(bufs, dim=0)  # [B_local, seq_len + 1]\n",
    "\n",
    "            _inputs  = buf[:, :-1]\n",
    "            _targets = buf[:,  1:]\n",
    "        else:\n",
    "            batch_span = num_tokens_global\n",
    "            start_idx = pos + rank * batch_size_local\n",
    "            buf = tokens[start_idx:][:num_tokens_local + 1]\n",
    "            _inputs = buf[:-1].view(batch_size_local, seq_len)\n",
    "            _targets = buf[1:].view(batch_size_local, seq_len)\n",
    "\n",
    "        pos += batch_span\n",
    "        yield (_inputs.to(device=\"cuda\", dtype=torch.int32, non_blocking=True), \n",
    "               _targets.to(device=\"cuda\", dtype=torch.int64,  non_blocking=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6ff7-2376-4818-b0c6-63ea5c91a4c6",
   "metadata": {},
   "source": [
    "## dist init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed75c35b-54c5-4816-b1ef-10f4da165b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['LOCAL_RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['GROUP_RANK'] = '0'\n",
    "os.environ['ROLE_RANK'] = '0'\n",
    "os.environ['ROLE_NAME'] = 'default'\n",
    "os.environ['ROLE_WORLD_SIZE'] = '1'\n",
    "os.environ['TORCHELASTIC_RESTART_COUNT'] = '0'\n",
    "os.environ['TORCHELASTIC_MAX_RESTARTS'] = '0'\n",
    "os.environ['TORCHELASTIC_RUN_ID'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666f8fcd-5586-41af-a170-3fc6948e173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "master_process = (rank == 0)  # this process will do logging, checkpointing etc.\n",
    "# assert world_size == 8  # this code is designed for 8xH100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "931ae567-26bd-4964-b2d0-6869410a1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79666103-36de-44e3-b320-fa5a077e8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc64ee3-9295-4345-b4a3-5cc483383749",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f33bd48e-caee-4ead-9786-c1411e1bdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f125ece-109d-43f6-8bcf-aa3e73c1ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially refactor code to take in minibatch sizes (e.g. per world sizes) instead\n",
    "# currently batch size gets distributed across all GPUs -- in this case, just 1\n",
    "# at current setup, when we scale to 8 gpus, we want to multiply batch size by 8 to maintain minibatch size\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files: str = \"../data/fineweb10B/fineweb_train_*.bin\"  # input .bin to train on\n",
    "    val_files: str = \"../data/fineweb10B/fineweb_val_*.bin\"  # input .bin to eval validation loss on\n",
    "    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    \n",
    "    # train_seq_len = 1024 * 6\n",
    "    # train_batch_size = 8 * world_size\n",
    "\n",
    "    # val_seq_len = 1024 * 5  # FlexAttention sequence length for validation\n",
    "    # val_batch_size = 4 * world_size\n",
    "    \n",
    "    # optimization\n",
    "    num_iterations = 2000  # number of iterations to run\n",
    "    cooldown_frac = 0.45  # fraction of training spent cooling down the learning rate\n",
    "    \n",
    "    # evaluation and logging\n",
    "    val_loss_every = 100  # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "\n",
    "    vocab_size = next_multiple_of_n(50257, n=128)\n",
    "    num_layers = 12\n",
    "    num_heads = 6\n",
    "    model_dim = 768\n",
    "\n",
    "    block_size = 128\n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0e0d651-85d1-41d7-a3bb-a08a24b5fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq_len, val_batch_size = 1024 * 5, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045b81c-2a0a-4002-8eae-8a7bdf76b6d2",
   "metadata": {},
   "source": [
    "### schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8ee4051-b2fd-463b-b76c-1780ad214171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c678927-eec9-4052-93a2-366025b34d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_piecewise(values, breaks):\n",
    "    assert len(values) == len(breaks) + 1, \"values must be one longer than breaks\"\n",
    "    \n",
    "    def f(t: int) -> int:\n",
    "        i = bisect_right(breaks, t)\n",
    "        return values[i]\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66b9926f-dc74-4499-9e29-bb6f8abe26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_len1, train_batch_size1, lr1 = 1024 * 6, 8, 1\n",
    "# train_seq_len2, train_batch_size2, lr2 = 128 * 6, 32, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1496e86e-f00a-4952-8f27-8dbe12ff2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = make_piecewise([\n",
    "    {\n",
    "        'dense': 4, 'sparse': 2,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 2,\n",
    "        'lr_mult': 0.707,\n",
    "    },\n",
    "    {\n",
    "        'dense': 8, 'sparse': 2,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 4,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "        {\n",
    "        'dense': 8, 'sparse': 4,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "        {\n",
    "        'dense': 8, 'sparse': 4,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "], breaks = [324, 768, 1152])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccf1ea-1424-41e6-8b39-7e66c3747a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee6ec065-dd1d-4d54-a1f1-06e7d3a6e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_schedule = [2, 2, 12, 12]\n",
    "sparse_schedule = [1, 1, 2, 2]\n",
    "\n",
    "dense_bm_schedule = make_piecewise(\n",
    "    values = dense_schedule,\n",
    "    breaks = [324, 768, 1152]\n",
    ")\n",
    "sparse_bm_schedule = make_piecewise(\n",
    "    values = sparse_schedule,\n",
    "    breaks = [324, 768, 1152]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c819b4e-a1f7-4663-aede-9f9900fb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    lr = 1.0\n",
    "    if x >= 1 - args.cooldown_frac:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        lr = w * 1.0 + (1 - w) * 0.1\n",
    "    # if step < 768:\n",
    "    #     lr *= lr1\n",
    "    # else:\n",
    "    #     lr *= lr2\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78b8a5c0-780a-4571-986a-4436f09b814f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6144, 5120)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq_len1, val_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb0b9b-de7f-4052-a2fe-e6f9a69fc930",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "253fdb42-b56c-4a3b-bb0c-7ac5837d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = GPT(vocab_size=args.vocab_size, \n",
    "                       num_layers=args.num_layers, \n",
    "                       num_heads=args.num_heads, \n",
    "                       model_dim=args.model_dim,\n",
    "                       max_seq_len=max(train_seq_len1, val_seq_len),\n",
    "                      ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c4760a4-1858-499b-b961-530cd04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a03b657a-b05f-4aa5-bf53-d07702385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12334b9f-193f-446b-a3e8-00a1479bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10,\n",
    "                      weight_decay=0.0)\n",
    "optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2672d18a-aad3-4706-822b-804d9ca90ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = torch.compile(model, dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809242-8dca-4f72-bc28-736aa58fc03c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e779f1-8889-4a9b-afcc-6a3172237e49",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bb87203-fcaa-4f27-a53a-b75a20d383f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch._dynamo import reset\n",
    "from torch._dynamo.utils import counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fc40a3d-7ad0-4a8c-a035-e14357268cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 100\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "908e97d0-4c3e-47f7-8c09-a04c0ba16694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = distributed_data_generator(args.train_files, 8, 1024*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83caec-1278-405b-a221-c5ca9722da07",
   "metadata": {},
   "source": [
    " 878ms wall time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bea800c-d838-4fa2-a4dd-aecd524dff13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e05d5e40e5a4fff96a603ffe8beebf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: This API is going to be deprecated, please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(warmup_steps)):\n",
    "    inputs, targets = next(train_loader)\n",
    "    long_bm = get_blockmask(1024*6, dense_schedule[step % 4])\n",
    "    short_bm = get_blockmask(1024*6, sparse_schedule[step % 4])\n",
    "    model(inputs, targets, long_bm, short_bm).backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bf4a142-a25f-4ebf-833d-2273d7dfc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(initial_state[\"model\"])\n",
    "for opt, opt_state in zip(optimizers, initial_state[\"optimizers\"]):\n",
    "    opt.load_state_dict(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00310994-df28-4324-97f3-37b17006dfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del train_loader, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27d96a-9085-417b-b8e3-479220a3f2e1",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b5bbf88-2f90-44ce-9203-74bae32b5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "train_seq_len = 1024 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc187144-6bec-4bca-aa0e-9405bf1693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = distributed_data_generator(args.train_files, train_batch_size, train_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5496e90d-9c4e-43ce-92f6-b49362e624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = args.num_iterations\n",
    "training_time_ms, tokens_processed = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40533ba-cc8e-40aa-a2c9-2730c0c0d16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---validation---\n",
      "step:0/2000 val_loss:10.8258 train_time:0ms step_avg:0.36ms\n",
      "---end of validation---\n",
      "First inputs retrieved torch.Size([8, 6144])\n",
      "step:1/2000 train_time:199ms step_avg:198.88ms tokens_processed:49152\n",
      "step:2/2000 train_time:336ms step_avg:168.16ms tokens_processed:98304\n",
      "step:3/2000 train_time:484ms step_avg:161.44ms tokens_processed:147456\n",
      "step:4/2000 train_time:635ms step_avg:158.86ms tokens_processed:196608\n",
      "step:5/2000 train_time:786ms step_avg:157.30ms tokens_processed:245760\n",
      "step:6/2000 train_time:938ms step_avg:156.34ms tokens_processed:294912\n",
      "step:7/2000 train_time:1093ms step_avg:156.10ms tokens_processed:344064\n",
      "step:8/2000 train_time:1249ms step_avg:156.16ms tokens_processed:393216\n",
      "step:9/2000 train_time:1403ms step_avg:155.91ms tokens_processed:442368\n",
      "step:10/2000 train_time:1554ms step_avg:155.44ms tokens_processed:491520\n",
      "step:11/2000 train_time:1707ms step_avg:155.19ms tokens_processed:540672\n",
      "step:12/2000 train_time:1859ms step_avg:154.92ms tokens_processed:589824\n",
      "step:13/2000 train_time:2012ms step_avg:154.77ms tokens_processed:638976\n",
      "step:14/2000 train_time:2167ms step_avg:154.77ms tokens_processed:688128\n",
      "step:15/2000 train_time:2321ms step_avg:154.72ms tokens_processed:737280\n",
      "step:16/2000 train_time:2475ms step_avg:154.66ms tokens_processed:786432\n",
      "step:17/2000 train_time:2628ms step_avg:154.61ms tokens_processed:835584\n",
      "step:18/2000 train_time:2781ms step_avg:154.51ms tokens_processed:884736\n",
      "step:19/2000 train_time:2934ms step_avg:154.40ms tokens_processed:933888\n",
      "step:20/2000 train_time:3088ms step_avg:154.38ms tokens_processed:983040\n",
      "step:21/2000 train_time:3242ms step_avg:154.36ms tokens_processed:1032192\n",
      "step:22/2000 train_time:3393ms step_avg:154.25ms tokens_processed:1081344\n",
      "step:23/2000 train_time:3548ms step_avg:154.27ms tokens_processed:1130496\n",
      "step:24/2000 train_time:3703ms step_avg:154.29ms tokens_processed:1179648\n",
      "step:25/2000 train_time:3856ms step_avg:154.23ms tokens_processed:1228800\n",
      "step:26/2000 train_time:4010ms step_avg:154.21ms tokens_processed:1277952\n",
      "step:27/2000 train_time:4165ms step_avg:154.24ms tokens_processed:1327104\n",
      "step:28/2000 train_time:4317ms step_avg:154.19ms tokens_processed:1376256\n",
      "step:29/2000 train_time:4471ms step_avg:154.17ms tokens_processed:1425408\n",
      "step:30/2000 train_time:4625ms step_avg:154.18ms tokens_processed:1474560\n",
      "step:31/2000 train_time:4779ms step_avg:154.16ms tokens_processed:1523712\n",
      "step:32/2000 train_time:4932ms step_avg:154.11ms tokens_processed:1572864\n",
      "step:33/2000 train_time:5086ms step_avg:154.12ms tokens_processed:1622016\n",
      "step:34/2000 train_time:5239ms step_avg:154.07ms tokens_processed:1671168\n",
      "step:35/2000 train_time:5392ms step_avg:154.06ms tokens_processed:1720320\n",
      "step:36/2000 train_time:5546ms step_avg:154.07ms tokens_processed:1769472\n",
      "step:37/2000 train_time:5700ms step_avg:154.04ms tokens_processed:1818624\n",
      "step:38/2000 train_time:5854ms step_avg:154.05ms tokens_processed:1867776\n",
      "step:39/2000 train_time:6008ms step_avg:154.05ms tokens_processed:1916928\n",
      "step:40/2000 train_time:6162ms step_avg:154.05ms tokens_processed:1966080\n",
      "step:41/2000 train_time:6316ms step_avg:154.05ms tokens_processed:2015232\n",
      "step:42/2000 train_time:6470ms step_avg:154.04ms tokens_processed:2064384\n",
      "step:43/2000 train_time:6623ms step_avg:154.02ms tokens_processed:2113536\n",
      "step:44/2000 train_time:6776ms step_avg:154.01ms tokens_processed:2162688\n",
      "step:45/2000 train_time:6931ms step_avg:154.02ms tokens_processed:2211840\n",
      "step:46/2000 train_time:7085ms step_avg:154.01ms tokens_processed:2260992\n",
      "step:47/2000 train_time:7239ms step_avg:154.02ms tokens_processed:2310144\n",
      "step:48/2000 train_time:7392ms step_avg:154.00ms tokens_processed:2359296\n",
      "step:49/2000 train_time:7547ms step_avg:154.02ms tokens_processed:2408448\n",
      "step:50/2000 train_time:7700ms step_avg:154.00ms tokens_processed:2457600\n",
      "step:51/2000 train_time:7853ms step_avg:153.99ms tokens_processed:2506752\n",
      "step:52/2000 train_time:8007ms step_avg:153.99ms tokens_processed:2555904\n",
      "step:53/2000 train_time:8161ms step_avg:153.98ms tokens_processed:2605056\n",
      "step:54/2000 train_time:8314ms step_avg:153.96ms tokens_processed:2654208\n",
      "step:55/2000 train_time:8469ms step_avg:153.98ms tokens_processed:2703360\n",
      "step:56/2000 train_time:8627ms step_avg:154.05ms tokens_processed:2752512\n",
      "step:57/2000 train_time:8780ms step_avg:154.03ms tokens_processed:2801664\n",
      "step:58/2000 train_time:8934ms step_avg:154.04ms tokens_processed:2850816\n",
      "step:59/2000 train_time:9087ms step_avg:154.02ms tokens_processed:2899968\n",
      "step:60/2000 train_time:9241ms step_avg:154.02ms tokens_processed:2949120\n",
      "step:61/2000 train_time:9395ms step_avg:154.01ms tokens_processed:2998272\n",
      "step:62/2000 train_time:9551ms step_avg:154.05ms tokens_processed:3047424\n",
      "step:63/2000 train_time:9705ms step_avg:154.05ms tokens_processed:3096576\n",
      "step:64/2000 train_time:9858ms step_avg:154.04ms tokens_processed:3145728\n",
      "step:65/2000 train_time:10012ms step_avg:154.03ms tokens_processed:3194880\n",
      "step:66/2000 train_time:10166ms step_avg:154.03ms tokens_processed:3244032\n",
      "step:67/2000 train_time:10319ms step_avg:154.01ms tokens_processed:3293184\n",
      "step:68/2000 train_time:10473ms step_avg:154.02ms tokens_processed:3342336\n",
      "step:69/2000 train_time:10628ms step_avg:154.02ms tokens_processed:3391488\n",
      "step:70/2000 train_time:10782ms step_avg:154.03ms tokens_processed:3440640\n",
      "step:71/2000 train_time:10935ms step_avg:154.02ms tokens_processed:3489792\n",
      "step:72/2000 train_time:11089ms step_avg:154.02ms tokens_processed:3538944\n",
      "step:73/2000 train_time:11243ms step_avg:154.02ms tokens_processed:3588096\n",
      "step:74/2000 train_time:11396ms step_avg:154.00ms tokens_processed:3637248\n",
      "step:75/2000 train_time:11551ms step_avg:154.02ms tokens_processed:3686400\n",
      "step:76/2000 train_time:11707ms step_avg:154.03ms tokens_processed:3735552\n",
      "step:77/2000 train_time:11860ms step_avg:154.03ms tokens_processed:3784704\n",
      "step:78/2000 train_time:12016ms step_avg:154.05ms tokens_processed:3833856\n",
      "step:79/2000 train_time:12171ms step_avg:154.06ms tokens_processed:3883008\n",
      "step:80/2000 train_time:12323ms step_avg:154.04ms tokens_processed:3932160\n",
      "step:81/2000 train_time:12479ms step_avg:154.06ms tokens_processed:3981312\n",
      "step:82/2000 train_time:12636ms step_avg:154.10ms tokens_processed:4030464\n",
      "step:83/2000 train_time:12794ms step_avg:154.14ms tokens_processed:4079616\n",
      "step:84/2000 train_time:12955ms step_avg:154.23ms tokens_processed:4128768\n",
      "step:85/2000 train_time:13109ms step_avg:154.22ms tokens_processed:4177920\n",
      "step:86/2000 train_time:13262ms step_avg:154.21ms tokens_processed:4227072\n",
      "step:87/2000 train_time:13415ms step_avg:154.19ms tokens_processed:4276224\n",
      "step:88/2000 train_time:13569ms step_avg:154.20ms tokens_processed:4325376\n",
      "step:89/2000 train_time:13723ms step_avg:154.19ms tokens_processed:4374528\n",
      "step:90/2000 train_time:13878ms step_avg:154.20ms tokens_processed:4423680\n",
      "step:91/2000 train_time:14031ms step_avg:154.18ms tokens_processed:4472832\n",
      "step:92/2000 train_time:14185ms step_avg:154.19ms tokens_processed:4521984\n",
      "step:93/2000 train_time:14338ms step_avg:154.18ms tokens_processed:4571136\n",
      "step:94/2000 train_time:14492ms step_avg:154.17ms tokens_processed:4620288\n",
      "step:95/2000 train_time:14647ms step_avg:154.18ms tokens_processed:4669440\n",
      "step:96/2000 train_time:14801ms step_avg:154.18ms tokens_processed:4718592\n",
      "step:97/2000 train_time:14955ms step_avg:154.17ms tokens_processed:4767744\n",
      "step:98/2000 train_time:15110ms step_avg:154.18ms tokens_processed:4816896\n",
      "step:99/2000 train_time:15264ms step_avg:154.18ms tokens_processed:4866048\n",
      "step:100/2000 train_time:15416ms step_avg:154.16ms tokens_processed:4915200\n",
      "---validation---\n",
      "step:100/2000 val_loss:5.5624 train_time:15425ms step_avg:154.25ms\n",
      "---end of validation---\n",
      "step:101/2000 train_time:15577ms step_avg:154.23ms tokens_processed:4964352\n",
      "step:102/2000 train_time:15730ms step_avg:154.22ms tokens_processed:5013504\n",
      "step:103/2000 train_time:15886ms step_avg:154.23ms tokens_processed:5062656\n",
      "step:104/2000 train_time:16038ms step_avg:154.22ms tokens_processed:5111808\n",
      "step:105/2000 train_time:16192ms step_avg:154.21ms tokens_processed:5160960\n",
      "step:106/2000 train_time:16346ms step_avg:154.21ms tokens_processed:5210112\n",
      "step:107/2000 train_time:16500ms step_avg:154.21ms tokens_processed:5259264\n",
      "step:108/2000 train_time:16654ms step_avg:154.20ms tokens_processed:5308416\n",
      "step:109/2000 train_time:16808ms step_avg:154.20ms tokens_processed:5357568\n",
      "step:110/2000 train_time:16966ms step_avg:154.24ms tokens_processed:5406720\n",
      "step:111/2000 train_time:17120ms step_avg:154.23ms tokens_processed:5455872\n",
      "step:112/2000 train_time:17273ms step_avg:154.22ms tokens_processed:5505024\n",
      "step:113/2000 train_time:17427ms step_avg:154.22ms tokens_processed:5554176\n",
      "step:114/2000 train_time:17581ms step_avg:154.22ms tokens_processed:5603328\n",
      "step:115/2000 train_time:17735ms step_avg:154.22ms tokens_processed:5652480\n",
      "step:116/2000 train_time:17889ms step_avg:154.21ms tokens_processed:5701632\n",
      "step:117/2000 train_time:18044ms step_avg:154.22ms tokens_processed:5750784\n",
      "step:118/2000 train_time:18197ms step_avg:154.21ms tokens_processed:5799936\n",
      "step:119/2000 train_time:18350ms step_avg:154.20ms tokens_processed:5849088\n",
      "step:120/2000 train_time:18505ms step_avg:154.21ms tokens_processed:5898240\n",
      "step:121/2000 train_time:18658ms step_avg:154.20ms tokens_processed:5947392\n",
      "step:122/2000 train_time:18812ms step_avg:154.20ms tokens_processed:5996544\n",
      "step:123/2000 train_time:18967ms step_avg:154.20ms tokens_processed:6045696\n",
      "step:124/2000 train_time:19122ms step_avg:154.21ms tokens_processed:6094848\n",
      "step:125/2000 train_time:19275ms step_avg:154.20ms tokens_processed:6144000\n",
      "step:126/2000 train_time:19428ms step_avg:154.19ms tokens_processed:6193152\n",
      "step:127/2000 train_time:19582ms step_avg:154.19ms tokens_processed:6242304\n",
      "step:128/2000 train_time:19736ms step_avg:154.18ms tokens_processed:6291456\n",
      "step:129/2000 train_time:19891ms step_avg:154.19ms tokens_processed:6340608\n",
      "step:130/2000 train_time:20046ms step_avg:154.20ms tokens_processed:6389760\n",
      "step:131/2000 train_time:20200ms step_avg:154.20ms tokens_processed:6438912\n",
      "step:132/2000 train_time:20354ms step_avg:154.20ms tokens_processed:6488064\n",
      "step:133/2000 train_time:20507ms step_avg:154.19ms tokens_processed:6537216\n",
      "step:134/2000 train_time:20662ms step_avg:154.19ms tokens_processed:6586368\n",
      "step:135/2000 train_time:20816ms step_avg:154.19ms tokens_processed:6635520\n",
      "step:136/2000 train_time:20970ms step_avg:154.19ms tokens_processed:6684672\n",
      "step:137/2000 train_time:21125ms step_avg:154.20ms tokens_processed:6733824\n",
      "step:138/2000 train_time:21277ms step_avg:154.18ms tokens_processed:6782976\n",
      "step:139/2000 train_time:21431ms step_avg:154.18ms tokens_processed:6832128\n",
      "step:140/2000 train_time:21585ms step_avg:154.18ms tokens_processed:6881280\n",
      "step:141/2000 train_time:21737ms step_avg:154.17ms tokens_processed:6930432\n",
      "step:142/2000 train_time:21892ms step_avg:154.17ms tokens_processed:6979584\n",
      "step:143/2000 train_time:22047ms step_avg:154.17ms tokens_processed:7028736\n",
      "step:144/2000 train_time:22202ms step_avg:154.18ms tokens_processed:7077888\n",
      "step:145/2000 train_time:22355ms step_avg:154.17ms tokens_processed:7127040\n",
      "step:146/2000 train_time:22510ms step_avg:154.18ms tokens_processed:7176192\n",
      "step:147/2000 train_time:22664ms step_avg:154.18ms tokens_processed:7225344\n",
      "step:148/2000 train_time:22818ms step_avg:154.17ms tokens_processed:7274496\n",
      "step:149/2000 train_time:22972ms step_avg:154.17ms tokens_processed:7323648\n",
      "step:150/2000 train_time:23126ms step_avg:154.18ms tokens_processed:7372800\n",
      "step:151/2000 train_time:23280ms step_avg:154.17ms tokens_processed:7421952\n",
      "step:152/2000 train_time:23433ms step_avg:154.17ms tokens_processed:7471104\n",
      "step:153/2000 train_time:23587ms step_avg:154.17ms tokens_processed:7520256\n",
      "step:154/2000 train_time:23740ms step_avg:154.16ms tokens_processed:7569408\n",
      "step:155/2000 train_time:23895ms step_avg:154.16ms tokens_processed:7618560\n",
      "step:156/2000 train_time:24048ms step_avg:154.16ms tokens_processed:7667712\n",
      "step:157/2000 train_time:24203ms step_avg:154.16ms tokens_processed:7716864\n",
      "step:158/2000 train_time:24358ms step_avg:154.16ms tokens_processed:7766016\n",
      "step:159/2000 train_time:24511ms step_avg:154.16ms tokens_processed:7815168\n",
      "step:160/2000 train_time:24666ms step_avg:154.16ms tokens_processed:7864320\n",
      "step:161/2000 train_time:24820ms step_avg:154.16ms tokens_processed:7913472\n",
      "step:162/2000 train_time:24974ms step_avg:154.16ms tokens_processed:7962624\n",
      "step:163/2000 train_time:25128ms step_avg:154.16ms tokens_processed:8011776\n",
      "step:164/2000 train_time:25282ms step_avg:154.16ms tokens_processed:8060928\n",
      "step:165/2000 train_time:25435ms step_avg:154.15ms tokens_processed:8110080\n",
      "step:166/2000 train_time:25590ms step_avg:154.15ms tokens_processed:8159232\n",
      "step:167/2000 train_time:25744ms step_avg:154.15ms tokens_processed:8208384\n",
      "step:168/2000 train_time:25897ms step_avg:154.15ms tokens_processed:8257536\n",
      "step:169/2000 train_time:26051ms step_avg:154.15ms tokens_processed:8306688\n",
      "step:170/2000 train_time:26205ms step_avg:154.15ms tokens_processed:8355840\n",
      "step:171/2000 train_time:26359ms step_avg:154.15ms tokens_processed:8404992\n",
      "step:172/2000 train_time:26512ms step_avg:154.14ms tokens_processed:8454144\n",
      "step:173/2000 train_time:26666ms step_avg:154.14ms tokens_processed:8503296\n",
      "step:174/2000 train_time:26820ms step_avg:154.14ms tokens_processed:8552448\n",
      "step:175/2000 train_time:26974ms step_avg:154.14ms tokens_processed:8601600\n",
      "step:176/2000 train_time:27127ms step_avg:154.13ms tokens_processed:8650752\n",
      "step:177/2000 train_time:27282ms step_avg:154.14ms tokens_processed:8699904\n",
      "step:178/2000 train_time:27435ms step_avg:154.13ms tokens_processed:8749056\n",
      "step:179/2000 train_time:27590ms step_avg:154.13ms tokens_processed:8798208\n",
      "step:180/2000 train_time:27744ms step_avg:154.13ms tokens_processed:8847360\n",
      "step:181/2000 train_time:27898ms step_avg:154.13ms tokens_processed:8896512\n",
      "step:182/2000 train_time:28051ms step_avg:154.13ms tokens_processed:8945664\n",
      "step:183/2000 train_time:28205ms step_avg:154.13ms tokens_processed:8994816\n",
      "step:184/2000 train_time:28360ms step_avg:154.13ms tokens_processed:9043968\n",
      "step:185/2000 train_time:28513ms step_avg:154.13ms tokens_processed:9093120\n",
      "step:186/2000 train_time:28668ms step_avg:154.13ms tokens_processed:9142272\n",
      "step:187/2000 train_time:28823ms step_avg:154.13ms tokens_processed:9191424\n",
      "step:188/2000 train_time:28975ms step_avg:154.12ms tokens_processed:9240576\n",
      "step:189/2000 train_time:29129ms step_avg:154.12ms tokens_processed:9289728\n",
      "step:190/2000 train_time:29284ms step_avg:154.13ms tokens_processed:9338880\n",
      "step:191/2000 train_time:29436ms step_avg:154.12ms tokens_processed:9388032\n",
      "step:192/2000 train_time:29592ms step_avg:154.12ms tokens_processed:9437184\n",
      "step:193/2000 train_time:29745ms step_avg:154.12ms tokens_processed:9486336\n",
      "step:194/2000 train_time:29900ms step_avg:154.12ms tokens_processed:9535488\n",
      "step:195/2000 train_time:30052ms step_avg:154.11ms tokens_processed:9584640\n",
      "step:196/2000 train_time:30209ms step_avg:154.13ms tokens_processed:9633792\n",
      "step:197/2000 train_time:30361ms step_avg:154.11ms tokens_processed:9682944\n",
      "step:198/2000 train_time:30514ms step_avg:154.11ms tokens_processed:9732096\n",
      "step:199/2000 train_time:30668ms step_avg:154.11ms tokens_processed:9781248\n",
      "step:200/2000 train_time:30823ms step_avg:154.11ms tokens_processed:9830400\n",
      "---validation---\n"
     ]
    }
   ],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    n_windows_long = dense_bm_schedule(step)\n",
    "    n_windows_short = sparse_bm_schedule(step)\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        \n",
    "        long_bm = get_blockmask(val_seq_len, n_windows_long)\n",
    "        short_bm = get_blockmask(val_seq_len, n_windows_short)\n",
    "\n",
    "        assert args.val_tokens % (val_batch_size * val_seq_len) == 0\n",
    "        val_steps = args.val_tokens // (val_batch_size * val_seq_len)\n",
    "\n",
    "        val_loader = distributed_data_generator(args.val_files, 4, 5*1024, align_to_bos=False)\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, long_bm, short_bm)\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader)\n",
    "    if step == 0: print(\"First inputs retrieved\", inputs.shape)\n",
    "    tokens_processed += inputs.numel() * world_size\n",
    "\n",
    "    long_bm = get_blockmask(train_seq_len, n_windows_long)\n",
    "    short_bm = get_blockmask(train_seq_len, n_windows_short)\n",
    "\n",
    "    model(inputs, targets, long_bm, short_bm).backward()\n",
    "\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{tokens_processed}\",\n",
    "        console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0f86-3515-4b2c-b6e7-ab96b5f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bc370-db63-4df0-b201-2354b66621fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a316db-a79c-4b98-ad06-5aa657360708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
