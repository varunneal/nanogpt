{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f894d5-2b23-46f6-8622-90ed61ff381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 1)) (2.1.2)\n",
      "Collecting tqdm (from -r ../requirements.txt (line 2))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r ../requirements.txt (line 3)) (2.7.0+cu126)\n",
      "Collecting huggingface-hub (from -r ../requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r ../requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch->-r ../requirements.txt (line 3)) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->-r ../requirements.txt (line 4)) (2.32.4)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub->-r ../requirements.txt (line 4))\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->-r ../requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r ../requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->-r ../requirements.txt (line 4)) (2025.8.3)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m203.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, hf-xet, huggingface-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [huggingface-hub] [huggingface-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.8 huggingface-hub-0.34.4 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/nightly/cu126\n",
      "Collecting torch==2.9.0.dev20250713+cu126\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.9.0.dev20250713%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.80)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.5.4.2)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.9 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/nvidia_nvshmem_cu12-3.3.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch==2.9.0.dev20250713+cu126) (1.11.1.6)\n",
      "Collecting pytorch-triton==3.4.0+gitae848267 (from torch==2.9.0.dev20250713+cu126)\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.4.0%2Bgitae848267-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.4.0+gitae848267->torch==2.9.0.dev20250713+cu126) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch==2.9.0.dev20250713+cu126) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.9.0.dev20250713+cu126) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/nightly/cu126/torch-2.9.0.dev20250713%2Bcu126-cp310-cp310-manylinux_2_28_x86_64.whl (822.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m822.6/822.6 MB\u001b[0m \u001b[31m157.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu126/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/nvidia_nvshmem_cu12-3.3.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 MB\u001b[0m \u001b[31m267.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.4.0%2Bgitae848267-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (154.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 MB\u001b[0m \u001b[31m201.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, pytorch-triton, nvidia-nvshmem-cu12, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0+cu126━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.0+cu126:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.0+cu1260m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torch]32m5/6\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.0+cu126 requires torch==2.7.0, but you have torch 2.9.0.dev20250713+cu126 which is incompatible.\n",
      "torchvision 0.22.0+cu126 requires torch==2.7.0, but you have torch 2.9.0.dev20250713+cu126 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.10.2.21 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvshmem-cu12-3.3.9 pytorch-triton-3.4.0+gitae848267 torch-2.9.0.dev20250713+cu126\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mName: torch\n",
      "Version: 2.9.0.dev20250713+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, pytorch-triton, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n",
      "fineweb_val_000000.bin: 100%|█████████████████| 200M/200M [00:00<00:00, 236MB/s]\n",
      "fineweb_train_000001.bin: 100%|███████████████| 200M/200M [00:00<00:00, 223MB/s]\n",
      "fineweb_train_000002.bin: 100%|███████████████| 200M/200M [00:00<00:00, 227MB/s]\n",
      "fineweb_train_000003.bin: 100%|███████████████| 200M/200M [00:00<00:00, 313MB/s]\n",
      "fineweb_train_000004.bin: 100%|███████████████| 200M/200M [00:00<00:00, 239MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip && pip install -r ../requirements.txt\n",
    "!pip install --pre \"torch==2.9.0.dev20250713+cu126\" --index-url https://download.pytorch.org/whl/nightly/cu126\n",
    "!pip show torch\n",
    "!pip install pytz\n",
    "!python ../data/cached_fineweb10B.py 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88f43-04ff-4317-9dbc-b7ae33cc69a2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62975aa6-f3b7-489a-9131-ea69089074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f91470-f6e0-431d-a490-23c80f3c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a879e98-a216-4626-a4ea-8db22d99df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432bad-bbd3-4853-9a8d-2b11d8feda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969746f-cc86-4121-a01e-19868c2a6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c63379b-7f98-4b29-aa53-31af47bc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f8de9-ae0d-4835-8897-b8def61c248b",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51675374-ca07-4849-89a2-f0cf8b130c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "timestamp = datetime.now(eastern).strftime(\"%H:%M-%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc86ec8-99e5-4389-96ed-3a6101591613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/03:02-2025-08-25.txt\n"
     ]
    }
   ],
   "source": [
    "# begin logging\n",
    "logfile = None\n",
    "\n",
    "run_id = os.environ.get(\"NB_BASE\",uuid.uuid4())\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "logfile = f\"logs/{timestamp}.txt\"\n",
    "print(logfile)\n",
    "\n",
    "\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a587fb-7ff8-46de-82cd-3c98b8ea96ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]',\n",
       " '2.9.0.dev20250713+cu126')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version, torch.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ccb5-4d25-4918-aeb5-1b5a1f44a0fe",
   "metadata": {},
   "source": [
    "## Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1dd716-9d1b-4086-ae9c-3cf032ec04a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.compile ## ns\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f0f206-d925-4920-8f53-681265a44628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # Efficient systems-wise implementation of step developed by @YouJiacheng,\n",
    "        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,\n",
    "        # @ryanyang0, and @vagrawal.\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                if base_i + rank < len(params):\n",
    "                    grad = params[base_i + rank].grad\n",
    "                # This gives strange dynamo warnings\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * world_size\n",
    "            momentum = group[\"momentum\"]\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                if base_i + rank < len(params):\n",
    "                    p = params[base_i + rank]\n",
    "                    grad = p.grad\n",
    "                    eff_lr = group[\"lr\"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, \"lr_mul\", 1.0)\n",
    "                    eff_weight_decay = group[\"lr\"] * group[\"weight_decay\"] * getattr(p, \"wd_mul\", 1.0)\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
    "                    momentum_buffer = state[\"momentum_buffer\"]\n",
    "                    p.mul_(1 - eff_weight_decay)\n",
    "                    momentum_buffer.lerp_(grad, 1 - momentum)\n",
    "                    grad = grad.lerp_(momentum_buffer, momentum)\n",
    "                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)\n",
    "                    p.add_(other=v, alpha=-eff_lr)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbc0f81-97ec-4ed7-99df-78f34bada062",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DistAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "        # DistributedAdam implementation by @vagrawal\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78ffb1-753b-4a6f-a57f-81781873c084",
   "metadata": {},
   "source": [
    "## Custom Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e2360d-f65a-451b-89c9-9020d252e665",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x.new_tensor(x_s, dtype=torch.float32),\n",
    "            scale_b=x.new_tensor(w_s, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)\n",
    "        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)\n",
    "        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)\n",
    "        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=grad_inv_s,\n",
    "            scale_b=w_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=x_inv_s,\n",
    "            scale_b=grad_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "        return grad_x, grad_w\n",
    "\n",
    "    return impl(g, x_f8, w_f8)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)\n",
    "\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    return grad_x, grad_w, None, None, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c129e66-be79-4122-857d-fe205176d0c6",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ec017e-065d-4f43-b092-9163c3aaf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43c37-4d2a-4fd6-bd58-63fcd1b5a54a",
   "metadata": {},
   "source": [
    "### CastedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4934dccb-7f32-442d-a7b8-4cc8d666d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastedLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.use_fp8 = use_fp8\n",
    "        self.x_s = x_s\n",
    "        self.w_s = w_s\n",
    "        self.grad_s = grad_s\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        assert x.size(-1) == self.in_features\n",
    "        if self.use_fp8 and self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65922453-7b13-425f-a888-899f53d243ea",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c79cc0a-7a69-4d76-88a0-906ee65da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim // 4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim // 4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a536ae-14b2-4b5c-b0ee-f2ca0ad7045a",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83a21056-70ee-4b77-a2b8-8c33f427bbb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        self.attn_scale = 0.12\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):\n",
    "        B, T = x.size(0), x.size(1)  # batch size, sequence length\n",
    "\n",
    "        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads,\n",
    "                                                                             self.head_dim).chunk(3, dim=-2)\n",
    "        q, k = norm(q), norm(k)  # QK norm @Grad62304977\n",
    "        q, k = self.rotary(q), self.rotary(k)\n",
    "        if ve is not None:\n",
    "            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)  # @KoszarskyB & @Grad62304977\n",
    "        else:  # skip mid-layers token value embeddings by @YouJiacheng\n",
    "            v = lambdas[0] * v\n",
    "        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask,\n",
    "                           scale=self.attn_scale).transpose(1, 2)\n",
    "        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6828-b90a-47c6-9759-73bbab68521c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992d3a44-eca9-4645-b9b9-b678cd76fa19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = CastedLinear(dim, hdim)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(\n",
    "            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa25d-96b7-4026-904d-e9b7f6e7ce7a",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c18407-3a67-4e22-9f5f-7717fb3cffcf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor,\n",
    "                block_mask: BlockMask):\n",
    "\n",
    "        x = lambdas[0] * x + lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db37f6-e22c-4a8c-9d02-8283ba4961b7",
   "metadata": {},
   "source": [
    "### Create blockmasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77098330-ee81-44a3-ae1f-947a4f04060c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_blockmasks(seq_len: int,\n",
    "                      sliding_window_num_blocks: int,\n",
    "                      BLOCK_SIZE: int = 128,\n",
    "                      device: torch.device = \"cuda\") -> BlockMask:\n",
    "    assert seq_len % BLOCK_SIZE == 0, \"seq_len must be a multiple of BLOCK_SIZE\"\n",
    "\n",
    "    Q = seq_len // BLOCK_SIZE\n",
    "    i32 = torch.int32\n",
    "\n",
    "    # Block coordinates\n",
    "    q = torch.arange(Q, device=device, dtype=i32).view(-1, 1)  # [Q,1]\n",
    "    k = torch.arange(Q, device=device, dtype=i32).view(1, -1)  # [1,K]\n",
    "\n",
    "    \n",
    "    # Partial (current block): counts=1, indices first column = q\n",
    "    partial_idx = torch.zeros((Q, Q), dtype=i32, device=device)   # [Q,K]\n",
    "    partial_idx[:, 0] = q.squeeze(1)\n",
    "    partial_cnt = torch.ones((Q,), dtype=i32, device=device)      # [Q]\n",
    "\n",
    "    # Full (previous blocks), nearest-first prefix: [q-1, q-2, ..., 0]\n",
    "    full_idx = (q - 1 - k) % Q                                    # [Q,K]\n",
    "    full_cnt = torch.arange(Q, dtype=i32, device=device)          # [Q] = number of previous blocks\n",
    "\n",
    "    # Add (B,H) broadcast dims\n",
    "    partial_kv_indices    = partial_idx[None, None].contiguous()  # [1,1,Q,K]\n",
    "    full_kv_indices       = full_idx[None, None].contiguous()     # [1,1,Q,K]\n",
    "    partial_kv_num_blocks = partial_cnt[None, None].contiguous()  # [1,1,Q]\n",
    "    full_kv_num_blocks    = full_cnt[None, None].contiguous()     # [1,1,Q]\n",
    "\n",
    "    # Token-level causal within the current (partial) block\n",
    "    def causal_mask_mod(b, h, q_idx, kv_idx):\n",
    "        return q_idx >= kv_idx\n",
    "\n",
    "    # Windowing (ints → tensor ops via broadcasting)\n",
    "    W = max(sliding_window_num_blocks, 1)\n",
    "    max_full = max(W - 1, 0)                                      # int\n",
    "    full_num = torch.clamp_max(full_kv_num_blocks, max_full)      # [1,1,Q]\n",
    "    remain_for_partial = torch.clamp_min(W - full_num, 1)         # [1,1,Q]\n",
    "    partial_num = torch.minimum(partial_kv_num_blocks, remain_for_partial)\n",
    "\n",
    "    return BlockMask.from_kv_blocks(\n",
    "        partial_num, partial_kv_indices,\n",
    "        full_num,    full_kv_indices,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        mask_mod=causal_mask_mod,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0c5feb-2400-4ece-9bfd-1435ca4eb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "@lru_cache(1)\n",
    "def get_blockmask(seq_len, n_windows):\n",
    "    if (seq_len, n_windows) not in masks:\n",
    "        bm = create_blockmasks(seq_len, n_windows)\n",
    "        masks[(seq_len, n_windows)] = bm\n",
    "    return masks[(seq_len, n_windows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e0737-3fe1-49c5-af57-54d677f9b5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5beb6881-789d-45e3-bc87-f9fad2f58cfd",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9eacdf-a884-417f-a89d-158d72ac8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "\n",
    "        # self.lm_head = CastedLinear(model_dim, vocab_size)\n",
    "        self.lm_head = CastedLinear(model_dim, \n",
    "                                    vocab_size, \n",
    "                                    use_fp8=True, \n",
    "                                    x_s=(model_dim ** 0.5) / 448, \n",
    "                                    w_s=24 / 448,\n",
    "                                    grad_s=1 / 448\n",
    "                                   )\n",
    "        self.lm_head.weight.detach().zero_()  # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        pad = (-num_layers * 5) % dist.get_world_size()\n",
    "        self.scalars = nn.Parameter(torch.cat([\n",
    "            torch.ones(num_layers),  # skip_weights\n",
    "            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],  # block lambdas\n",
    "            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],  # SA lambdas\n",
    "            torch.ones(pad),\n",
    "        ]))\n",
    "        # set learning rates\n",
    "        for param in self.embed.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        for param in self.value_embeds.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        self.lm_head.weight.lr_mul = 27.5\n",
    "        self.scalars.lr_mul = 5.0\n",
    "\n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor, long_bm: BlockMask, short_bm: BlockMask):\n",
    "        assert input_seq.ndim == 2\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm,\n",
    "                       short_bm, long_bm]\n",
    "        assert len(block_masks) == len(self.blocks)\n",
    "        \n",
    "        x = x0 = norm(self.embed(input_seq))  # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        skip_weights = self.scalars[:(len(self.blocks) // 2)]\n",
    "        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)\n",
    "        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)\n",
    "\n",
    "        n = len(self.blocks) // 2\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "\n",
    "        logits = self.lm_head(x).float()\n",
    "\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1) ** 0.5))\n",
    "        # loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq,\n",
    "        #                        reduction=\"sum\" if self.training else \"mean\")\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq.view(-1),\n",
    "                       reduction=\"sum\" if self.training else \"mean\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18b470-51e6-466a-a90b-20606dcac5dc",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1f0904-4288-4559-b8ca-20f6084d5bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EOSBatchFinder:\n",
    "    \"\"\"\n",
    "    Distributed data generator class.\n",
    "    - align_to_bos: True -> every sequence starts at the beginning of a document.\n",
    "    - Supports batch_size and seq_len getting changed changed after initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename_pattern: str, seq_len: int, batch_size: int, \n",
    "                 align_to_bos: bool = True, eos_id: int = 50256, device: str = \"cuda\",\n",
    "    ):\n",
    "        self.files: List[Path] = [Path(p) for p in sorted(glob.glob(filename_pattern))]\n",
    "        if not self.files:\n",
    "            raise FileNotFoundError(f\"No files match: {filename_pattern}\")\n",
    "        self.file_iter = iter(self.files)\n",
    "\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        self.seq_len, self.batch_size = seq_len, batch_size\n",
    "        self.align_to_bos, self.eos_id, self.device = align_to_bos, eos_id, device\n",
    "\n",
    "        if self.batch_size % self.world_size:\n",
    "            raise ValueError(\"batch_size must be divisible by world_size\")\n",
    "\n",
    "        self.tokens, self.pos = self._load_next_shard()\n",
    "        self.eos_idx, self.i = self._build_eos_index(), 0\n",
    "        if self.align_to_bos:\n",
    "            self._seek(self.pos)\n",
    "\n",
    "    def _load_next_shard(self):\n",
    "        path = next(self.file_iter)  # may raise StopIteration (end of data)\n",
    "        header = torch.from_file(str(path), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "        assert header[1] == 1, \"unsupported version\"\n",
    "        num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "        with path.open(\"rb\", buffering=0) as f:\n",
    "            tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "            f.seek(256 * 4)\n",
    "            nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "            assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "        return tokens, 0\n",
    "\n",
    "    def _build_eos_index(self):\n",
    "        return (self.tokens == self.eos_id).nonzero(as_tuple=True)[0].to(torch.int64).cpu()\n",
    "\n",
    "    def _seek(self, pos: int):\n",
    "        j = int(torch.searchsorted(self.eos_idx, int(pos)))\n",
    "        if j >= int(self.eos_idx.numel()):\n",
    "            raise StopIteration(\"Seek past last EOS in shard.\")\n",
    "        self.i, self.pos = j, pos\n",
    "    \n",
    "    def generator(self):\n",
    "        return self._gen_eos() if self.align_to_bos else self._gen_contig()\n",
    "\n",
    "    def _gen_contig(self):\n",
    "        \"\"\"Contiguous slicing (no EOS alignment).\"\"\"\n",
    "        while True:\n",
    "            B, L, W, R = self.batch_size, self.seq_len, self.world_size, self.rank\n",
    "            if B % W: \n",
    "                raise ValueError(\"batch_size must be divisible by world_size\")\n",
    "            B_local = B // W\n",
    "            tokens_global, tokens_local = B * L, (B // W) * L\n",
    "\n",
    "            if self.pos + tokens_global + 1 >= len(self.tokens):\n",
    "                self.tokens, self.pos = self._load_next_shard(); continue\n",
    "\n",
    "            start = self.pos + R * tokens_local\n",
    "            buf = self.tokens[start : start + tokens_local + 1]\n",
    "            if buf.numel() < tokens_local + 1:\n",
    "                self.tokens, self.pos = self._load_next_shard(); continue\n",
    "\n",
    "            x = buf[:-1].view(B_local, L)\n",
    "            y = buf[1: ].view(B_local, L)\n",
    "            self.pos += tokens_global\n",
    "\n",
    "            yield (\n",
    "                x.to(self.device, dtype=torch.int32, non_blocking=True),\n",
    "                y.to(self.device, dtype=torch.int64, non_blocking=True)\n",
    "            )\n",
    "\n",
    "    def _gen_eos(self):\n",
    "        \"\"\"EOS-aligned slicing (each sample starts after an EOS).\"\"\"\n",
    "        while True:\n",
    "            B, L, W, R = self.batch_size, self.seq_len, self.world_size, self.rank\n",
    "            if B % W: \n",
    "                raise ValueError(\"batch_size must be divisible by world_size\")\n",
    "            B_local = B // W\n",
    "\n",
    "            n = int(self.eos_idx.numel())\n",
    "            if self.i >= n:\n",
    "                self.tokens, self.pos = self._load_next_shard()\n",
    "                self.eos_idx, self.i = self._build_eos_index(), 0\n",
    "                continue\n",
    "\n",
    "            starts = [[] for _ in range(W)]\n",
    "            idx = self.i\n",
    "            cur = self.eos_idx[idx]\n",
    "            \n",
    "            ok = True\n",
    "            for r in range(W):\n",
    "                for _ in range(B_local):\n",
    "                    start = cur + 1\n",
    "                    target = start + L\n",
    "                    j = int(torch.searchsorted(self.eos_idx, target))\n",
    "                    if j >= n:\n",
    "                        ok = False; break\n",
    "                    starts[r].append(start)\n",
    "                    idx, cur = j, self.eos_idx[idx]\n",
    "                if not ok: break\n",
    "\n",
    "            if not ok:\n",
    "                self.tokens, self.pos = self._load_next_shard()\n",
    "                self.eos_idx, self.i = self._build_eos_index(), 0\n",
    "                continue\n",
    "\n",
    "            self.pos += self.eos_idx[idx] - self.pos\n",
    "            self.i = idx\n",
    "\n",
    "            bufs = [self.tokens[s : s + L + 1] for s in starts[R]]\n",
    "            buf = torch.stack(bufs, dim=0)  # [B_local, L+1]\n",
    "            x, y = buf[:, :-1], buf[:, 1:]\n",
    "\n",
    "            yield (\n",
    "                x.to(self.device, dtype=torch.int32, non_blocking=True),\n",
    "                y.to(self.device, dtype=torch.int64, non_blocking=True)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6ff7-2376-4818-b0c6-63ea5c91a4c6",
   "metadata": {},
   "source": [
    "## dist init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed75c35b-54c5-4816-b1ef-10f4da165b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['LOCAL_RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['GROUP_RANK'] = '0'\n",
    "os.environ['ROLE_RANK'] = '0'\n",
    "os.environ['ROLE_NAME'] = 'default'\n",
    "os.environ['ROLE_WORLD_SIZE'] = '1'\n",
    "os.environ['TORCHELASTIC_RESTART_COUNT'] = '0'\n",
    "os.environ['TORCHELASTIC_MAX_RESTARTS'] = '0'\n",
    "os.environ['TORCHELASTIC_RUN_ID'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666f8fcd-5586-41af-a170-3fc6948e173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "master_process = (rank == 0)  # this process will do logging, checkpointing etc.\n",
    "# assert world_size == 8  # this code is designed for 8xH100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "931ae567-26bd-4964-b2d0-6869410a1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79666103-36de-44e3-b320-fa5a077e8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc64ee3-9295-4345-b4a3-5cc483383749",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f33bd48e-caee-4ead-9786-c1411e1bdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f125ece-109d-43f6-8bcf-aa3e73c1ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially refactor code to take in minibatch sizes (e.g. per world sizes) instead\n",
    "# currently batch size gets distributed across all GPUs -- in this case, just 1\n",
    "# at current setup, when we scale to 8 gpus, we want to multiply batch size by 8 to maintain minibatch size\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files: str = \"../data/fineweb10B/fineweb_train_*.bin\"  # input .bin to train on\n",
    "    val_files: str = \"../data/fineweb10B/fineweb_val_*.bin\"  # input .bin to eval validation loss on\n",
    "    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    \n",
    "    # train_seq_len = 1024 * 6\n",
    "    # train_batch_size = 8 * world_size\n",
    "\n",
    "    # val_seq_len = 1024 * 5  # FlexAttention sequence length for validation\n",
    "    # val_batch_size = 4 * world_size\n",
    "    \n",
    "    # optimization\n",
    "    num_iterations = 1750  # number of iterations to run\n",
    "    cooldown_frac = 0.45  # fraction of training spent cooling down the learning rate\n",
    "    \n",
    "    # evaluation and logging\n",
    "    val_loss_every = 100  # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "\n",
    "    vocab_size = next_multiple_of_n(50257, n=128)\n",
    "    num_layers = 12\n",
    "    num_heads = 6\n",
    "    model_dim = 768\n",
    "\n",
    "    block_size = 128\n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0e0d651-85d1-41d7-a3bb-a08a24b5fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq_len, val_batch_size = 1024 * 5, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045b81c-2a0a-4002-8eae-8a7bdf76b6d2",
   "metadata": {},
   "source": [
    "### schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ee4051-b2fd-463b-b76c-1780ad214171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c678927-eec9-4052-93a2-366025b34d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_piecewise(values, breaks):\n",
    "    assert len(values) == len(breaks) + 1, \"values must be one longer than breaks\"\n",
    "    \n",
    "    def f(t: int) -> int:\n",
    "        i = bisect_right(breaks, t)\n",
    "        return values[i]\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1496e86e-f00a-4952-8f27-8dbe12ff2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = [\n",
    "    {\n",
    "        'dense': 2, 'sparse': 1,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "    {\n",
    "        'dense': 2, 'sparse': 1,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "        {\n",
    "        'dense': 8, 'sparse': 2,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "    {\n",
    "        'dense': 8, 'sparse': 2,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "    {\n",
    "        'dense': 8, 'sparse': 2,\n",
    "        'seq_len': 1024 * 6, 'batch_size': 8,\n",
    "        'lr_mult': 1.0,\n",
    "    },\n",
    "]\n",
    "scheduler = make_piecewise(schedule, breaks = [324, 768, 1152, 1476])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c819b4e-a1f7-4663-aede-9f9900fb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    lr = 1.0\n",
    "    if x >= 1 - args.cooldown_frac:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        lr = w * 1.0 + (1 - w) * 0.1\n",
    "    return lr * scheduler(step)['lr_mult']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb0b9b-de7f-4052-a2fe-e6f9a69fc930",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "253fdb42-b56c-4a3b-bb0c-7ac5837d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = GPT(vocab_size=args.vocab_size, \n",
    "                       num_layers=args.num_layers, \n",
    "                       num_heads=args.num_heads, \n",
    "                       model_dim=args.model_dim,\n",
    "                       max_seq_len=max(schedule[-1]['seq_len'], val_seq_len),\n",
    "                      ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c4760a4-1858-499b-b961-530cd04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a03b657a-b05f-4aa5-bf53-d07702385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12334b9f-193f-446b-a3e8-00a1479bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10,\n",
    "                      weight_decay=0.0)\n",
    "optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2672d18a-aad3-4706-822b-804d9ca90ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = torch.compile(model, dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809242-8dca-4f72-bc28-736aa58fc03c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e779f1-8889-4a9b-afcc-6a3172237e49",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bb87203-fcaa-4f27-a53a-b75a20d383f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch._dynamo import reset\n",
    "from torch._dynamo.utils import counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fc40a3d-7ad0-4a8c-a035-e14357268cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 100\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "908e97d0-4c3e-47f7-8c09-a04c0ba16694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = EOSBatchFinder(args.train_files, scheduler(0)['seq_len'], scheduler(0)['batch_size'])\n",
    "train_loader_gen = train_loader.generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83caec-1278-405b-a221-c5ca9722da07",
   "metadata": {},
   "source": [
    " 878ms wall time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bea800c-d838-4fa2-a4dd-aecd524dff13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b623eee157b2408e9deffee56dee696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: This API is going to be deprecated, please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(warmup_steps)):\n",
    "    inputs, targets = next(train_loader_gen)\n",
    "    long_bm = get_blockmask(train_loader.seq_len, schedule[step % 4]['dense'])\n",
    "    short_bm = get_blockmask(train_loader.seq_len, schedule[step % 4]['sparse'])\n",
    "    model(inputs, targets, long_bm, short_bm).backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bf4a142-a25f-4ebf-833d-2273d7dfc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(initial_state[\"model\"])\n",
    "for opt, opt_state in zip(optimizers, initial_state[\"optimizers\"]):\n",
    "    opt.load_state_dict(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00310994-df28-4324-97f3-37b17006dfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del train_loader, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27d96a-9085-417b-b8e3-479220a3f2e1",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc187144-6bec-4bca-aa0e-9405bf1693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = EOSBatchFinder(args.train_files, scheduler(0)['seq_len'], scheduler(0)['batch_size'])\n",
    "train_loader_gen = train_loader.generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5496e90d-9c4e-43ce-92f6-b49362e624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = args.num_iterations\n",
    "training_time_ms, tokens_processed = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c3dd5e0-d354-474d-b3c3-7a3ebb8c2ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense': 8, 'sparse': 2, 'seq_len': 6144, 'batch_size': 8, 'lr_mult': 1.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b40533ba-cc8e-40aa-a2c9-2730c0c0d16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---validation---\n",
      "step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.37ms\n",
      "---end of validation---\n",
      "First inputs retrieved\n",
      "step:1/1750 train_time:159ms step_avg:158.90ms tokens_processed:49152\n",
      "step:2/1750 train_time:296ms step_avg:148.15ms tokens_processed:98304\n",
      "step:3/1750 train_time:443ms step_avg:147.51ms tokens_processed:147456\n",
      "step:4/1750 train_time:593ms step_avg:148.15ms tokens_processed:196608\n",
      "step:5/1750 train_time:743ms step_avg:148.55ms tokens_processed:245760\n",
      "step:6/1750 train_time:893ms step_avg:148.87ms tokens_processed:294912\n",
      "step:7/1750 train_time:1044ms step_avg:149.16ms tokens_processed:344064\n",
      "step:8/1750 train_time:1199ms step_avg:149.86ms tokens_processed:393216\n",
      "step:9/1750 train_time:1350ms step_avg:150.04ms tokens_processed:442368\n",
      "step:10/1750 train_time:1502ms step_avg:150.21ms tokens_processed:491520\n",
      "step:11/1750 train_time:1653ms step_avg:150.26ms tokens_processed:540672\n",
      "step:12/1750 train_time:1805ms step_avg:150.45ms tokens_processed:589824\n",
      "step:13/1750 train_time:1956ms step_avg:150.48ms tokens_processed:638976\n",
      "step:14/1750 train_time:2112ms step_avg:150.88ms tokens_processed:688128\n",
      "step:15/1750 train_time:2264ms step_avg:150.94ms tokens_processed:737280\n",
      "step:16/1750 train_time:2417ms step_avg:151.07ms tokens_processed:786432\n",
      "step:17/1750 train_time:2569ms step_avg:151.12ms tokens_processed:835584\n",
      "step:18/1750 train_time:2722ms step_avg:151.20ms tokens_processed:884736\n",
      "step:19/1750 train_time:2871ms step_avg:151.13ms tokens_processed:933888\n",
      "step:20/1750 train_time:3024ms step_avg:151.18ms tokens_processed:983040\n",
      "step:21/1750 train_time:3176ms step_avg:151.25ms tokens_processed:1032192\n",
      "step:22/1750 train_time:3333ms step_avg:151.50ms tokens_processed:1081344\n",
      "step:23/1750 train_time:3487ms step_avg:151.60ms tokens_processed:1130496\n",
      "step:24/1750 train_time:3639ms step_avg:151.62ms tokens_processed:1179648\n",
      "step:25/1750 train_time:3791ms step_avg:151.62ms tokens_processed:1228800\n",
      "step:26/1750 train_time:3944ms step_avg:151.68ms tokens_processed:1277952\n",
      "step:27/1750 train_time:4095ms step_avg:151.65ms tokens_processed:1327104\n",
      "step:28/1750 train_time:4248ms step_avg:151.70ms tokens_processed:1376256\n",
      "step:29/1750 train_time:4399ms step_avg:151.70ms tokens_processed:1425408\n",
      "step:30/1750 train_time:4552ms step_avg:151.74ms tokens_processed:1474560\n",
      "step:31/1750 train_time:4704ms step_avg:151.73ms tokens_processed:1523712\n",
      "step:32/1750 train_time:4856ms step_avg:151.74ms tokens_processed:1572864\n",
      "step:33/1750 train_time:5008ms step_avg:151.76ms tokens_processed:1622016\n",
      "step:34/1750 train_time:5161ms step_avg:151.80ms tokens_processed:1671168\n",
      "step:35/1750 train_time:5313ms step_avg:151.81ms tokens_processed:1720320\n",
      "step:36/1750 train_time:5466ms step_avg:151.83ms tokens_processed:1769472\n",
      "step:37/1750 train_time:5620ms step_avg:151.89ms tokens_processed:1818624\n",
      "step:38/1750 train_time:5772ms step_avg:151.88ms tokens_processed:1867776\n",
      "step:39/1750 train_time:5924ms step_avg:151.89ms tokens_processed:1916928\n",
      "step:40/1750 train_time:6075ms step_avg:151.88ms tokens_processed:1966080\n",
      "step:41/1750 train_time:6228ms step_avg:151.90ms tokens_processed:2015232\n",
      "step:42/1750 train_time:6381ms step_avg:151.92ms tokens_processed:2064384\n",
      "step:43/1750 train_time:6532ms step_avg:151.91ms tokens_processed:2113536\n",
      "step:44/1750 train_time:6685ms step_avg:151.94ms tokens_processed:2162688\n",
      "step:45/1750 train_time:6837ms step_avg:151.93ms tokens_processed:2211840\n",
      "step:46/1750 train_time:6990ms step_avg:151.95ms tokens_processed:2260992\n",
      "step:47/1750 train_time:7142ms step_avg:151.96ms tokens_processed:2310144\n",
      "step:48/1750 train_time:7295ms step_avg:151.99ms tokens_processed:2359296\n",
      "step:49/1750 train_time:7449ms step_avg:152.02ms tokens_processed:2408448\n",
      "step:50/1750 train_time:7600ms step_avg:151.99ms tokens_processed:2457600\n",
      "step:51/1750 train_time:7751ms step_avg:151.99ms tokens_processed:2506752\n",
      "step:52/1750 train_time:7904ms step_avg:152.00ms tokens_processed:2555904\n",
      "step:53/1750 train_time:8056ms step_avg:151.99ms tokens_processed:2605056\n",
      "step:54/1750 train_time:8209ms step_avg:152.01ms tokens_processed:2654208\n",
      "step:55/1750 train_time:8360ms step_avg:152.00ms tokens_processed:2703360\n",
      "step:56/1750 train_time:8514ms step_avg:152.03ms tokens_processed:2752512\n",
      "step:57/1750 train_time:8664ms step_avg:152.00ms tokens_processed:2801664\n",
      "step:58/1750 train_time:8819ms step_avg:152.05ms tokens_processed:2850816\n",
      "step:59/1750 train_time:8971ms step_avg:152.05ms tokens_processed:2899968\n",
      "step:60/1750 train_time:9124ms step_avg:152.07ms tokens_processed:2949120\n",
      "step:61/1750 train_time:9279ms step_avg:152.11ms tokens_processed:2998272\n",
      "step:62/1750 train_time:9430ms step_avg:152.10ms tokens_processed:3047424\n",
      "step:63/1750 train_time:9582ms step_avg:152.09ms tokens_processed:3096576\n",
      "step:64/1750 train_time:9734ms step_avg:152.09ms tokens_processed:3145728\n",
      "step:65/1750 train_time:9889ms step_avg:152.14ms tokens_processed:3194880\n",
      "step:66/1750 train_time:10041ms step_avg:152.13ms tokens_processed:3244032\n",
      "step:67/1750 train_time:10193ms step_avg:152.14ms tokens_processed:3293184\n",
      "step:68/1750 train_time:10346ms step_avg:152.15ms tokens_processed:3342336\n",
      "step:69/1750 train_time:10498ms step_avg:152.14ms tokens_processed:3391488\n",
      "step:70/1750 train_time:10651ms step_avg:152.15ms tokens_processed:3440640\n",
      "step:71/1750 train_time:10802ms step_avg:152.15ms tokens_processed:3489792\n",
      "step:72/1750 train_time:10954ms step_avg:152.14ms tokens_processed:3538944\n",
      "step:73/1750 train_time:11107ms step_avg:152.15ms tokens_processed:3588096\n",
      "step:74/1750 train_time:11259ms step_avg:152.15ms tokens_processed:3637248\n",
      "step:75/1750 train_time:11413ms step_avg:152.17ms tokens_processed:3686400\n",
      "step:76/1750 train_time:11565ms step_avg:152.17ms tokens_processed:3735552\n",
      "step:77/1750 train_time:11717ms step_avg:152.17ms tokens_processed:3784704\n",
      "step:78/1750 train_time:11870ms step_avg:152.18ms tokens_processed:3833856\n",
      "step:79/1750 train_time:12022ms step_avg:152.18ms tokens_processed:3883008\n",
      "step:80/1750 train_time:12175ms step_avg:152.19ms tokens_processed:3932160\n",
      "step:81/1750 train_time:12329ms step_avg:152.21ms tokens_processed:3981312\n",
      "step:82/1750 train_time:12482ms step_avg:152.22ms tokens_processed:4030464\n",
      "step:83/1750 train_time:12632ms step_avg:152.20ms tokens_processed:4079616\n",
      "step:84/1750 train_time:12785ms step_avg:152.21ms tokens_processed:4128768\n",
      "step:85/1750 train_time:12937ms step_avg:152.20ms tokens_processed:4177920\n",
      "step:86/1750 train_time:13091ms step_avg:152.22ms tokens_processed:4227072\n",
      "step:87/1750 train_time:13243ms step_avg:152.22ms tokens_processed:4276224\n",
      "step:88/1750 train_time:13396ms step_avg:152.23ms tokens_processed:4325376\n",
      "step:89/1750 train_time:13549ms step_avg:152.24ms tokens_processed:4374528\n",
      "step:90/1750 train_time:13702ms step_avg:152.25ms tokens_processed:4423680\n",
      "step:91/1750 train_time:13853ms step_avg:152.23ms tokens_processed:4472832\n",
      "step:92/1750 train_time:14007ms step_avg:152.25ms tokens_processed:4521984\n",
      "step:93/1750 train_time:14159ms step_avg:152.24ms tokens_processed:4571136\n",
      "step:94/1750 train_time:14313ms step_avg:152.26ms tokens_processed:4620288\n",
      "step:95/1750 train_time:14465ms step_avg:152.27ms tokens_processed:4669440\n",
      "step:96/1750 train_time:14620ms step_avg:152.29ms tokens_processed:4718592\n",
      "step:97/1750 train_time:14772ms step_avg:152.29ms tokens_processed:4767744\n",
      "step:98/1750 train_time:14925ms step_avg:152.29ms tokens_processed:4816896\n",
      "step:99/1750 train_time:15076ms step_avg:152.28ms tokens_processed:4866048\n",
      "step:100/1750 train_time:15230ms step_avg:152.30ms tokens_processed:4915200\n",
      "---validation---\n",
      "step:100/1750 val_loss:5.9614 train_time:15236ms step_avg:152.36ms\n",
      "---end of validation---\n",
      "step:101/1750 train_time:15388ms step_avg:152.36ms tokens_processed:4964352\n",
      "step:102/1750 train_time:15539ms step_avg:152.34ms tokens_processed:5013504\n",
      "step:103/1750 train_time:15691ms step_avg:152.34ms tokens_processed:5062656\n",
      "step:104/1750 train_time:15845ms step_avg:152.36ms tokens_processed:5111808\n",
      "step:105/1750 train_time:15999ms step_avg:152.37ms tokens_processed:5160960\n",
      "step:106/1750 train_time:16153ms step_avg:152.39ms tokens_processed:5210112\n",
      "step:107/1750 train_time:16309ms step_avg:152.42ms tokens_processed:5259264\n",
      "step:108/1750 train_time:16461ms step_avg:152.42ms tokens_processed:5308416\n",
      "step:109/1750 train_time:16614ms step_avg:152.42ms tokens_processed:5357568\n",
      "step:110/1750 train_time:16767ms step_avg:152.43ms tokens_processed:5406720\n",
      "step:111/1750 train_time:16921ms step_avg:152.44ms tokens_processed:5455872\n",
      "step:112/1750 train_time:17074ms step_avg:152.45ms tokens_processed:5505024\n",
      "step:113/1750 train_time:17229ms step_avg:152.47ms tokens_processed:5554176\n",
      "step:114/1750 train_time:17383ms step_avg:152.49ms tokens_processed:5603328\n",
      "step:115/1750 train_time:17537ms step_avg:152.49ms tokens_processed:5652480\n",
      "step:116/1750 train_time:17690ms step_avg:152.50ms tokens_processed:5701632\n",
      "step:117/1750 train_time:17844ms step_avg:152.51ms tokens_processed:5750784\n",
      "step:118/1750 train_time:17996ms step_avg:152.51ms tokens_processed:5799936\n",
      "step:119/1750 train_time:18151ms step_avg:152.53ms tokens_processed:5849088\n",
      "step:120/1750 train_time:18306ms step_avg:152.55ms tokens_processed:5898240\n",
      "step:121/1750 train_time:18458ms step_avg:152.55ms tokens_processed:5947392\n",
      "step:122/1750 train_time:18611ms step_avg:152.55ms tokens_processed:5996544\n",
      "step:123/1750 train_time:18765ms step_avg:152.56ms tokens_processed:6045696\n",
      "step:124/1750 train_time:18918ms step_avg:152.56ms tokens_processed:6094848\n",
      "step:125/1750 train_time:19072ms step_avg:152.58ms tokens_processed:6144000\n",
      "step:126/1750 train_time:19226ms step_avg:152.58ms tokens_processed:6193152\n",
      "step:127/1750 train_time:19380ms step_avg:152.60ms tokens_processed:6242304\n",
      "step:128/1750 train_time:19534ms step_avg:152.61ms tokens_processed:6291456\n",
      "step:129/1750 train_time:19688ms step_avg:152.62ms tokens_processed:6340608\n",
      "step:130/1750 train_time:19842ms step_avg:152.63ms tokens_processed:6389760\n",
      "step:131/1750 train_time:19995ms step_avg:152.63ms tokens_processed:6438912\n",
      "step:132/1750 train_time:20149ms step_avg:152.64ms tokens_processed:6488064\n",
      "step:133/1750 train_time:20302ms step_avg:152.64ms tokens_processed:6537216\n",
      "step:134/1750 train_time:20456ms step_avg:152.66ms tokens_processed:6586368\n",
      "step:135/1750 train_time:20609ms step_avg:152.66ms tokens_processed:6635520\n",
      "step:136/1750 train_time:20762ms step_avg:152.66ms tokens_processed:6684672\n",
      "step:137/1750 train_time:20914ms step_avg:152.66ms tokens_processed:6733824\n",
      "step:138/1750 train_time:21068ms step_avg:152.66ms tokens_processed:6782976\n",
      "step:139/1750 train_time:21222ms step_avg:152.68ms tokens_processed:6832128\n",
      "step:140/1750 train_time:21376ms step_avg:152.69ms tokens_processed:6881280\n",
      "step:141/1750 train_time:21530ms step_avg:152.69ms tokens_processed:6930432\n",
      "step:142/1750 train_time:21684ms step_avg:152.71ms tokens_processed:6979584\n",
      "step:143/1750 train_time:21836ms step_avg:152.70ms tokens_processed:7028736\n",
      "step:144/1750 train_time:21990ms step_avg:152.71ms tokens_processed:7077888\n",
      "step:145/1750 train_time:22145ms step_avg:152.72ms tokens_processed:7127040\n",
      "step:146/1750 train_time:22298ms step_avg:152.73ms tokens_processed:7176192\n",
      "step:147/1750 train_time:22454ms step_avg:152.74ms tokens_processed:7225344\n",
      "step:148/1750 train_time:22609ms step_avg:152.76ms tokens_processed:7274496\n",
      "step:149/1750 train_time:22764ms step_avg:152.78ms tokens_processed:7323648\n",
      "step:150/1750 train_time:22916ms step_avg:152.77ms tokens_processed:7372800\n",
      "step:151/1750 train_time:23070ms step_avg:152.78ms tokens_processed:7421952\n",
      "step:152/1750 train_time:23223ms step_avg:152.78ms tokens_processed:7471104\n",
      "step:153/1750 train_time:23378ms step_avg:152.80ms tokens_processed:7520256\n",
      "step:154/1750 train_time:23531ms step_avg:152.80ms tokens_processed:7569408\n",
      "step:155/1750 train_time:23687ms step_avg:152.82ms tokens_processed:7618560\n",
      "step:156/1750 train_time:23840ms step_avg:152.82ms tokens_processed:7667712\n",
      "step:157/1750 train_time:23994ms step_avg:152.83ms tokens_processed:7716864\n",
      "step:158/1750 train_time:24148ms step_avg:152.83ms tokens_processed:7766016\n",
      "step:159/1750 train_time:24301ms step_avg:152.84ms tokens_processed:7815168\n",
      "step:160/1750 train_time:24456ms step_avg:152.85ms tokens_processed:7864320\n",
      "step:161/1750 train_time:24609ms step_avg:152.85ms tokens_processed:7913472\n",
      "step:162/1750 train_time:24762ms step_avg:152.85ms tokens_processed:7962624\n",
      "step:163/1750 train_time:24915ms step_avg:152.85ms tokens_processed:8011776\n",
      "step:164/1750 train_time:25069ms step_avg:152.86ms tokens_processed:8060928\n",
      "step:165/1750 train_time:25222ms step_avg:152.86ms tokens_processed:8110080\n",
      "step:166/1750 train_time:25378ms step_avg:152.88ms tokens_processed:8159232\n",
      "step:167/1750 train_time:25532ms step_avg:152.89ms tokens_processed:8208384\n",
      "step:168/1750 train_time:25687ms step_avg:152.90ms tokens_processed:8257536\n",
      "step:169/1750 train_time:25840ms step_avg:152.90ms tokens_processed:8306688\n",
      "step:170/1750 train_time:25994ms step_avg:152.90ms tokens_processed:8355840\n",
      "step:171/1750 train_time:26148ms step_avg:152.91ms tokens_processed:8404992\n",
      "step:172/1750 train_time:26301ms step_avg:152.91ms tokens_processed:8454144\n",
      "step:173/1750 train_time:26456ms step_avg:152.93ms tokens_processed:8503296\n",
      "step:174/1750 train_time:26610ms step_avg:152.93ms tokens_processed:8552448\n",
      "step:175/1750 train_time:26764ms step_avg:152.94ms tokens_processed:8601600\n",
      "step:176/1750 train_time:26916ms step_avg:152.93ms tokens_processed:8650752\n",
      "step:177/1750 train_time:27071ms step_avg:152.95ms tokens_processed:8699904\n",
      "step:178/1750 train_time:27225ms step_avg:152.95ms tokens_processed:8749056\n",
      "step:179/1750 train_time:27379ms step_avg:152.95ms tokens_processed:8798208\n",
      "step:180/1750 train_time:27533ms step_avg:152.96ms tokens_processed:8847360\n",
      "step:181/1750 train_time:27688ms step_avg:152.97ms tokens_processed:8896512\n",
      "step:182/1750 train_time:27840ms step_avg:152.97ms tokens_processed:8945664\n",
      "step:183/1750 train_time:27994ms step_avg:152.97ms tokens_processed:8994816\n",
      "step:184/1750 train_time:28150ms step_avg:152.99ms tokens_processed:9043968\n",
      "step:185/1750 train_time:28304ms step_avg:153.00ms tokens_processed:9093120\n",
      "step:186/1750 train_time:28457ms step_avg:153.00ms tokens_processed:9142272\n",
      "step:187/1750 train_time:28611ms step_avg:153.00ms tokens_processed:9191424\n",
      "step:188/1750 train_time:28765ms step_avg:153.01ms tokens_processed:9240576\n",
      "step:189/1750 train_time:28918ms step_avg:153.00ms tokens_processed:9289728\n",
      "step:190/1750 train_time:29072ms step_avg:153.01ms tokens_processed:9338880\n",
      "step:191/1750 train_time:29226ms step_avg:153.01ms tokens_processed:9388032\n",
      "step:192/1750 train_time:29380ms step_avg:153.02ms tokens_processed:9437184\n",
      "step:193/1750 train_time:29534ms step_avg:153.03ms tokens_processed:9486336\n",
      "step:194/1750 train_time:29689ms step_avg:153.04ms tokens_processed:9535488\n",
      "step:195/1750 train_time:29842ms step_avg:153.04ms tokens_processed:9584640\n",
      "step:196/1750 train_time:29995ms step_avg:153.04ms tokens_processed:9633792\n",
      "step:197/1750 train_time:30151ms step_avg:153.05ms tokens_processed:9682944\n",
      "step:198/1750 train_time:30307ms step_avg:153.07ms tokens_processed:9732096\n",
      "step:199/1750 train_time:30461ms step_avg:153.07ms tokens_processed:9781248\n",
      "step:200/1750 train_time:30613ms step_avg:153.07ms tokens_processed:9830400\n",
      "---validation---\n",
      "step:200/1750 val_loss:5.4818 train_time:30621ms step_avg:153.10ms\n",
      "---end of validation---\n",
      "step:201/1750 train_time:30774ms step_avg:153.10ms tokens_processed:9879552\n",
      "step:202/1750 train_time:30927ms step_avg:153.11ms tokens_processed:9928704\n",
      "step:203/1750 train_time:31081ms step_avg:153.11ms tokens_processed:9977856\n",
      "step:204/1750 train_time:31235ms step_avg:153.11ms tokens_processed:10027008\n",
      "step:205/1750 train_time:31391ms step_avg:153.12ms tokens_processed:10076160\n",
      "step:206/1750 train_time:31544ms step_avg:153.13ms tokens_processed:10125312\n",
      "step:207/1750 train_time:31699ms step_avg:153.13ms tokens_processed:10174464\n",
      "step:208/1750 train_time:31854ms step_avg:153.14ms tokens_processed:10223616\n",
      "step:209/1750 train_time:32008ms step_avg:153.15ms tokens_processed:10272768\n",
      "step:210/1750 train_time:32162ms step_avg:153.15ms tokens_processed:10321920\n",
      "step:211/1750 train_time:32317ms step_avg:153.16ms tokens_processed:10371072\n",
      "step:212/1750 train_time:32472ms step_avg:153.17ms tokens_processed:10420224\n",
      "step:213/1750 train_time:32627ms step_avg:153.18ms tokens_processed:10469376\n",
      "step:214/1750 train_time:32780ms step_avg:153.18ms tokens_processed:10518528\n",
      "step:215/1750 train_time:32936ms step_avg:153.19ms tokens_processed:10567680\n",
      "step:216/1750 train_time:33089ms step_avg:153.19ms tokens_processed:10616832\n",
      "step:217/1750 train_time:33243ms step_avg:153.19ms tokens_processed:10665984\n",
      "step:218/1750 train_time:33398ms step_avg:153.20ms tokens_processed:10715136\n",
      "step:219/1750 train_time:33553ms step_avg:153.21ms tokens_processed:10764288\n",
      "step:220/1750 train_time:33707ms step_avg:153.21ms tokens_processed:10813440\n",
      "step:221/1750 train_time:33863ms step_avg:153.22ms tokens_processed:10862592\n",
      "step:222/1750 train_time:34018ms step_avg:153.23ms tokens_processed:10911744\n",
      "step:223/1750 train_time:34172ms step_avg:153.24ms tokens_processed:10960896\n",
      "step:224/1750 train_time:34328ms step_avg:153.25ms tokens_processed:11010048\n",
      "step:225/1750 train_time:34481ms step_avg:153.25ms tokens_processed:11059200\n",
      "step:226/1750 train_time:34637ms step_avg:153.26ms tokens_processed:11108352\n",
      "step:227/1750 train_time:34792ms step_avg:153.27ms tokens_processed:11157504\n",
      "step:228/1750 train_time:34949ms step_avg:153.29ms tokens_processed:11206656\n",
      "step:229/1750 train_time:35101ms step_avg:153.28ms tokens_processed:11255808\n",
      "step:230/1750 train_time:35258ms step_avg:153.30ms tokens_processed:11304960\n",
      "step:231/1750 train_time:35410ms step_avg:153.29ms tokens_processed:11354112\n",
      "step:232/1750 train_time:35565ms step_avg:153.30ms tokens_processed:11403264\n",
      "step:233/1750 train_time:35720ms step_avg:153.30ms tokens_processed:11452416\n",
      "step:234/1750 train_time:35874ms step_avg:153.31ms tokens_processed:11501568\n",
      "step:235/1750 train_time:36029ms step_avg:153.31ms tokens_processed:11550720\n",
      "step:236/1750 train_time:36184ms step_avg:153.32ms tokens_processed:11599872\n",
      "step:237/1750 train_time:36338ms step_avg:153.32ms tokens_processed:11649024\n",
      "step:238/1750 train_time:36492ms step_avg:153.33ms tokens_processed:11698176\n",
      "step:239/1750 train_time:36645ms step_avg:153.33ms tokens_processed:11747328\n",
      "step:240/1750 train_time:36798ms step_avg:153.32ms tokens_processed:11796480\n",
      "step:241/1750 train_time:36951ms step_avg:153.32ms tokens_processed:11845632\n",
      "step:242/1750 train_time:37104ms step_avg:153.32ms tokens_processed:11894784\n",
      "step:243/1750 train_time:37257ms step_avg:153.32ms tokens_processed:11943936\n",
      "step:244/1750 train_time:37411ms step_avg:153.32ms tokens_processed:11993088\n",
      "step:245/1750 train_time:37565ms step_avg:153.33ms tokens_processed:12042240\n",
      "step:246/1750 train_time:37718ms step_avg:153.32ms tokens_processed:12091392\n",
      "step:247/1750 train_time:37872ms step_avg:153.33ms tokens_processed:12140544\n",
      "step:248/1750 train_time:38025ms step_avg:153.32ms tokens_processed:12189696\n",
      "step:249/1750 train_time:38178ms step_avg:153.33ms tokens_processed:12238848\n",
      "step:250/1750 train_time:38333ms step_avg:153.33ms tokens_processed:12288000\n",
      "step:251/1750 train_time:38485ms step_avg:153.33ms tokens_processed:12337152\n",
      "step:252/1750 train_time:38639ms step_avg:153.33ms tokens_processed:12386304\n",
      "step:253/1750 train_time:38792ms step_avg:153.33ms tokens_processed:12435456\n",
      "step:254/1750 train_time:38945ms step_avg:153.33ms tokens_processed:12484608\n",
      "step:255/1750 train_time:39098ms step_avg:153.33ms tokens_processed:12533760\n",
      "step:256/1750 train_time:39252ms step_avg:153.33ms tokens_processed:12582912\n",
      "step:257/1750 train_time:39405ms step_avg:153.32ms tokens_processed:12632064\n",
      "step:258/1750 train_time:39559ms step_avg:153.33ms tokens_processed:12681216\n",
      "step:259/1750 train_time:39713ms step_avg:153.33ms tokens_processed:12730368\n",
      "step:260/1750 train_time:39867ms step_avg:153.34ms tokens_processed:12779520\n",
      "step:261/1750 train_time:40020ms step_avg:153.33ms tokens_processed:12828672\n",
      "step:262/1750 train_time:40174ms step_avg:153.34ms tokens_processed:12877824\n",
      "step:263/1750 train_time:40328ms step_avg:153.34ms tokens_processed:12926976\n",
      "step:264/1750 train_time:40481ms step_avg:153.34ms tokens_processed:12976128\n",
      "step:265/1750 train_time:40635ms step_avg:153.34ms tokens_processed:13025280\n",
      "step:266/1750 train_time:40789ms step_avg:153.34ms tokens_processed:13074432\n",
      "step:267/1750 train_time:40942ms step_avg:153.34ms tokens_processed:13123584\n",
      "step:268/1750 train_time:41096ms step_avg:153.34ms tokens_processed:13172736\n",
      "step:269/1750 train_time:41250ms step_avg:153.34ms tokens_processed:13221888\n",
      "step:270/1750 train_time:41403ms step_avg:153.35ms tokens_processed:13271040\n",
      "step:271/1750 train_time:41557ms step_avg:153.35ms tokens_processed:13320192\n",
      "step:272/1750 train_time:41711ms step_avg:153.35ms tokens_processed:13369344\n",
      "step:273/1750 train_time:41864ms step_avg:153.35ms tokens_processed:13418496\n",
      "step:274/1750 train_time:42017ms step_avg:153.35ms tokens_processed:13467648\n",
      "step:275/1750 train_time:42176ms step_avg:153.37ms tokens_processed:13516800\n",
      "step:276/1750 train_time:42330ms step_avg:153.37ms tokens_processed:13565952\n",
      "step:277/1750 train_time:42483ms step_avg:153.37ms tokens_processed:13615104\n",
      "step:278/1750 train_time:42638ms step_avg:153.37ms tokens_processed:13664256\n",
      "step:279/1750 train_time:42792ms step_avg:153.38ms tokens_processed:13713408\n",
      "step:280/1750 train_time:42944ms step_avg:153.37ms tokens_processed:13762560\n",
      "step:281/1750 train_time:43097ms step_avg:153.37ms tokens_processed:13811712\n",
      "step:282/1750 train_time:43252ms step_avg:153.38ms tokens_processed:13860864\n",
      "step:283/1750 train_time:43405ms step_avg:153.37ms tokens_processed:13910016\n",
      "step:284/1750 train_time:43559ms step_avg:153.38ms tokens_processed:13959168\n",
      "step:285/1750 train_time:43713ms step_avg:153.38ms tokens_processed:14008320\n",
      "step:286/1750 train_time:43867ms step_avg:153.38ms tokens_processed:14057472\n",
      "step:287/1750 train_time:44019ms step_avg:153.38ms tokens_processed:14106624\n",
      "step:288/1750 train_time:44172ms step_avg:153.37ms tokens_processed:14155776\n",
      "step:289/1750 train_time:44326ms step_avg:153.38ms tokens_processed:14204928\n",
      "step:290/1750 train_time:44478ms step_avg:153.37ms tokens_processed:14254080\n",
      "step:291/1750 train_time:44633ms step_avg:153.38ms tokens_processed:14303232\n",
      "step:292/1750 train_time:44787ms step_avg:153.38ms tokens_processed:14352384\n",
      "step:293/1750 train_time:44939ms step_avg:153.38ms tokens_processed:14401536\n",
      "step:294/1750 train_time:45093ms step_avg:153.38ms tokens_processed:14450688\n",
      "step:295/1750 train_time:45246ms step_avg:153.38ms tokens_processed:14499840\n",
      "step:296/1750 train_time:45400ms step_avg:153.38ms tokens_processed:14548992\n",
      "step:297/1750 train_time:45553ms step_avg:153.38ms tokens_processed:14598144\n",
      "step:298/1750 train_time:45708ms step_avg:153.38ms tokens_processed:14647296\n",
      "step:299/1750 train_time:45860ms step_avg:153.38ms tokens_processed:14696448\n",
      "step:300/1750 train_time:46014ms step_avg:153.38ms tokens_processed:14745600\n",
      "---validation---\n",
      "step:300/1750 val_loss:5.2029 train_time:46022ms step_avg:153.41ms\n",
      "---end of validation---\n",
      "step:301/1750 train_time:46175ms step_avg:153.41ms tokens_processed:14794752\n",
      "step:302/1750 train_time:46329ms step_avg:153.41ms tokens_processed:14843904\n",
      "step:303/1750 train_time:46484ms step_avg:153.41ms tokens_processed:14893056\n",
      "step:304/1750 train_time:46636ms step_avg:153.41ms tokens_processed:14942208\n",
      "step:305/1750 train_time:46789ms step_avg:153.41ms tokens_processed:14991360\n",
      "step:306/1750 train_time:46942ms step_avg:153.40ms tokens_processed:15040512\n",
      "step:307/1750 train_time:47096ms step_avg:153.41ms tokens_processed:15089664\n",
      "step:308/1750 train_time:47250ms step_avg:153.41ms tokens_processed:15138816\n",
      "step:309/1750 train_time:47404ms step_avg:153.41ms tokens_processed:15187968\n",
      "step:310/1750 train_time:47559ms step_avg:153.42ms tokens_processed:15237120\n",
      "step:311/1750 train_time:47714ms step_avg:153.42ms tokens_processed:15286272\n",
      "step:312/1750 train_time:47867ms step_avg:153.42ms tokens_processed:15335424\n",
      "step:313/1750 train_time:48020ms step_avg:153.42ms tokens_processed:15384576\n",
      "step:314/1750 train_time:48175ms step_avg:153.42ms tokens_processed:15433728\n",
      "step:315/1750 train_time:48327ms step_avg:153.42ms tokens_processed:15482880\n",
      "step:316/1750 train_time:48482ms step_avg:153.42ms tokens_processed:15532032\n",
      "step:317/1750 train_time:48637ms step_avg:153.43ms tokens_processed:15581184\n",
      "step:318/1750 train_time:48791ms step_avg:153.43ms tokens_processed:15630336\n",
      "step:319/1750 train_time:48946ms step_avg:153.44ms tokens_processed:15679488\n",
      "step:320/1750 train_time:49100ms step_avg:153.44ms tokens_processed:15728640\n",
      "step:321/1750 train_time:49255ms step_avg:153.44ms tokens_processed:15777792\n",
      "step:322/1750 train_time:49408ms step_avg:153.44ms tokens_processed:15826944\n",
      "step:323/1750 train_time:49562ms step_avg:153.44ms tokens_processed:15876096\n",
      "step:324/1750 train_time:49716ms step_avg:153.45ms tokens_processed:15925248\n",
      "step:325/1750 train_time:49869ms step_avg:153.44ms tokens_processed:15974400\n",
      "step:326/1750 train_time:50023ms step_avg:153.44ms tokens_processed:16023552\n",
      "step:327/1750 train_time:50176ms step_avg:153.44ms tokens_processed:16072704\n",
      "step:328/1750 train_time:50327ms step_avg:153.44ms tokens_processed:16121856\n",
      "step:329/1750 train_time:50482ms step_avg:153.44ms tokens_processed:16171008\n",
      "step:330/1750 train_time:50635ms step_avg:153.44ms tokens_processed:16220160\n",
      "step:331/1750 train_time:50789ms step_avg:153.44ms tokens_processed:16269312\n",
      "step:332/1750 train_time:50943ms step_avg:153.44ms tokens_processed:16318464\n",
      "step:333/1750 train_time:51097ms step_avg:153.45ms tokens_processed:16367616\n",
      "step:334/1750 train_time:51250ms step_avg:153.44ms tokens_processed:16416768\n",
      "step:335/1750 train_time:51402ms step_avg:153.44ms tokens_processed:16465920\n",
      "step:336/1750 train_time:51557ms step_avg:153.44ms tokens_processed:16515072\n",
      "step:337/1750 train_time:51711ms step_avg:153.45ms tokens_processed:16564224\n",
      "step:338/1750 train_time:51864ms step_avg:153.44ms tokens_processed:16613376\n",
      "step:339/1750 train_time:52019ms step_avg:153.45ms tokens_processed:16662528\n",
      "step:340/1750 train_time:52173ms step_avg:153.45ms tokens_processed:16711680\n",
      "step:341/1750 train_time:52325ms step_avg:153.45ms tokens_processed:16760832\n",
      "step:342/1750 train_time:52480ms step_avg:153.45ms tokens_processed:16809984\n",
      "step:343/1750 train_time:52635ms step_avg:153.45ms tokens_processed:16859136\n",
      "step:344/1750 train_time:52788ms step_avg:153.45ms tokens_processed:16908288\n",
      "step:345/1750 train_time:52943ms step_avg:153.46ms tokens_processed:16957440\n",
      "step:346/1750 train_time:53097ms step_avg:153.46ms tokens_processed:17006592\n",
      "step:347/1750 train_time:53251ms step_avg:153.46ms tokens_processed:17055744\n",
      "step:348/1750 train_time:53404ms step_avg:153.46ms tokens_processed:17104896\n",
      "step:349/1750 train_time:53558ms step_avg:153.46ms tokens_processed:17154048\n",
      "step:350/1750 train_time:53713ms step_avg:153.46ms tokens_processed:17203200\n",
      "step:351/1750 train_time:53866ms step_avg:153.46ms tokens_processed:17252352\n",
      "step:352/1750 train_time:54021ms step_avg:153.47ms tokens_processed:17301504\n",
      "step:353/1750 train_time:54175ms step_avg:153.47ms tokens_processed:17350656\n",
      "step:354/1750 train_time:54328ms step_avg:153.47ms tokens_processed:17399808\n",
      "step:355/1750 train_time:54482ms step_avg:153.47ms tokens_processed:17448960\n",
      "step:356/1750 train_time:54637ms step_avg:153.47ms tokens_processed:17498112\n",
      "step:357/1750 train_time:54790ms step_avg:153.47ms tokens_processed:17547264\n",
      "step:358/1750 train_time:54944ms step_avg:153.47ms tokens_processed:17596416\n",
      "step:359/1750 train_time:55098ms step_avg:153.48ms tokens_processed:17645568\n",
      "step:360/1750 train_time:55252ms step_avg:153.48ms tokens_processed:17694720\n",
      "step:361/1750 train_time:55407ms step_avg:153.48ms tokens_processed:17743872\n",
      "step:362/1750 train_time:55560ms step_avg:153.48ms tokens_processed:17793024\n",
      "step:363/1750 train_time:55716ms step_avg:153.49ms tokens_processed:17842176\n",
      "step:364/1750 train_time:55870ms step_avg:153.49ms tokens_processed:17891328\n",
      "step:365/1750 train_time:56022ms step_avg:153.49ms tokens_processed:17940480\n",
      "step:366/1750 train_time:56177ms step_avg:153.49ms tokens_processed:17989632\n",
      "step:367/1750 train_time:56329ms step_avg:153.49ms tokens_processed:18038784\n",
      "step:368/1750 train_time:56484ms step_avg:153.49ms tokens_processed:18087936\n",
      "step:369/1750 train_time:56638ms step_avg:153.49ms tokens_processed:18137088\n",
      "step:370/1750 train_time:56792ms step_avg:153.49ms tokens_processed:18186240\n",
      "step:371/1750 train_time:56945ms step_avg:153.49ms tokens_processed:18235392\n",
      "step:372/1750 train_time:57100ms step_avg:153.49ms tokens_processed:18284544\n",
      "step:373/1750 train_time:57256ms step_avg:153.50ms tokens_processed:18333696\n",
      "step:374/1750 train_time:57408ms step_avg:153.50ms tokens_processed:18382848\n",
      "step:375/1750 train_time:57562ms step_avg:153.50ms tokens_processed:18432000\n",
      "step:376/1750 train_time:57716ms step_avg:153.50ms tokens_processed:18481152\n",
      "step:377/1750 train_time:57870ms step_avg:153.50ms tokens_processed:18530304\n",
      "step:378/1750 train_time:58023ms step_avg:153.50ms tokens_processed:18579456\n",
      "step:379/1750 train_time:58177ms step_avg:153.50ms tokens_processed:18628608\n",
      "step:380/1750 train_time:58331ms step_avg:153.50ms tokens_processed:18677760\n",
      "step:381/1750 train_time:58485ms step_avg:153.50ms tokens_processed:18726912\n",
      "step:382/1750 train_time:58638ms step_avg:153.50ms tokens_processed:18776064\n",
      "step:383/1750 train_time:58793ms step_avg:153.51ms tokens_processed:18825216\n",
      "step:384/1750 train_time:58947ms step_avg:153.51ms tokens_processed:18874368\n",
      "step:385/1750 train_time:59101ms step_avg:153.51ms tokens_processed:18923520\n",
      "step:386/1750 train_time:59256ms step_avg:153.51ms tokens_processed:18972672\n",
      "step:387/1750 train_time:59409ms step_avg:153.51ms tokens_processed:19021824\n",
      "step:388/1750 train_time:59563ms step_avg:153.51ms tokens_processed:19070976\n",
      "step:389/1750 train_time:59718ms step_avg:153.52ms tokens_processed:19120128\n",
      "step:390/1750 train_time:59872ms step_avg:153.52ms tokens_processed:19169280\n",
      "step:391/1750 train_time:60025ms step_avg:153.52ms tokens_processed:19218432\n",
      "step:392/1750 train_time:60180ms step_avg:153.52ms tokens_processed:19267584\n",
      "step:393/1750 train_time:60333ms step_avg:153.52ms tokens_processed:19316736\n",
      "step:394/1750 train_time:60487ms step_avg:153.52ms tokens_processed:19365888\n",
      "step:395/1750 train_time:60640ms step_avg:153.52ms tokens_processed:19415040\n",
      "step:396/1750 train_time:60795ms step_avg:153.52ms tokens_processed:19464192\n",
      "step:397/1750 train_time:60948ms step_avg:153.52ms tokens_processed:19513344\n",
      "step:398/1750 train_time:61102ms step_avg:153.52ms tokens_processed:19562496\n",
      "step:399/1750 train_time:61257ms step_avg:153.53ms tokens_processed:19611648\n",
      "step:400/1750 train_time:61410ms step_avg:153.53ms tokens_processed:19660800\n",
      "---validation---\n",
      "step:400/1750 val_loss:5.0204 train_time:61417ms step_avg:153.54ms\n",
      "---end of validation---\n",
      "step:401/1750 train_time:61569ms step_avg:153.54ms tokens_processed:19709952\n",
      "step:402/1750 train_time:61721ms step_avg:153.54ms tokens_processed:19759104\n",
      "step:403/1750 train_time:61875ms step_avg:153.54ms tokens_processed:19808256\n",
      "step:404/1750 train_time:62029ms step_avg:153.54ms tokens_processed:19857408\n",
      "step:405/1750 train_time:62182ms step_avg:153.54ms tokens_processed:19906560\n",
      "step:406/1750 train_time:62336ms step_avg:153.54ms tokens_processed:19955712\n",
      "step:407/1750 train_time:62489ms step_avg:153.54ms tokens_processed:20004864\n",
      "step:408/1750 train_time:62644ms step_avg:153.54ms tokens_processed:20054016\n",
      "step:409/1750 train_time:62799ms step_avg:153.54ms tokens_processed:20103168\n",
      "step:410/1750 train_time:62951ms step_avg:153.54ms tokens_processed:20152320\n",
      "step:411/1750 train_time:63105ms step_avg:153.54ms tokens_processed:20201472\n",
      "step:412/1750 train_time:63259ms step_avg:153.54ms tokens_processed:20250624\n",
      "step:413/1750 train_time:63412ms step_avg:153.54ms tokens_processed:20299776\n",
      "step:414/1750 train_time:63566ms step_avg:153.54ms tokens_processed:20348928\n",
      "step:415/1750 train_time:63720ms step_avg:153.54ms tokens_processed:20398080\n",
      "step:416/1750 train_time:63874ms step_avg:153.54ms tokens_processed:20447232\n",
      "step:417/1750 train_time:64028ms step_avg:153.54ms tokens_processed:20496384\n",
      "step:418/1750 train_time:64182ms step_avg:153.55ms tokens_processed:20545536\n",
      "step:419/1750 train_time:64335ms step_avg:153.54ms tokens_processed:20594688\n",
      "step:420/1750 train_time:64489ms step_avg:153.54ms tokens_processed:20643840\n",
      "step:421/1750 train_time:64643ms step_avg:153.55ms tokens_processed:20692992\n",
      "step:422/1750 train_time:64798ms step_avg:153.55ms tokens_processed:20742144\n",
      "step:423/1750 train_time:64952ms step_avg:153.55ms tokens_processed:20791296\n",
      "step:424/1750 train_time:65106ms step_avg:153.55ms tokens_processed:20840448\n",
      "step:425/1750 train_time:65260ms step_avg:153.55ms tokens_processed:20889600\n",
      "step:426/1750 train_time:65412ms step_avg:153.55ms tokens_processed:20938752\n",
      "step:427/1750 train_time:65567ms step_avg:153.55ms tokens_processed:20987904\n",
      "step:428/1750 train_time:65723ms step_avg:153.56ms tokens_processed:21037056\n",
      "step:429/1750 train_time:65876ms step_avg:153.56ms tokens_processed:21086208\n",
      "step:430/1750 train_time:66030ms step_avg:153.56ms tokens_processed:21135360\n",
      "step:431/1750 train_time:66184ms step_avg:153.56ms tokens_processed:21184512\n",
      "step:432/1750 train_time:66340ms step_avg:153.56ms tokens_processed:21233664\n",
      "step:433/1750 train_time:66493ms step_avg:153.56ms tokens_processed:21282816\n",
      "step:434/1750 train_time:66647ms step_avg:153.56ms tokens_processed:21331968\n",
      "step:435/1750 train_time:66802ms step_avg:153.57ms tokens_processed:21381120\n",
      "step:436/1750 train_time:66956ms step_avg:153.57ms tokens_processed:21430272\n",
      "step:437/1750 train_time:67109ms step_avg:153.57ms tokens_processed:21479424\n",
      "step:438/1750 train_time:67264ms step_avg:153.57ms tokens_processed:21528576\n",
      "step:439/1750 train_time:67418ms step_avg:153.57ms tokens_processed:21577728\n",
      "step:440/1750 train_time:67571ms step_avg:153.57ms tokens_processed:21626880\n",
      "step:441/1750 train_time:67726ms step_avg:153.57ms tokens_processed:21676032\n",
      "step:442/1750 train_time:67881ms step_avg:153.58ms tokens_processed:21725184\n",
      "step:443/1750 train_time:68035ms step_avg:153.58ms tokens_processed:21774336\n",
      "step:444/1750 train_time:68189ms step_avg:153.58ms tokens_processed:21823488\n",
      "step:445/1750 train_time:68344ms step_avg:153.58ms tokens_processed:21872640\n",
      "step:446/1750 train_time:68498ms step_avg:153.58ms tokens_processed:21921792\n",
      "step:447/1750 train_time:68650ms step_avg:153.58ms tokens_processed:21970944\n",
      "step:448/1750 train_time:68805ms step_avg:153.58ms tokens_processed:22020096\n",
      "step:449/1750 train_time:68959ms step_avg:153.58ms tokens_processed:22069248\n",
      "step:450/1750 train_time:69112ms step_avg:153.58ms tokens_processed:22118400\n",
      "step:451/1750 train_time:69267ms step_avg:153.59ms tokens_processed:22167552\n",
      "step:452/1750 train_time:69422ms step_avg:153.59ms tokens_processed:22216704\n",
      "step:453/1750 train_time:69576ms step_avg:153.59ms tokens_processed:22265856\n",
      "step:454/1750 train_time:69729ms step_avg:153.59ms tokens_processed:22315008\n",
      "step:455/1750 train_time:69883ms step_avg:153.59ms tokens_processed:22364160\n",
      "step:456/1750 train_time:70038ms step_avg:153.59ms tokens_processed:22413312\n",
      "step:457/1750 train_time:70191ms step_avg:153.59ms tokens_processed:22462464\n",
      "step:458/1750 train_time:70347ms step_avg:153.60ms tokens_processed:22511616\n",
      "step:459/1750 train_time:70501ms step_avg:153.60ms tokens_processed:22560768\n",
      "step:460/1750 train_time:70655ms step_avg:153.60ms tokens_processed:22609920\n",
      "step:461/1750 train_time:70807ms step_avg:153.59ms tokens_processed:22659072\n",
      "step:462/1750 train_time:70962ms step_avg:153.60ms tokens_processed:22708224\n",
      "step:463/1750 train_time:71114ms step_avg:153.59ms tokens_processed:22757376\n",
      "step:464/1750 train_time:71269ms step_avg:153.60ms tokens_processed:22806528\n",
      "step:465/1750 train_time:71423ms step_avg:153.60ms tokens_processed:22855680\n",
      "step:466/1750 train_time:71577ms step_avg:153.60ms tokens_processed:22904832\n",
      "step:467/1750 train_time:71731ms step_avg:153.60ms tokens_processed:22953984\n",
      "step:468/1750 train_time:71885ms step_avg:153.60ms tokens_processed:23003136\n",
      "step:469/1750 train_time:72040ms step_avg:153.60ms tokens_processed:23052288\n",
      "step:470/1750 train_time:72194ms step_avg:153.60ms tokens_processed:23101440\n",
      "step:471/1750 train_time:72348ms step_avg:153.60ms tokens_processed:23150592\n",
      "step:472/1750 train_time:72503ms step_avg:153.61ms tokens_processed:23199744\n",
      "step:473/1750 train_time:72657ms step_avg:153.61ms tokens_processed:23248896\n",
      "step:474/1750 train_time:72810ms step_avg:153.61ms tokens_processed:23298048\n",
      "step:475/1750 train_time:72964ms step_avg:153.61ms tokens_processed:23347200\n",
      "step:476/1750 train_time:73118ms step_avg:153.61ms tokens_processed:23396352\n",
      "step:477/1750 train_time:73272ms step_avg:153.61ms tokens_processed:23445504\n",
      "step:478/1750 train_time:73425ms step_avg:153.61ms tokens_processed:23494656\n",
      "step:479/1750 train_time:73580ms step_avg:153.61ms tokens_processed:23543808\n",
      "step:480/1750 train_time:73734ms step_avg:153.61ms tokens_processed:23592960\n",
      "step:481/1750 train_time:73888ms step_avg:153.61ms tokens_processed:23642112\n",
      "step:482/1750 train_time:74043ms step_avg:153.62ms tokens_processed:23691264\n",
      "step:483/1750 train_time:74196ms step_avg:153.62ms tokens_processed:23740416\n",
      "step:484/1750 train_time:74350ms step_avg:153.61ms tokens_processed:23789568\n",
      "step:485/1750 train_time:74504ms step_avg:153.62ms tokens_processed:23838720\n",
      "step:486/1750 train_time:74660ms step_avg:153.62ms tokens_processed:23887872\n",
      "step:487/1750 train_time:74813ms step_avg:153.62ms tokens_processed:23937024\n",
      "step:488/1750 train_time:74967ms step_avg:153.62ms tokens_processed:23986176\n",
      "step:489/1750 train_time:75122ms step_avg:153.62ms tokens_processed:24035328\n",
      "step:490/1750 train_time:75275ms step_avg:153.62ms tokens_processed:24084480\n",
      "step:491/1750 train_time:75429ms step_avg:153.62ms tokens_processed:24133632\n",
      "step:492/1750 train_time:75583ms step_avg:153.62ms tokens_processed:24182784\n",
      "step:493/1750 train_time:75738ms step_avg:153.63ms tokens_processed:24231936\n",
      "step:494/1750 train_time:75892ms step_avg:153.63ms tokens_processed:24281088\n",
      "step:495/1750 train_time:76047ms step_avg:153.63ms tokens_processed:24330240\n",
      "step:496/1750 train_time:76203ms step_avg:153.63ms tokens_processed:24379392\n",
      "step:497/1750 train_time:76355ms step_avg:153.63ms tokens_processed:24428544\n",
      "step:498/1750 train_time:76510ms step_avg:153.63ms tokens_processed:24477696\n",
      "step:499/1750 train_time:76663ms step_avg:153.63ms tokens_processed:24526848\n",
      "step:500/1750 train_time:76816ms step_avg:153.63ms tokens_processed:24576000\n",
      "---validation---\n",
      "step:500/1750 val_loss:4.9096 train_time:76823ms step_avg:153.65ms\n",
      "---end of validation---\n",
      "step:501/1750 train_time:76977ms step_avg:153.65ms tokens_processed:24625152\n",
      "step:502/1750 train_time:77131ms step_avg:153.65ms tokens_processed:24674304\n",
      "step:503/1750 train_time:77285ms step_avg:153.65ms tokens_processed:24723456\n",
      "step:504/1750 train_time:77439ms step_avg:153.65ms tokens_processed:24772608\n",
      "step:505/1750 train_time:77593ms step_avg:153.65ms tokens_processed:24821760\n",
      "step:506/1750 train_time:77745ms step_avg:153.65ms tokens_processed:24870912\n",
      "step:507/1750 train_time:77900ms step_avg:153.65ms tokens_processed:24920064\n",
      "step:508/1750 train_time:78054ms step_avg:153.65ms tokens_processed:24969216\n",
      "step:509/1750 train_time:78210ms step_avg:153.65ms tokens_processed:25018368\n",
      "step:510/1750 train_time:78365ms step_avg:153.66ms tokens_processed:25067520\n",
      "step:511/1750 train_time:78518ms step_avg:153.66ms tokens_processed:25116672\n",
      "step:512/1750 train_time:78673ms step_avg:153.66ms tokens_processed:25165824\n",
      "step:513/1750 train_time:78827ms step_avg:153.66ms tokens_processed:25214976\n",
      "step:514/1750 train_time:78980ms step_avg:153.66ms tokens_processed:25264128\n",
      "step:515/1750 train_time:79136ms step_avg:153.66ms tokens_processed:25313280\n",
      "step:516/1750 train_time:79290ms step_avg:153.66ms tokens_processed:25362432\n",
      "step:517/1750 train_time:79443ms step_avg:153.66ms tokens_processed:25411584\n",
      "step:518/1750 train_time:79597ms step_avg:153.66ms tokens_processed:25460736\n",
      "step:519/1750 train_time:79751ms step_avg:153.66ms tokens_processed:25509888\n",
      "step:520/1750 train_time:79906ms step_avg:153.67ms tokens_processed:25559040\n",
      "step:521/1750 train_time:80060ms step_avg:153.67ms tokens_processed:25608192\n",
      "step:522/1750 train_time:80218ms step_avg:153.67ms tokens_processed:25657344\n",
      "step:523/1750 train_time:80371ms step_avg:153.67ms tokens_processed:25706496\n",
      "step:524/1750 train_time:80525ms step_avg:153.67ms tokens_processed:25755648\n",
      "step:525/1750 train_time:80677ms step_avg:153.67ms tokens_processed:25804800\n",
      "step:526/1750 train_time:80831ms step_avg:153.67ms tokens_processed:25853952\n",
      "step:527/1750 train_time:80985ms step_avg:153.67ms tokens_processed:25903104\n",
      "step:528/1750 train_time:81139ms step_avg:153.67ms tokens_processed:25952256\n",
      "step:529/1750 train_time:81293ms step_avg:153.67ms tokens_processed:26001408\n",
      "step:530/1750 train_time:81448ms step_avg:153.68ms tokens_processed:26050560\n",
      "step:531/1750 train_time:81602ms step_avg:153.68ms tokens_processed:26099712\n",
      "step:532/1750 train_time:81756ms step_avg:153.68ms tokens_processed:26148864\n",
      "step:533/1750 train_time:81911ms step_avg:153.68ms tokens_processed:26198016\n",
      "step:534/1750 train_time:82064ms step_avg:153.68ms tokens_processed:26247168\n",
      "step:535/1750 train_time:82219ms step_avg:153.68ms tokens_processed:26296320\n",
      "step:536/1750 train_time:82372ms step_avg:153.68ms tokens_processed:26345472\n",
      "step:537/1750 train_time:82527ms step_avg:153.68ms tokens_processed:26394624\n",
      "step:538/1750 train_time:82681ms step_avg:153.68ms tokens_processed:26443776\n",
      "step:539/1750 train_time:82836ms step_avg:153.69ms tokens_processed:26492928\n",
      "step:540/1750 train_time:82990ms step_avg:153.69ms tokens_processed:26542080\n",
      "step:541/1750 train_time:83145ms step_avg:153.69ms tokens_processed:26591232\n",
      "step:542/1750 train_time:83298ms step_avg:153.69ms tokens_processed:26640384\n",
      "step:543/1750 train_time:83452ms step_avg:153.69ms tokens_processed:26689536\n",
      "step:544/1750 train_time:83607ms step_avg:153.69ms tokens_processed:26738688\n",
      "step:545/1750 train_time:83760ms step_avg:153.69ms tokens_processed:26787840\n",
      "step:546/1750 train_time:83915ms step_avg:153.69ms tokens_processed:26836992\n",
      "step:547/1750 train_time:84070ms step_avg:153.69ms tokens_processed:26886144\n",
      "step:548/1750 train_time:84224ms step_avg:153.69ms tokens_processed:26935296\n",
      "step:549/1750 train_time:84377ms step_avg:153.69ms tokens_processed:26984448\n",
      "step:550/1750 train_time:84531ms step_avg:153.69ms tokens_processed:27033600\n",
      "step:551/1750 train_time:84684ms step_avg:153.69ms tokens_processed:27082752\n",
      "step:552/1750 train_time:84839ms step_avg:153.69ms tokens_processed:27131904\n",
      "step:553/1750 train_time:84992ms step_avg:153.69ms tokens_processed:27181056\n",
      "step:554/1750 train_time:85146ms step_avg:153.69ms tokens_processed:27230208\n",
      "step:555/1750 train_time:85301ms step_avg:153.70ms tokens_processed:27279360\n",
      "step:556/1750 train_time:85457ms step_avg:153.70ms tokens_processed:27328512\n",
      "step:557/1750 train_time:85609ms step_avg:153.70ms tokens_processed:27377664\n",
      "step:558/1750 train_time:85764ms step_avg:153.70ms tokens_processed:27426816\n",
      "step:559/1750 train_time:85917ms step_avg:153.70ms tokens_processed:27475968\n",
      "step:560/1750 train_time:86071ms step_avg:153.70ms tokens_processed:27525120\n",
      "step:561/1750 train_time:86226ms step_avg:153.70ms tokens_processed:27574272\n",
      "step:562/1750 train_time:86379ms step_avg:153.70ms tokens_processed:27623424\n",
      "step:563/1750 train_time:86534ms step_avg:153.70ms tokens_processed:27672576\n",
      "step:564/1750 train_time:86687ms step_avg:153.70ms tokens_processed:27721728\n",
      "step:565/1750 train_time:86845ms step_avg:153.71ms tokens_processed:27770880\n",
      "step:566/1750 train_time:86995ms step_avg:153.70ms tokens_processed:27820032\n",
      "step:567/1750 train_time:87151ms step_avg:153.70ms tokens_processed:27869184\n",
      "step:568/1750 train_time:87304ms step_avg:153.70ms tokens_processed:27918336\n",
      "step:569/1750 train_time:87457ms step_avg:153.70ms tokens_processed:27967488\n",
      "step:570/1750 train_time:87612ms step_avg:153.70ms tokens_processed:28016640\n",
      "step:571/1750 train_time:87766ms step_avg:153.71ms tokens_processed:28065792\n",
      "step:572/1750 train_time:87919ms step_avg:153.70ms tokens_processed:28114944\n",
      "step:573/1750 train_time:88074ms step_avg:153.71ms tokens_processed:28164096\n",
      "step:574/1750 train_time:88228ms step_avg:153.71ms tokens_processed:28213248\n",
      "step:575/1750 train_time:88383ms step_avg:153.71ms tokens_processed:28262400\n",
      "step:576/1750 train_time:88537ms step_avg:153.71ms tokens_processed:28311552\n",
      "step:577/1750 train_time:88691ms step_avg:153.71ms tokens_processed:28360704\n",
      "step:578/1750 train_time:88844ms step_avg:153.71ms tokens_processed:28409856\n",
      "step:579/1750 train_time:88998ms step_avg:153.71ms tokens_processed:28459008\n",
      "step:580/1750 train_time:89152ms step_avg:153.71ms tokens_processed:28508160\n",
      "step:581/1750 train_time:89307ms step_avg:153.71ms tokens_processed:28557312\n",
      "step:582/1750 train_time:89461ms step_avg:153.71ms tokens_processed:28606464\n",
      "step:583/1750 train_time:89616ms step_avg:153.71ms tokens_processed:28655616\n",
      "step:584/1750 train_time:89771ms step_avg:153.72ms tokens_processed:28704768\n",
      "step:585/1750 train_time:89924ms step_avg:153.72ms tokens_processed:28753920\n",
      "step:586/1750 train_time:90078ms step_avg:153.72ms tokens_processed:28803072\n",
      "step:587/1750 train_time:90232ms step_avg:153.72ms tokens_processed:28852224\n",
      "step:588/1750 train_time:90386ms step_avg:153.72ms tokens_processed:28901376\n",
      "step:589/1750 train_time:90540ms step_avg:153.72ms tokens_processed:28950528\n",
      "step:590/1750 train_time:90695ms step_avg:153.72ms tokens_processed:28999680\n",
      "step:591/1750 train_time:90849ms step_avg:153.72ms tokens_processed:29048832\n",
      "step:592/1750 train_time:91004ms step_avg:153.72ms tokens_processed:29097984\n",
      "step:593/1750 train_time:91158ms step_avg:153.72ms tokens_processed:29147136\n",
      "step:594/1750 train_time:91312ms step_avg:153.72ms tokens_processed:29196288\n",
      "step:595/1750 train_time:91467ms step_avg:153.73ms tokens_processed:29245440\n",
      "step:596/1750 train_time:91622ms step_avg:153.73ms tokens_processed:29294592\n",
      "step:597/1750 train_time:91775ms step_avg:153.73ms tokens_processed:29343744\n",
      "step:598/1750 train_time:91930ms step_avg:153.73ms tokens_processed:29392896\n",
      "step:599/1750 train_time:92082ms step_avg:153.73ms tokens_processed:29442048\n",
      "step:600/1750 train_time:92237ms step_avg:153.73ms tokens_processed:29491200\n",
      "---validation---\n",
      "step:600/1750 val_loss:4.8243 train_time:92245ms step_avg:153.74ms\n",
      "---end of validation---\n",
      "step:601/1750 train_time:92397ms step_avg:153.74ms tokens_processed:29540352\n",
      "step:602/1750 train_time:92550ms step_avg:153.74ms tokens_processed:29589504\n",
      "step:603/1750 train_time:92704ms step_avg:153.74ms tokens_processed:29638656\n",
      "step:604/1750 train_time:92857ms step_avg:153.74ms tokens_processed:29687808\n",
      "step:605/1750 train_time:93012ms step_avg:153.74ms tokens_processed:29736960\n",
      "step:606/1750 train_time:93165ms step_avg:153.74ms tokens_processed:29786112\n",
      "step:607/1750 train_time:93319ms step_avg:153.74ms tokens_processed:29835264\n",
      "step:608/1750 train_time:93475ms step_avg:153.74ms tokens_processed:29884416\n",
      "step:609/1750 train_time:93628ms step_avg:153.74ms tokens_processed:29933568\n",
      "step:610/1750 train_time:93782ms step_avg:153.74ms tokens_processed:29982720\n",
      "step:611/1750 train_time:93938ms step_avg:153.74ms tokens_processed:30031872\n",
      "step:612/1750 train_time:94091ms step_avg:153.74ms tokens_processed:30081024\n",
      "step:613/1750 train_time:94246ms step_avg:153.74ms tokens_processed:30130176\n",
      "step:614/1750 train_time:94400ms step_avg:153.75ms tokens_processed:30179328\n",
      "step:615/1750 train_time:94554ms step_avg:153.75ms tokens_processed:30228480\n",
      "step:616/1750 train_time:94709ms step_avg:153.75ms tokens_processed:30277632\n",
      "step:617/1750 train_time:94863ms step_avg:153.75ms tokens_processed:30326784\n",
      "step:618/1750 train_time:95020ms step_avg:153.75ms tokens_processed:30375936\n",
      "step:619/1750 train_time:95172ms step_avg:153.75ms tokens_processed:30425088\n",
      "step:620/1750 train_time:95326ms step_avg:153.75ms tokens_processed:30474240\n",
      "step:621/1750 train_time:95484ms step_avg:153.76ms tokens_processed:30523392\n",
      "step:622/1750 train_time:95636ms step_avg:153.76ms tokens_processed:30572544\n",
      "step:623/1750 train_time:95790ms step_avg:153.76ms tokens_processed:30621696\n",
      "step:624/1750 train_time:95944ms step_avg:153.76ms tokens_processed:30670848\n",
      "step:625/1750 train_time:96099ms step_avg:153.76ms tokens_processed:30720000\n",
      "step:626/1750 train_time:96253ms step_avg:153.76ms tokens_processed:30769152\n",
      "step:627/1750 train_time:96408ms step_avg:153.76ms tokens_processed:30818304\n",
      "step:628/1750 train_time:96561ms step_avg:153.76ms tokens_processed:30867456\n",
      "step:629/1750 train_time:96715ms step_avg:153.76ms tokens_processed:30916608\n",
      "step:630/1750 train_time:96869ms step_avg:153.76ms tokens_processed:30965760\n",
      "step:631/1750 train_time:97024ms step_avg:153.76ms tokens_processed:31014912\n",
      "step:632/1750 train_time:97179ms step_avg:153.76ms tokens_processed:31064064\n",
      "step:633/1750 train_time:97334ms step_avg:153.77ms tokens_processed:31113216\n",
      "step:634/1750 train_time:97487ms step_avg:153.76ms tokens_processed:31162368\n",
      "step:635/1750 train_time:97641ms step_avg:153.77ms tokens_processed:31211520\n",
      "step:636/1750 train_time:97796ms step_avg:153.77ms tokens_processed:31260672\n",
      "step:637/1750 train_time:97948ms step_avg:153.76ms tokens_processed:31309824\n",
      "step:638/1750 train_time:98103ms step_avg:153.77ms tokens_processed:31358976\n",
      "step:639/1750 train_time:98257ms step_avg:153.77ms tokens_processed:31408128\n",
      "step:640/1750 train_time:98412ms step_avg:153.77ms tokens_processed:31457280\n",
      "step:641/1750 train_time:98565ms step_avg:153.77ms tokens_processed:31506432\n",
      "step:642/1750 train_time:98720ms step_avg:153.77ms tokens_processed:31555584\n",
      "step:643/1750 train_time:98874ms step_avg:153.77ms tokens_processed:31604736\n",
      "step:644/1750 train_time:99027ms step_avg:153.77ms tokens_processed:31653888\n",
      "step:645/1750 train_time:99182ms step_avg:153.77ms tokens_processed:31703040\n",
      "step:646/1750 train_time:99338ms step_avg:153.77ms tokens_processed:31752192\n",
      "step:647/1750 train_time:99491ms step_avg:153.77ms tokens_processed:31801344\n",
      "step:648/1750 train_time:99645ms step_avg:153.77ms tokens_processed:31850496\n",
      "step:649/1750 train_time:99799ms step_avg:153.77ms tokens_processed:31899648\n",
      "step:650/1750 train_time:99955ms step_avg:153.78ms tokens_processed:31948800\n",
      "step:651/1750 train_time:100108ms step_avg:153.78ms tokens_processed:31997952\n",
      "step:652/1750 train_time:100263ms step_avg:153.78ms tokens_processed:32047104\n",
      "step:653/1750 train_time:100417ms step_avg:153.78ms tokens_processed:32096256\n",
      "step:654/1750 train_time:100571ms step_avg:153.78ms tokens_processed:32145408\n",
      "step:655/1750 train_time:100725ms step_avg:153.78ms tokens_processed:32194560\n",
      "step:656/1750 train_time:100879ms step_avg:153.78ms tokens_processed:32243712\n",
      "step:657/1750 train_time:101033ms step_avg:153.78ms tokens_processed:32292864\n",
      "step:658/1750 train_time:101186ms step_avg:153.78ms tokens_processed:32342016\n",
      "step:659/1750 train_time:101340ms step_avg:153.78ms tokens_processed:32391168\n",
      "step:660/1750 train_time:101494ms step_avg:153.78ms tokens_processed:32440320\n",
      "step:661/1750 train_time:101647ms step_avg:153.78ms tokens_processed:32489472\n",
      "step:662/1750 train_time:101802ms step_avg:153.78ms tokens_processed:32538624\n",
      "step:663/1750 train_time:101957ms step_avg:153.78ms tokens_processed:32587776\n",
      "step:664/1750 train_time:102111ms step_avg:153.78ms tokens_processed:32636928\n",
      "step:665/1750 train_time:102266ms step_avg:153.78ms tokens_processed:32686080\n",
      "step:666/1750 train_time:102419ms step_avg:153.78ms tokens_processed:32735232\n",
      "step:667/1750 train_time:102574ms step_avg:153.78ms tokens_processed:32784384\n",
      "step:668/1750 train_time:102727ms step_avg:153.78ms tokens_processed:32833536\n",
      "step:669/1750 train_time:102882ms step_avg:153.79ms tokens_processed:32882688\n",
      "step:670/1750 train_time:103036ms step_avg:153.79ms tokens_processed:32931840\n",
      "step:671/1750 train_time:103191ms step_avg:153.79ms tokens_processed:32980992\n",
      "step:672/1750 train_time:103345ms step_avg:153.79ms tokens_processed:33030144\n",
      "step:673/1750 train_time:103499ms step_avg:153.79ms tokens_processed:33079296\n",
      "step:674/1750 train_time:103652ms step_avg:153.79ms tokens_processed:33128448\n",
      "step:675/1750 train_time:103806ms step_avg:153.79ms tokens_processed:33177600\n",
      "step:676/1750 train_time:103960ms step_avg:153.79ms tokens_processed:33226752\n",
      "step:677/1750 train_time:104114ms step_avg:153.79ms tokens_processed:33275904\n",
      "step:678/1750 train_time:104267ms step_avg:153.79ms tokens_processed:33325056\n",
      "step:679/1750 train_time:104421ms step_avg:153.79ms tokens_processed:33374208\n",
      "step:680/1750 train_time:104578ms step_avg:153.79ms tokens_processed:33423360\n",
      "step:681/1750 train_time:104731ms step_avg:153.79ms tokens_processed:33472512\n",
      "step:682/1750 train_time:104885ms step_avg:153.79ms tokens_processed:33521664\n",
      "step:683/1750 train_time:105039ms step_avg:153.79ms tokens_processed:33570816\n",
      "step:684/1750 train_time:105193ms step_avg:153.79ms tokens_processed:33619968\n",
      "step:685/1750 train_time:105344ms step_avg:153.79ms tokens_processed:33669120\n",
      "step:686/1750 train_time:105501ms step_avg:153.79ms tokens_processed:33718272\n",
      "step:687/1750 train_time:105654ms step_avg:153.79ms tokens_processed:33767424\n",
      "step:688/1750 train_time:105808ms step_avg:153.79ms tokens_processed:33816576\n",
      "step:689/1750 train_time:105963ms step_avg:153.79ms tokens_processed:33865728\n",
      "step:690/1750 train_time:106117ms step_avg:153.79ms tokens_processed:33914880\n",
      "step:691/1750 train_time:106271ms step_avg:153.79ms tokens_processed:33964032\n",
      "step:692/1750 train_time:106425ms step_avg:153.79ms tokens_processed:34013184\n",
      "step:693/1750 train_time:106579ms step_avg:153.79ms tokens_processed:34062336\n",
      "step:694/1750 train_time:106734ms step_avg:153.80ms tokens_processed:34111488\n",
      "step:695/1750 train_time:106888ms step_avg:153.80ms tokens_processed:34160640\n",
      "step:696/1750 train_time:107043ms step_avg:153.80ms tokens_processed:34209792\n",
      "step:697/1750 train_time:107197ms step_avg:153.80ms tokens_processed:34258944\n",
      "step:698/1750 train_time:107350ms step_avg:153.80ms tokens_processed:34308096\n",
      "step:699/1750 train_time:107504ms step_avg:153.80ms tokens_processed:34357248\n",
      "step:700/1750 train_time:107658ms step_avg:153.80ms tokens_processed:34406400\n",
      "---validation---\n",
      "step:700/1750 val_loss:4.7658 train_time:107666ms step_avg:153.81ms\n",
      "---end of validation---\n",
      "step:701/1750 train_time:107817ms step_avg:153.80ms tokens_processed:34455552\n",
      "step:702/1750 train_time:107971ms step_avg:153.80ms tokens_processed:34504704\n",
      "step:703/1750 train_time:108125ms step_avg:153.80ms tokens_processed:34553856\n",
      "step:704/1750 train_time:108279ms step_avg:153.80ms tokens_processed:34603008\n",
      "step:705/1750 train_time:108432ms step_avg:153.80ms tokens_processed:34652160\n",
      "step:706/1750 train_time:108587ms step_avg:153.81ms tokens_processed:34701312\n",
      "step:707/1750 train_time:108741ms step_avg:153.81ms tokens_processed:34750464\n",
      "step:708/1750 train_time:108895ms step_avg:153.81ms tokens_processed:34799616\n",
      "step:709/1750 train_time:109049ms step_avg:153.81ms tokens_processed:34848768\n",
      "step:710/1750 train_time:109205ms step_avg:153.81ms tokens_processed:34897920\n",
      "step:711/1750 train_time:109357ms step_avg:153.81ms tokens_processed:34947072\n",
      "step:712/1750 train_time:109511ms step_avg:153.81ms tokens_processed:34996224\n",
      "step:713/1750 train_time:109665ms step_avg:153.81ms tokens_processed:35045376\n",
      "step:714/1750 train_time:109824ms step_avg:153.81ms tokens_processed:35094528\n",
      "step:715/1750 train_time:109977ms step_avg:153.81ms tokens_processed:35143680\n",
      "step:716/1750 train_time:110131ms step_avg:153.81ms tokens_processed:35192832\n",
      "step:717/1750 train_time:110285ms step_avg:153.81ms tokens_processed:35241984\n",
      "step:718/1750 train_time:110439ms step_avg:153.81ms tokens_processed:35291136\n",
      "step:719/1750 train_time:110592ms step_avg:153.81ms tokens_processed:35340288\n",
      "step:720/1750 train_time:110748ms step_avg:153.82ms tokens_processed:35389440\n",
      "step:721/1750 train_time:110902ms step_avg:153.82ms tokens_processed:35438592\n",
      "step:722/1750 train_time:111057ms step_avg:153.82ms tokens_processed:35487744\n",
      "step:723/1750 train_time:111210ms step_avg:153.82ms tokens_processed:35536896\n",
      "step:724/1750 train_time:111365ms step_avg:153.82ms tokens_processed:35586048\n",
      "step:725/1750 train_time:111519ms step_avg:153.82ms tokens_processed:35635200\n",
      "step:726/1750 train_time:111673ms step_avg:153.82ms tokens_processed:35684352\n",
      "step:727/1750 train_time:111828ms step_avg:153.82ms tokens_processed:35733504\n",
      "step:728/1750 train_time:111982ms step_avg:153.82ms tokens_processed:35782656\n",
      "step:729/1750 train_time:112139ms step_avg:153.83ms tokens_processed:35831808\n",
      "step:730/1750 train_time:112292ms step_avg:153.82ms tokens_processed:35880960\n",
      "step:731/1750 train_time:112447ms step_avg:153.83ms tokens_processed:35930112\n",
      "step:732/1750 train_time:112601ms step_avg:153.83ms tokens_processed:35979264\n",
      "step:733/1750 train_time:112754ms step_avg:153.82ms tokens_processed:36028416\n",
      "step:734/1750 train_time:112907ms step_avg:153.82ms tokens_processed:36077568\n",
      "step:735/1750 train_time:113061ms step_avg:153.82ms tokens_processed:36126720\n",
      "step:736/1750 train_time:113214ms step_avg:153.82ms tokens_processed:36175872\n",
      "step:737/1750 train_time:113368ms step_avg:153.82ms tokens_processed:36225024\n",
      "step:738/1750 train_time:113523ms step_avg:153.83ms tokens_processed:36274176\n",
      "step:739/1750 train_time:113678ms step_avg:153.83ms tokens_processed:36323328\n",
      "step:740/1750 train_time:113831ms step_avg:153.83ms tokens_processed:36372480\n",
      "step:741/1750 train_time:113986ms step_avg:153.83ms tokens_processed:36421632\n",
      "step:742/1750 train_time:114141ms step_avg:153.83ms tokens_processed:36470784\n",
      "step:743/1750 train_time:114295ms step_avg:153.83ms tokens_processed:36519936\n",
      "step:744/1750 train_time:114449ms step_avg:153.83ms tokens_processed:36569088\n",
      "step:745/1750 train_time:114603ms step_avg:153.83ms tokens_processed:36618240\n",
      "step:746/1750 train_time:114758ms step_avg:153.83ms tokens_processed:36667392\n",
      "step:747/1750 train_time:114911ms step_avg:153.83ms tokens_processed:36716544\n",
      "step:748/1750 train_time:115066ms step_avg:153.83ms tokens_processed:36765696\n",
      "step:749/1750 train_time:115220ms step_avg:153.83ms tokens_processed:36814848\n",
      "step:750/1750 train_time:115374ms step_avg:153.83ms tokens_processed:36864000\n",
      "step:751/1750 train_time:115528ms step_avg:153.83ms tokens_processed:36913152\n",
      "step:752/1750 train_time:115683ms step_avg:153.83ms tokens_processed:36962304\n",
      "step:753/1750 train_time:115838ms step_avg:153.83ms tokens_processed:37011456\n",
      "step:754/1750 train_time:115992ms step_avg:153.84ms tokens_processed:37060608\n",
      "step:755/1750 train_time:116147ms step_avg:153.84ms tokens_processed:37109760\n",
      "step:756/1750 train_time:116302ms step_avg:153.84ms tokens_processed:37158912\n",
      "step:757/1750 train_time:116456ms step_avg:153.84ms tokens_processed:37208064\n",
      "step:758/1750 train_time:116609ms step_avg:153.84ms tokens_processed:37257216\n",
      "step:759/1750 train_time:116764ms step_avg:153.84ms tokens_processed:37306368\n",
      "step:760/1750 train_time:116916ms step_avg:153.84ms tokens_processed:37355520\n",
      "step:761/1750 train_time:117072ms step_avg:153.84ms tokens_processed:37404672\n",
      "step:762/1750 train_time:117225ms step_avg:153.84ms tokens_processed:37453824\n",
      "step:763/1750 train_time:117379ms step_avg:153.84ms tokens_processed:37502976\n",
      "step:764/1750 train_time:117533ms step_avg:153.84ms tokens_processed:37552128\n",
      "step:765/1750 train_time:117687ms step_avg:153.84ms tokens_processed:37601280\n",
      "step:766/1750 train_time:117842ms step_avg:153.84ms tokens_processed:37650432\n",
      "step:767/1750 train_time:117996ms step_avg:153.84ms tokens_processed:37699584\n",
      "step:768/1750 train_time:118150ms step_avg:153.84ms tokens_processed:37748736\n",
      "step:769/1750 train_time:118312ms step_avg:153.85ms tokens_processed:37797888\n",
      "step:770/1750 train_time:118473ms step_avg:153.86ms tokens_processed:37847040\n",
      "step:771/1750 train_time:118632ms step_avg:153.87ms tokens_processed:37896192\n",
      "step:772/1750 train_time:118792ms step_avg:153.88ms tokens_processed:37945344\n",
      "step:773/1750 train_time:118953ms step_avg:153.89ms tokens_processed:37994496\n",
      "step:774/1750 train_time:119114ms step_avg:153.89ms tokens_processed:38043648\n",
      "step:775/1750 train_time:119275ms step_avg:153.90ms tokens_processed:38092800\n",
      "step:776/1750 train_time:119435ms step_avg:153.91ms tokens_processed:38141952\n",
      "step:777/1750 train_time:119595ms step_avg:153.92ms tokens_processed:38191104\n",
      "step:778/1750 train_time:119756ms step_avg:153.93ms tokens_processed:38240256\n",
      "step:779/1750 train_time:119915ms step_avg:153.93ms tokens_processed:38289408\n",
      "step:780/1750 train_time:120077ms step_avg:153.95ms tokens_processed:38338560\n",
      "step:781/1750 train_time:120237ms step_avg:153.95ms tokens_processed:38387712\n",
      "step:782/1750 train_time:120398ms step_avg:153.96ms tokens_processed:38436864\n",
      "step:783/1750 train_time:120558ms step_avg:153.97ms tokens_processed:38486016\n",
      "step:784/1750 train_time:120718ms step_avg:153.98ms tokens_processed:38535168\n",
      "step:785/1750 train_time:120878ms step_avg:153.99ms tokens_processed:38584320\n",
      "step:786/1750 train_time:121040ms step_avg:153.99ms tokens_processed:38633472\n",
      "step:787/1750 train_time:121200ms step_avg:154.00ms tokens_processed:38682624\n",
      "step:788/1750 train_time:121360ms step_avg:154.01ms tokens_processed:38731776\n",
      "step:789/1750 train_time:121520ms step_avg:154.02ms tokens_processed:38780928\n",
      "step:790/1750 train_time:121680ms step_avg:154.03ms tokens_processed:38830080\n",
      "step:791/1750 train_time:121840ms step_avg:154.03ms tokens_processed:38879232\n",
      "step:792/1750 train_time:122001ms step_avg:154.04ms tokens_processed:38928384\n",
      "step:793/1750 train_time:122161ms step_avg:154.05ms tokens_processed:38977536\n",
      "step:794/1750 train_time:122321ms step_avg:154.06ms tokens_processed:39026688\n",
      "step:795/1750 train_time:122481ms step_avg:154.06ms tokens_processed:39075840\n",
      "step:796/1750 train_time:122640ms step_avg:154.07ms tokens_processed:39124992\n",
      "step:797/1750 train_time:122801ms step_avg:154.08ms tokens_processed:39174144\n",
      "step:798/1750 train_time:122961ms step_avg:154.09ms tokens_processed:39223296\n",
      "step:799/1750 train_time:123122ms step_avg:154.09ms tokens_processed:39272448\n",
      "step:800/1750 train_time:123282ms step_avg:154.10ms tokens_processed:39321600\n",
      "---validation---\n",
      "step:800/1750 val_loss:4.6503 train_time:123289ms step_avg:154.11ms\n",
      "---end of validation---\n",
      "step:801/1750 train_time:123446ms step_avg:154.12ms tokens_processed:39370752\n",
      "step:802/1750 train_time:123608ms step_avg:154.12ms tokens_processed:39419904\n",
      "step:803/1750 train_time:123767ms step_avg:154.13ms tokens_processed:39469056\n",
      "step:804/1750 train_time:123929ms step_avg:154.14ms tokens_processed:39518208\n",
      "step:805/1750 train_time:124088ms step_avg:154.15ms tokens_processed:39567360\n",
      "step:806/1750 train_time:124249ms step_avg:154.15ms tokens_processed:39616512\n",
      "step:807/1750 train_time:124409ms step_avg:154.16ms tokens_processed:39665664\n",
      "step:808/1750 train_time:124568ms step_avg:154.17ms tokens_processed:39714816\n",
      "step:809/1750 train_time:124729ms step_avg:154.18ms tokens_processed:39763968\n",
      "step:810/1750 train_time:124891ms step_avg:154.19ms tokens_processed:39813120\n",
      "step:811/1750 train_time:125051ms step_avg:154.19ms tokens_processed:39862272\n",
      "step:812/1750 train_time:125211ms step_avg:154.20ms tokens_processed:39911424\n",
      "step:813/1750 train_time:125371ms step_avg:154.21ms tokens_processed:39960576\n",
      "step:814/1750 train_time:125531ms step_avg:154.21ms tokens_processed:40009728\n",
      "step:815/1750 train_time:125692ms step_avg:154.22ms tokens_processed:40058880\n",
      "step:816/1750 train_time:125852ms step_avg:154.23ms tokens_processed:40108032\n",
      "step:817/1750 train_time:126012ms step_avg:154.24ms tokens_processed:40157184\n",
      "step:818/1750 train_time:126172ms step_avg:154.24ms tokens_processed:40206336\n",
      "step:819/1750 train_time:126333ms step_avg:154.25ms tokens_processed:40255488\n",
      "step:820/1750 train_time:126492ms step_avg:154.26ms tokens_processed:40304640\n",
      "step:821/1750 train_time:126652ms step_avg:154.27ms tokens_processed:40353792\n",
      "step:822/1750 train_time:126812ms step_avg:154.27ms tokens_processed:40402944\n",
      "step:823/1750 train_time:126974ms step_avg:154.28ms tokens_processed:40452096\n",
      "step:824/1750 train_time:127135ms step_avg:154.29ms tokens_processed:40501248\n",
      "step:825/1750 train_time:127294ms step_avg:154.30ms tokens_processed:40550400\n",
      "step:826/1750 train_time:127455ms step_avg:154.30ms tokens_processed:40599552\n",
      "step:827/1750 train_time:127616ms step_avg:154.31ms tokens_processed:40648704\n",
      "step:828/1750 train_time:127776ms step_avg:154.32ms tokens_processed:40697856\n",
      "step:829/1750 train_time:127938ms step_avg:154.33ms tokens_processed:40747008\n",
      "step:830/1750 train_time:128097ms step_avg:154.33ms tokens_processed:40796160\n",
      "step:831/1750 train_time:128257ms step_avg:154.34ms tokens_processed:40845312\n",
      "step:832/1750 train_time:128415ms step_avg:154.35ms tokens_processed:40894464\n",
      "step:833/1750 train_time:128577ms step_avg:154.35ms tokens_processed:40943616\n",
      "step:834/1750 train_time:128736ms step_avg:154.36ms tokens_processed:40992768\n",
      "step:835/1750 train_time:128895ms step_avg:154.37ms tokens_processed:41041920\n",
      "step:836/1750 train_time:129060ms step_avg:154.38ms tokens_processed:41091072\n",
      "step:837/1750 train_time:129221ms step_avg:154.39ms tokens_processed:41140224\n",
      "step:838/1750 train_time:129381ms step_avg:154.39ms tokens_processed:41189376\n",
      "step:839/1750 train_time:129542ms step_avg:154.40ms tokens_processed:41238528\n",
      "step:840/1750 train_time:129701ms step_avg:154.41ms tokens_processed:41287680\n",
      "step:841/1750 train_time:129862ms step_avg:154.41ms tokens_processed:41336832\n",
      "step:842/1750 train_time:130022ms step_avg:154.42ms tokens_processed:41385984\n",
      "step:843/1750 train_time:130184ms step_avg:154.43ms tokens_processed:41435136\n",
      "step:844/1750 train_time:130344ms step_avg:154.44ms tokens_processed:41484288\n",
      "step:845/1750 train_time:130504ms step_avg:154.44ms tokens_processed:41533440\n",
      "step:846/1750 train_time:130664ms step_avg:154.45ms tokens_processed:41582592\n",
      "step:847/1750 train_time:130824ms step_avg:154.46ms tokens_processed:41631744\n",
      "step:848/1750 train_time:130985ms step_avg:154.46ms tokens_processed:41680896\n",
      "step:849/1750 train_time:131148ms step_avg:154.47ms tokens_processed:41730048\n",
      "step:850/1750 train_time:131308ms step_avg:154.48ms tokens_processed:41779200\n",
      "step:851/1750 train_time:131469ms step_avg:154.49ms tokens_processed:41828352\n",
      "step:852/1750 train_time:131629ms step_avg:154.49ms tokens_processed:41877504\n",
      "step:853/1750 train_time:131788ms step_avg:154.50ms tokens_processed:41926656\n",
      "step:854/1750 train_time:131950ms step_avg:154.51ms tokens_processed:41975808\n",
      "step:855/1750 train_time:132111ms step_avg:154.52ms tokens_processed:42024960\n",
      "step:856/1750 train_time:132272ms step_avg:154.52ms tokens_processed:42074112\n",
      "step:857/1750 train_time:132431ms step_avg:154.53ms tokens_processed:42123264\n",
      "step:858/1750 train_time:132591ms step_avg:154.53ms tokens_processed:42172416\n",
      "step:859/1750 train_time:132751ms step_avg:154.54ms tokens_processed:42221568\n",
      "step:860/1750 train_time:132913ms step_avg:154.55ms tokens_processed:42270720\n",
      "step:861/1750 train_time:133073ms step_avg:154.56ms tokens_processed:42319872\n",
      "step:862/1750 train_time:133233ms step_avg:154.56ms tokens_processed:42369024\n",
      "step:863/1750 train_time:133393ms step_avg:154.57ms tokens_processed:42418176\n",
      "step:864/1750 train_time:133554ms step_avg:154.58ms tokens_processed:42467328\n",
      "step:865/1750 train_time:133714ms step_avg:154.58ms tokens_processed:42516480\n",
      "step:866/1750 train_time:133874ms step_avg:154.59ms tokens_processed:42565632\n",
      "step:867/1750 train_time:134034ms step_avg:154.59ms tokens_processed:42614784\n",
      "step:868/1750 train_time:134195ms step_avg:154.60ms tokens_processed:42663936\n",
      "step:869/1750 train_time:134355ms step_avg:154.61ms tokens_processed:42713088\n",
      "step:870/1750 train_time:134515ms step_avg:154.62ms tokens_processed:42762240\n",
      "step:871/1750 train_time:134676ms step_avg:154.62ms tokens_processed:42811392\n",
      "step:872/1750 train_time:134835ms step_avg:154.63ms tokens_processed:42860544\n",
      "step:873/1750 train_time:134996ms step_avg:154.63ms tokens_processed:42909696\n",
      "step:874/1750 train_time:135155ms step_avg:154.64ms tokens_processed:42958848\n",
      "step:875/1750 train_time:135316ms step_avg:154.65ms tokens_processed:43008000\n",
      "step:876/1750 train_time:135477ms step_avg:154.65ms tokens_processed:43057152\n",
      "step:877/1750 train_time:135637ms step_avg:154.66ms tokens_processed:43106304\n",
      "step:878/1750 train_time:135797ms step_avg:154.67ms tokens_processed:43155456\n",
      "step:879/1750 train_time:135956ms step_avg:154.67ms tokens_processed:43204608\n",
      "step:880/1750 train_time:136117ms step_avg:154.68ms tokens_processed:43253760\n",
      "step:881/1750 train_time:136278ms step_avg:154.69ms tokens_processed:43302912\n",
      "step:882/1750 train_time:136438ms step_avg:154.69ms tokens_processed:43352064\n",
      "step:883/1750 train_time:136599ms step_avg:154.70ms tokens_processed:43401216\n",
      "step:884/1750 train_time:136758ms step_avg:154.70ms tokens_processed:43450368\n",
      "step:885/1750 train_time:136919ms step_avg:154.71ms tokens_processed:43499520\n",
      "step:886/1750 train_time:137078ms step_avg:154.72ms tokens_processed:43548672\n",
      "step:887/1750 train_time:137237ms step_avg:154.72ms tokens_processed:43597824\n",
      "step:888/1750 train_time:137397ms step_avg:154.73ms tokens_processed:43646976\n",
      "step:889/1750 train_time:137558ms step_avg:154.73ms tokens_processed:43696128\n",
      "step:890/1750 train_time:137719ms step_avg:154.74ms tokens_processed:43745280\n",
      "step:891/1750 train_time:137880ms step_avg:154.75ms tokens_processed:43794432\n",
      "step:892/1750 train_time:138041ms step_avg:154.75ms tokens_processed:43843584\n",
      "step:893/1750 train_time:138201ms step_avg:154.76ms tokens_processed:43892736\n",
      "step:894/1750 train_time:138361ms step_avg:154.77ms tokens_processed:43941888\n",
      "step:895/1750 train_time:138523ms step_avg:154.77ms tokens_processed:43991040\n",
      "step:896/1750 train_time:138683ms step_avg:154.78ms tokens_processed:44040192\n",
      "step:897/1750 train_time:138843ms step_avg:154.79ms tokens_processed:44089344\n",
      "step:898/1750 train_time:139003ms step_avg:154.79ms tokens_processed:44138496\n",
      "step:899/1750 train_time:139163ms step_avg:154.80ms tokens_processed:44187648\n",
      "step:900/1750 train_time:139324ms step_avg:154.80ms tokens_processed:44236800\n",
      "---validation---\n",
      "step:900/1750 val_loss:4.5866 train_time:139330ms step_avg:154.81ms\n",
      "---end of validation---\n",
      "step:901/1750 train_time:139489ms step_avg:154.82ms tokens_processed:44285952\n",
      "step:902/1750 train_time:139648ms step_avg:154.82ms tokens_processed:44335104\n",
      "step:903/1750 train_time:139808ms step_avg:154.83ms tokens_processed:44384256\n",
      "step:904/1750 train_time:139968ms step_avg:154.83ms tokens_processed:44433408\n",
      "step:905/1750 train_time:140129ms step_avg:154.84ms tokens_processed:44482560\n",
      "step:906/1750 train_time:140288ms step_avg:154.84ms tokens_processed:44531712\n",
      "step:907/1750 train_time:140449ms step_avg:154.85ms tokens_processed:44580864\n",
      "step:908/1750 train_time:140609ms step_avg:154.86ms tokens_processed:44630016\n",
      "step:909/1750 train_time:140769ms step_avg:154.86ms tokens_processed:44679168\n",
      "step:910/1750 train_time:140929ms step_avg:154.87ms tokens_processed:44728320\n",
      "step:911/1750 train_time:141089ms step_avg:154.87ms tokens_processed:44777472\n",
      "step:912/1750 train_time:141248ms step_avg:154.88ms tokens_processed:44826624\n",
      "step:913/1750 train_time:141409ms step_avg:154.88ms tokens_processed:44875776\n",
      "step:914/1750 train_time:141570ms step_avg:154.89ms tokens_processed:44924928\n",
      "step:915/1750 train_time:141731ms step_avg:154.90ms tokens_processed:44974080\n",
      "step:916/1750 train_time:141890ms step_avg:154.90ms tokens_processed:45023232\n",
      "step:917/1750 train_time:142050ms step_avg:154.91ms tokens_processed:45072384\n",
      "step:918/1750 train_time:142210ms step_avg:154.91ms tokens_processed:45121536\n",
      "step:919/1750 train_time:142369ms step_avg:154.92ms tokens_processed:45170688\n",
      "step:920/1750 train_time:142530ms step_avg:154.92ms tokens_processed:45219840\n",
      "step:921/1750 train_time:142691ms step_avg:154.93ms tokens_processed:45268992\n",
      "step:922/1750 train_time:142852ms step_avg:154.94ms tokens_processed:45318144\n",
      "step:923/1750 train_time:143013ms step_avg:154.94ms tokens_processed:45367296\n",
      "step:924/1750 train_time:143172ms step_avg:154.95ms tokens_processed:45416448\n",
      "step:925/1750 train_time:143332ms step_avg:154.95ms tokens_processed:45465600\n",
      "step:926/1750 train_time:143492ms step_avg:154.96ms tokens_processed:45514752\n",
      "step:927/1750 train_time:143654ms step_avg:154.97ms tokens_processed:45563904\n",
      "step:928/1750 train_time:143813ms step_avg:154.97ms tokens_processed:45613056\n",
      "step:929/1750 train_time:143974ms step_avg:154.98ms tokens_processed:45662208\n",
      "step:930/1750 train_time:144134ms step_avg:154.98ms tokens_processed:45711360\n",
      "step:931/1750 train_time:144295ms step_avg:154.99ms tokens_processed:45760512\n",
      "step:932/1750 train_time:144456ms step_avg:155.00ms tokens_processed:45809664\n",
      "step:933/1750 train_time:144615ms step_avg:155.00ms tokens_processed:45858816\n",
      "step:934/1750 train_time:144775ms step_avg:155.01ms tokens_processed:45907968\n",
      "step:935/1750 train_time:144934ms step_avg:155.01ms tokens_processed:45957120\n",
      "step:936/1750 train_time:145095ms step_avg:155.02ms tokens_processed:46006272\n",
      "step:937/1750 train_time:145255ms step_avg:155.02ms tokens_processed:46055424\n",
      "step:938/1750 train_time:145415ms step_avg:155.03ms tokens_processed:46104576\n",
      "step:939/1750 train_time:145574ms step_avg:155.03ms tokens_processed:46153728\n",
      "step:940/1750 train_time:145733ms step_avg:155.04ms tokens_processed:46202880\n",
      "step:941/1750 train_time:145892ms step_avg:155.04ms tokens_processed:46252032\n",
      "step:942/1750 train_time:146053ms step_avg:155.05ms tokens_processed:46301184\n",
      "step:943/1750 train_time:146213ms step_avg:155.05ms tokens_processed:46350336\n",
      "step:944/1750 train_time:146375ms step_avg:155.06ms tokens_processed:46399488\n",
      "step:945/1750 train_time:146535ms step_avg:155.06ms tokens_processed:46448640\n",
      "step:946/1750 train_time:146694ms step_avg:155.07ms tokens_processed:46497792\n",
      "step:947/1750 train_time:146854ms step_avg:155.07ms tokens_processed:46546944\n",
      "step:948/1750 train_time:147014ms step_avg:155.08ms tokens_processed:46596096\n",
      "step:949/1750 train_time:147174ms step_avg:155.08ms tokens_processed:46645248\n",
      "step:950/1750 train_time:147333ms step_avg:155.09ms tokens_processed:46694400\n",
      "step:951/1750 train_time:147495ms step_avg:155.09ms tokens_processed:46743552\n",
      "step:952/1750 train_time:147655ms step_avg:155.10ms tokens_processed:46792704\n",
      "step:953/1750 train_time:147815ms step_avg:155.10ms tokens_processed:46841856\n",
      "step:954/1750 train_time:147974ms step_avg:155.11ms tokens_processed:46891008\n",
      "step:955/1750 train_time:148133ms step_avg:155.11ms tokens_processed:46940160\n",
      "step:956/1750 train_time:148293ms step_avg:155.12ms tokens_processed:46989312\n",
      "step:957/1750 train_time:148454ms step_avg:155.12ms tokens_processed:47038464\n",
      "step:958/1750 train_time:148615ms step_avg:155.13ms tokens_processed:47087616\n",
      "step:959/1750 train_time:148774ms step_avg:155.13ms tokens_processed:47136768\n",
      "step:960/1750 train_time:148934ms step_avg:155.14ms tokens_processed:47185920\n",
      "step:961/1750 train_time:149094ms step_avg:155.14ms tokens_processed:47235072\n",
      "step:962/1750 train_time:149254ms step_avg:155.15ms tokens_processed:47284224\n",
      "step:963/1750 train_time:149415ms step_avg:155.16ms tokens_processed:47333376\n",
      "step:964/1750 train_time:149575ms step_avg:155.16ms tokens_processed:47382528\n",
      "step:965/1750 train_time:149735ms step_avg:155.17ms tokens_processed:47431680\n",
      "step:966/1750 train_time:149896ms step_avg:155.17ms tokens_processed:47480832\n",
      "step:967/1750 train_time:150056ms step_avg:155.18ms tokens_processed:47529984\n",
      "step:968/1750 train_time:150217ms step_avg:155.18ms tokens_processed:47579136\n",
      "step:969/1750 train_time:150377ms step_avg:155.19ms tokens_processed:47628288\n",
      "step:970/1750 train_time:150537ms step_avg:155.19ms tokens_processed:47677440\n",
      "step:971/1750 train_time:150695ms step_avg:155.20ms tokens_processed:47726592\n",
      "step:972/1750 train_time:150855ms step_avg:155.20ms tokens_processed:47775744\n",
      "step:973/1750 train_time:151016ms step_avg:155.21ms tokens_processed:47824896\n",
      "step:974/1750 train_time:151175ms step_avg:155.21ms tokens_processed:47874048\n",
      "step:975/1750 train_time:151335ms step_avg:155.22ms tokens_processed:47923200\n",
      "step:976/1750 train_time:151496ms step_avg:155.22ms tokens_processed:47972352\n",
      "step:977/1750 train_time:151656ms step_avg:155.23ms tokens_processed:48021504\n",
      "step:978/1750 train_time:151817ms step_avg:155.23ms tokens_processed:48070656\n",
      "step:979/1750 train_time:151976ms step_avg:155.24ms tokens_processed:48119808\n",
      "step:980/1750 train_time:152137ms step_avg:155.24ms tokens_processed:48168960\n",
      "step:981/1750 train_time:152296ms step_avg:155.25ms tokens_processed:48218112\n",
      "step:982/1750 train_time:152461ms step_avg:155.26ms tokens_processed:48267264\n",
      "step:983/1750 train_time:152623ms step_avg:155.26ms tokens_processed:48316416\n",
      "step:984/1750 train_time:152782ms step_avg:155.27ms tokens_processed:48365568\n",
      "step:985/1750 train_time:152942ms step_avg:155.27ms tokens_processed:48414720\n",
      "step:986/1750 train_time:153102ms step_avg:155.28ms tokens_processed:48463872\n",
      "step:987/1750 train_time:153263ms step_avg:155.28ms tokens_processed:48513024\n",
      "step:988/1750 train_time:153424ms step_avg:155.29ms tokens_processed:48562176\n",
      "step:989/1750 train_time:153584ms step_avg:155.29ms tokens_processed:48611328\n",
      "step:990/1750 train_time:153743ms step_avg:155.30ms tokens_processed:48660480\n",
      "step:991/1750 train_time:153904ms step_avg:155.30ms tokens_processed:48709632\n",
      "step:992/1750 train_time:154064ms step_avg:155.31ms tokens_processed:48758784\n",
      "step:993/1750 train_time:154225ms step_avg:155.31ms tokens_processed:48807936\n",
      "step:994/1750 train_time:154384ms step_avg:155.32ms tokens_processed:48857088\n",
      "step:995/1750 train_time:154544ms step_avg:155.32ms tokens_processed:48906240\n",
      "step:996/1750 train_time:154705ms step_avg:155.33ms tokens_processed:48955392\n",
      "step:997/1750 train_time:154866ms step_avg:155.33ms tokens_processed:49004544\n",
      "step:998/1750 train_time:155027ms step_avg:155.34ms tokens_processed:49053696\n",
      "step:999/1750 train_time:155187ms step_avg:155.34ms tokens_processed:49102848\n",
      "step:1000/1750 train_time:155348ms step_avg:155.35ms tokens_processed:49152000\n",
      "---validation---\n",
      "step:1000/1750 val_loss:4.5270 train_time:155354ms step_avg:155.35ms\n",
      "---end of validation---\n",
      "step:1001/1750 train_time:155513ms step_avg:155.36ms tokens_processed:49201152\n",
      "step:1002/1750 train_time:155675ms step_avg:155.36ms tokens_processed:49250304\n",
      "step:1003/1750 train_time:155835ms step_avg:155.37ms tokens_processed:49299456\n",
      "step:1004/1750 train_time:155995ms step_avg:155.37ms tokens_processed:49348608\n",
      "step:1005/1750 train_time:156154ms step_avg:155.38ms tokens_processed:49397760\n",
      "step:1006/1750 train_time:156314ms step_avg:155.38ms tokens_processed:49446912\n",
      "step:1007/1750 train_time:156476ms step_avg:155.39ms tokens_processed:49496064\n",
      "step:1008/1750 train_time:156638ms step_avg:155.39ms tokens_processed:49545216\n",
      "step:1009/1750 train_time:156797ms step_avg:155.40ms tokens_processed:49594368\n",
      "step:1010/1750 train_time:156957ms step_avg:155.40ms tokens_processed:49643520\n",
      "step:1011/1750 train_time:157116ms step_avg:155.41ms tokens_processed:49692672\n",
      "step:1012/1750 train_time:157276ms step_avg:155.41ms tokens_processed:49741824\n",
      "step:1013/1750 train_time:157438ms step_avg:155.42ms tokens_processed:49790976\n",
      "step:1014/1750 train_time:157600ms step_avg:155.42ms tokens_processed:49840128\n",
      "step:1015/1750 train_time:157760ms step_avg:155.43ms tokens_processed:49889280\n",
      "step:1016/1750 train_time:157921ms step_avg:155.43ms tokens_processed:49938432\n",
      "step:1017/1750 train_time:158082ms step_avg:155.44ms tokens_processed:49987584\n",
      "step:1018/1750 train_time:158240ms step_avg:155.44ms tokens_processed:50036736\n",
      "step:1019/1750 train_time:158401ms step_avg:155.45ms tokens_processed:50085888\n",
      "step:1020/1750 train_time:158561ms step_avg:155.45ms tokens_processed:50135040\n",
      "step:1021/1750 train_time:158721ms step_avg:155.46ms tokens_processed:50184192\n",
      "step:1022/1750 train_time:158882ms step_avg:155.46ms tokens_processed:50233344\n",
      "step:1023/1750 train_time:159042ms step_avg:155.47ms tokens_processed:50282496\n",
      "step:1024/1750 train_time:159202ms step_avg:155.47ms tokens_processed:50331648\n",
      "step:1025/1750 train_time:159364ms step_avg:155.48ms tokens_processed:50380800\n",
      "step:1026/1750 train_time:159525ms step_avg:155.48ms tokens_processed:50429952\n",
      "step:1027/1750 train_time:159685ms step_avg:155.49ms tokens_processed:50479104\n",
      "step:1028/1750 train_time:159844ms step_avg:155.49ms tokens_processed:50528256\n",
      "step:1029/1750 train_time:160004ms step_avg:155.49ms tokens_processed:50577408\n",
      "step:1030/1750 train_time:160165ms step_avg:155.50ms tokens_processed:50626560\n",
      "step:1031/1750 train_time:160324ms step_avg:155.50ms tokens_processed:50675712\n",
      "step:1032/1750 train_time:160485ms step_avg:155.51ms tokens_processed:50724864\n",
      "step:1033/1750 train_time:160644ms step_avg:155.51ms tokens_processed:50774016\n",
      "step:1034/1750 train_time:160805ms step_avg:155.52ms tokens_processed:50823168\n",
      "step:1035/1750 train_time:160967ms step_avg:155.52ms tokens_processed:50872320\n",
      "step:1036/1750 train_time:161125ms step_avg:155.53ms tokens_processed:50921472\n",
      "step:1037/1750 train_time:161285ms step_avg:155.53ms tokens_processed:50970624\n",
      "step:1038/1750 train_time:161446ms step_avg:155.54ms tokens_processed:51019776\n",
      "step:1039/1750 train_time:161606ms step_avg:155.54ms tokens_processed:51068928\n",
      "step:1040/1750 train_time:161765ms step_avg:155.54ms tokens_processed:51118080\n",
      "step:1041/1750 train_time:161926ms step_avg:155.55ms tokens_processed:51167232\n",
      "step:1042/1750 train_time:162087ms step_avg:155.55ms tokens_processed:51216384\n",
      "step:1043/1750 train_time:162246ms step_avg:155.56ms tokens_processed:51265536\n",
      "step:1044/1750 train_time:162406ms step_avg:155.56ms tokens_processed:51314688\n",
      "step:1045/1750 train_time:162566ms step_avg:155.57ms tokens_processed:51363840\n",
      "step:1046/1750 train_time:162726ms step_avg:155.57ms tokens_processed:51412992\n",
      "step:1047/1750 train_time:162887ms step_avg:155.58ms tokens_processed:51462144\n",
      "step:1048/1750 train_time:163047ms step_avg:155.58ms tokens_processed:51511296\n",
      "step:1049/1750 train_time:163206ms step_avg:155.58ms tokens_processed:51560448\n",
      "step:1050/1750 train_time:163365ms step_avg:155.59ms tokens_processed:51609600\n",
      "step:1051/1750 train_time:163526ms step_avg:155.59ms tokens_processed:51658752\n",
      "step:1052/1750 train_time:163688ms step_avg:155.60ms tokens_processed:51707904\n",
      "step:1053/1750 train_time:163847ms step_avg:155.60ms tokens_processed:51757056\n",
      "step:1054/1750 train_time:164007ms step_avg:155.60ms tokens_processed:51806208\n",
      "step:1055/1750 train_time:164166ms step_avg:155.61ms tokens_processed:51855360\n",
      "step:1056/1750 train_time:164327ms step_avg:155.61ms tokens_processed:51904512\n",
      "step:1057/1750 train_time:164487ms step_avg:155.62ms tokens_processed:51953664\n",
      "step:1058/1750 train_time:164648ms step_avg:155.62ms tokens_processed:52002816\n",
      "step:1059/1750 train_time:164808ms step_avg:155.63ms tokens_processed:52051968\n",
      "step:1060/1750 train_time:164968ms step_avg:155.63ms tokens_processed:52101120\n",
      "step:1061/1750 train_time:165129ms step_avg:155.63ms tokens_processed:52150272\n",
      "step:1062/1750 train_time:165289ms step_avg:155.64ms tokens_processed:52199424\n",
      "step:1063/1750 train_time:165449ms step_avg:155.64ms tokens_processed:52248576\n",
      "step:1064/1750 train_time:165610ms step_avg:155.65ms tokens_processed:52297728\n",
      "step:1065/1750 train_time:165771ms step_avg:155.65ms tokens_processed:52346880\n",
      "step:1066/1750 train_time:165932ms step_avg:155.66ms tokens_processed:52396032\n",
      "step:1067/1750 train_time:166093ms step_avg:155.66ms tokens_processed:52445184\n",
      "step:1068/1750 train_time:166253ms step_avg:155.67ms tokens_processed:52494336\n",
      "step:1069/1750 train_time:166413ms step_avg:155.67ms tokens_processed:52543488\n",
      "step:1070/1750 train_time:166573ms step_avg:155.68ms tokens_processed:52592640\n",
      "step:1071/1750 train_time:166735ms step_avg:155.68ms tokens_processed:52641792\n",
      "step:1072/1750 train_time:166896ms step_avg:155.69ms tokens_processed:52690944\n",
      "step:1073/1750 train_time:167056ms step_avg:155.69ms tokens_processed:52740096\n",
      "step:1074/1750 train_time:167217ms step_avg:155.70ms tokens_processed:52789248\n",
      "step:1075/1750 train_time:167376ms step_avg:155.70ms tokens_processed:52838400\n",
      "step:1076/1750 train_time:167542ms step_avg:155.71ms tokens_processed:52887552\n",
      "step:1077/1750 train_time:167701ms step_avg:155.71ms tokens_processed:52936704\n",
      "step:1078/1750 train_time:167866ms step_avg:155.72ms tokens_processed:52985856\n",
      "step:1079/1750 train_time:168026ms step_avg:155.72ms tokens_processed:53035008\n",
      "step:1080/1750 train_time:168186ms step_avg:155.73ms tokens_processed:53084160\n",
      "step:1081/1750 train_time:168346ms step_avg:155.73ms tokens_processed:53133312\n",
      "step:1082/1750 train_time:168506ms step_avg:155.74ms tokens_processed:53182464\n",
      "step:1083/1750 train_time:168666ms step_avg:155.74ms tokens_processed:53231616\n",
      "step:1084/1750 train_time:168828ms step_avg:155.74ms tokens_processed:53280768\n",
      "step:1085/1750 train_time:168987ms step_avg:155.75ms tokens_processed:53329920\n",
      "step:1086/1750 train_time:169146ms step_avg:155.75ms tokens_processed:53379072\n",
      "step:1087/1750 train_time:169306ms step_avg:155.76ms tokens_processed:53428224\n",
      "step:1088/1750 train_time:169466ms step_avg:155.76ms tokens_processed:53477376\n",
      "step:1089/1750 train_time:169632ms step_avg:155.77ms tokens_processed:53526528\n",
      "step:1090/1750 train_time:169794ms step_avg:155.77ms tokens_processed:53575680\n",
      "step:1091/1750 train_time:169954ms step_avg:155.78ms tokens_processed:53624832\n",
      "step:1092/1750 train_time:170113ms step_avg:155.78ms tokens_processed:53673984\n",
      "step:1093/1750 train_time:170274ms step_avg:155.79ms tokens_processed:53723136\n",
      "step:1094/1750 train_time:170436ms step_avg:155.79ms tokens_processed:53772288\n",
      "step:1095/1750 train_time:170597ms step_avg:155.80ms tokens_processed:53821440\n",
      "step:1096/1750 train_time:170757ms step_avg:155.80ms tokens_processed:53870592\n",
      "step:1097/1750 train_time:170917ms step_avg:155.80ms tokens_processed:53919744\n",
      "step:1098/1750 train_time:171076ms step_avg:155.81ms tokens_processed:53968896\n",
      "step:1099/1750 train_time:171237ms step_avg:155.81ms tokens_processed:54018048\n",
      "step:1100/1750 train_time:171398ms step_avg:155.82ms tokens_processed:54067200\n",
      "---validation---\n",
      "step:1100/1750 val_loss:4.4605 train_time:171405ms step_avg:155.82ms\n",
      "---end of validation---\n",
      "step:1101/1750 train_time:171563ms step_avg:155.82ms tokens_processed:54116352\n",
      "step:1102/1750 train_time:171725ms step_avg:155.83ms tokens_processed:54165504\n",
      "step:1103/1750 train_time:171884ms step_avg:155.83ms tokens_processed:54214656\n",
      "step:1104/1750 train_time:172046ms step_avg:155.84ms tokens_processed:54263808\n",
      "step:1105/1750 train_time:172206ms step_avg:155.84ms tokens_processed:54312960\n",
      "step:1106/1750 train_time:172366ms step_avg:155.85ms tokens_processed:54362112\n",
      "step:1107/1750 train_time:172528ms step_avg:155.85ms tokens_processed:54411264\n",
      "step:1108/1750 train_time:172689ms step_avg:155.86ms tokens_processed:54460416\n",
      "step:1109/1750 train_time:172848ms step_avg:155.86ms tokens_processed:54509568\n",
      "step:1110/1750 train_time:173009ms step_avg:155.86ms tokens_processed:54558720\n",
      "step:1111/1750 train_time:173169ms step_avg:155.87ms tokens_processed:54607872\n",
      "step:1112/1750 train_time:173330ms step_avg:155.87ms tokens_processed:54657024\n",
      "step:1113/1750 train_time:173490ms step_avg:155.88ms tokens_processed:54706176\n",
      "step:1114/1750 train_time:173650ms step_avg:155.88ms tokens_processed:54755328\n",
      "step:1115/1750 train_time:173811ms step_avg:155.88ms tokens_processed:54804480\n",
      "step:1116/1750 train_time:173972ms step_avg:155.89ms tokens_processed:54853632\n",
      "step:1117/1750 train_time:174132ms step_avg:155.89ms tokens_processed:54902784\n",
      "step:1118/1750 train_time:174292ms step_avg:155.90ms tokens_processed:54951936\n",
      "step:1119/1750 train_time:174453ms step_avg:155.90ms tokens_processed:55001088\n",
      "step:1120/1750 train_time:174613ms step_avg:155.90ms tokens_processed:55050240\n",
      "step:1121/1750 train_time:174774ms step_avg:155.91ms tokens_processed:55099392\n",
      "step:1122/1750 train_time:174934ms step_avg:155.91ms tokens_processed:55148544\n",
      "step:1123/1750 train_time:175094ms step_avg:155.92ms tokens_processed:55197696\n",
      "step:1124/1750 train_time:175255ms step_avg:155.92ms tokens_processed:55246848\n",
      "step:1125/1750 train_time:175415ms step_avg:155.92ms tokens_processed:55296000\n",
      "step:1126/1750 train_time:175575ms step_avg:155.93ms tokens_processed:55345152\n",
      "step:1127/1750 train_time:175735ms step_avg:155.93ms tokens_processed:55394304\n",
      "step:1128/1750 train_time:175897ms step_avg:155.94ms tokens_processed:55443456\n",
      "step:1129/1750 train_time:176058ms step_avg:155.94ms tokens_processed:55492608\n",
      "step:1130/1750 train_time:176217ms step_avg:155.94ms tokens_processed:55541760\n",
      "step:1131/1750 train_time:176376ms step_avg:155.95ms tokens_processed:55590912\n",
      "step:1132/1750 train_time:176537ms step_avg:155.95ms tokens_processed:55640064\n",
      "step:1133/1750 train_time:176696ms step_avg:155.95ms tokens_processed:55689216\n",
      "step:1134/1750 train_time:176857ms step_avg:155.96ms tokens_processed:55738368\n",
      "step:1135/1750 train_time:177017ms step_avg:155.96ms tokens_processed:55787520\n",
      "step:1136/1750 train_time:177176ms step_avg:155.96ms tokens_processed:55836672\n",
      "step:1137/1750 train_time:177337ms step_avg:155.97ms tokens_processed:55885824\n",
      "step:1138/1750 train_time:177497ms step_avg:155.97ms tokens_processed:55934976\n",
      "step:1139/1750 train_time:177658ms step_avg:155.98ms tokens_processed:55984128\n",
      "step:1140/1750 train_time:177818ms step_avg:155.98ms tokens_processed:56033280\n",
      "step:1141/1750 train_time:177979ms step_avg:155.98ms tokens_processed:56082432\n",
      "step:1142/1750 train_time:178139ms step_avg:155.99ms tokens_processed:56131584\n",
      "step:1143/1750 train_time:178300ms step_avg:155.99ms tokens_processed:56180736\n",
      "step:1144/1750 train_time:178461ms step_avg:156.00ms tokens_processed:56229888\n",
      "step:1145/1750 train_time:178622ms step_avg:156.00ms tokens_processed:56279040\n",
      "step:1146/1750 train_time:178782ms step_avg:156.01ms tokens_processed:56328192\n",
      "step:1147/1750 train_time:178943ms step_avg:156.01ms tokens_processed:56377344\n",
      "step:1148/1750 train_time:179104ms step_avg:156.01ms tokens_processed:56426496\n",
      "step:1149/1750 train_time:179264ms step_avg:156.02ms tokens_processed:56475648\n",
      "step:1150/1750 train_time:179424ms step_avg:156.02ms tokens_processed:56524800\n",
      "step:1151/1750 train_time:179585ms step_avg:156.03ms tokens_processed:56573952\n",
      "step:1152/1750 train_time:179746ms step_avg:156.03ms tokens_processed:56623104\n",
      "step:1153/1750 train_time:179913ms step_avg:156.04ms tokens_processed:56672256\n",
      "step:1154/1750 train_time:180080ms step_avg:156.05ms tokens_processed:56721408\n",
      "step:1155/1750 train_time:180237ms step_avg:156.05ms tokens_processed:56770560\n",
      "step:1156/1750 train_time:180396ms step_avg:156.05ms tokens_processed:56819712\n",
      "step:1157/1750 train_time:180555ms step_avg:156.05ms tokens_processed:56868864\n",
      "step:1158/1750 train_time:180716ms step_avg:156.06ms tokens_processed:56918016\n",
      "step:1159/1750 train_time:180877ms step_avg:156.06ms tokens_processed:56967168\n",
      "step:1160/1750 train_time:181037ms step_avg:156.07ms tokens_processed:57016320\n",
      "step:1161/1750 train_time:181198ms step_avg:156.07ms tokens_processed:57065472\n",
      "step:1162/1750 train_time:181358ms step_avg:156.07ms tokens_processed:57114624\n",
      "step:1163/1750 train_time:181517ms step_avg:156.08ms tokens_processed:57163776\n",
      "step:1164/1750 train_time:181677ms step_avg:156.08ms tokens_processed:57212928\n",
      "step:1165/1750 train_time:181838ms step_avg:156.08ms tokens_processed:57262080\n",
      "step:1166/1750 train_time:181999ms step_avg:156.09ms tokens_processed:57311232\n",
      "step:1167/1750 train_time:182160ms step_avg:156.09ms tokens_processed:57360384\n",
      "step:1168/1750 train_time:182321ms step_avg:156.10ms tokens_processed:57409536\n",
      "step:1169/1750 train_time:182482ms step_avg:156.10ms tokens_processed:57458688\n",
      "step:1170/1750 train_time:182642ms step_avg:156.10ms tokens_processed:57507840\n",
      "step:1171/1750 train_time:182805ms step_avg:156.11ms tokens_processed:57556992\n",
      "step:1172/1750 train_time:182968ms step_avg:156.12ms tokens_processed:57606144\n",
      "step:1173/1750 train_time:183127ms step_avg:156.12ms tokens_processed:57655296\n",
      "step:1174/1750 train_time:183287ms step_avg:156.12ms tokens_processed:57704448\n",
      "step:1175/1750 train_time:183449ms step_avg:156.13ms tokens_processed:57753600\n",
      "step:1176/1750 train_time:183608ms step_avg:156.13ms tokens_processed:57802752\n",
      "step:1177/1750 train_time:183769ms step_avg:156.13ms tokens_processed:57851904\n",
      "step:1178/1750 train_time:183930ms step_avg:156.14ms tokens_processed:57901056\n",
      "step:1179/1750 train_time:184091ms step_avg:156.14ms tokens_processed:57950208\n",
      "step:1180/1750 train_time:184250ms step_avg:156.14ms tokens_processed:57999360\n",
      "step:1181/1750 train_time:184411ms step_avg:156.15ms tokens_processed:58048512\n",
      "step:1182/1750 train_time:184571ms step_avg:156.15ms tokens_processed:58097664\n",
      "step:1183/1750 train_time:184731ms step_avg:156.15ms tokens_processed:58146816\n",
      "step:1184/1750 train_time:184891ms step_avg:156.16ms tokens_processed:58195968\n",
      "step:1185/1750 train_time:185051ms step_avg:156.16ms tokens_processed:58245120\n",
      "step:1186/1750 train_time:185212ms step_avg:156.17ms tokens_processed:58294272\n",
      "step:1187/1750 train_time:185373ms step_avg:156.17ms tokens_processed:58343424\n",
      "step:1188/1750 train_time:185533ms step_avg:156.17ms tokens_processed:58392576\n",
      "step:1189/1750 train_time:185693ms step_avg:156.18ms tokens_processed:58441728\n",
      "step:1190/1750 train_time:185853ms step_avg:156.18ms tokens_processed:58490880\n",
      "step:1191/1750 train_time:186015ms step_avg:156.18ms tokens_processed:58540032\n",
      "step:1192/1750 train_time:186175ms step_avg:156.19ms tokens_processed:58589184\n",
      "step:1193/1750 train_time:186334ms step_avg:156.19ms tokens_processed:58638336\n",
      "step:1194/1750 train_time:186495ms step_avg:156.19ms tokens_processed:58687488\n",
      "step:1195/1750 train_time:186656ms step_avg:156.20ms tokens_processed:58736640\n",
      "step:1196/1750 train_time:186816ms step_avg:156.20ms tokens_processed:58785792\n",
      "step:1197/1750 train_time:186975ms step_avg:156.20ms tokens_processed:58834944\n",
      "step:1198/1750 train_time:187135ms step_avg:156.21ms tokens_processed:58884096\n",
      "step:1199/1750 train_time:187295ms step_avg:156.21ms tokens_processed:58933248\n",
      "step:1200/1750 train_time:187455ms step_avg:156.21ms tokens_processed:58982400\n",
      "---validation---\n",
      "step:1200/1750 val_loss:4.3962 train_time:187463ms step_avg:156.22ms\n",
      "---end of validation---\n",
      "step:1201/1750 train_time:187621ms step_avg:156.22ms tokens_processed:59031552\n",
      "step:1202/1750 train_time:187780ms step_avg:156.22ms tokens_processed:59080704\n",
      "step:1203/1750 train_time:187940ms step_avg:156.23ms tokens_processed:59129856\n",
      "step:1204/1750 train_time:188100ms step_avg:156.23ms tokens_processed:59179008\n",
      "step:1205/1750 train_time:188260ms step_avg:156.23ms tokens_processed:59228160\n",
      "step:1206/1750 train_time:188421ms step_avg:156.24ms tokens_processed:59277312\n",
      "step:1207/1750 train_time:188581ms step_avg:156.24ms tokens_processed:59326464\n",
      "step:1208/1750 train_time:188742ms step_avg:156.24ms tokens_processed:59375616\n",
      "step:1209/1750 train_time:188902ms step_avg:156.25ms tokens_processed:59424768\n",
      "step:1210/1750 train_time:189062ms step_avg:156.25ms tokens_processed:59473920\n",
      "step:1211/1750 train_time:189223ms step_avg:156.25ms tokens_processed:59523072\n",
      "step:1212/1750 train_time:189381ms step_avg:156.25ms tokens_processed:59572224\n",
      "step:1213/1750 train_time:189543ms step_avg:156.26ms tokens_processed:59621376\n",
      "step:1214/1750 train_time:189703ms step_avg:156.26ms tokens_processed:59670528\n",
      "step:1215/1750 train_time:189863ms step_avg:156.27ms tokens_processed:59719680\n",
      "step:1216/1750 train_time:190025ms step_avg:156.27ms tokens_processed:59768832\n",
      "step:1217/1750 train_time:190183ms step_avg:156.27ms tokens_processed:59817984\n",
      "step:1218/1750 train_time:190344ms step_avg:156.28ms tokens_processed:59867136\n",
      "step:1219/1750 train_time:190504ms step_avg:156.28ms tokens_processed:59916288\n",
      "step:1220/1750 train_time:190664ms step_avg:156.28ms tokens_processed:59965440\n",
      "step:1221/1750 train_time:190825ms step_avg:156.29ms tokens_processed:60014592\n",
      "step:1222/1750 train_time:190984ms step_avg:156.29ms tokens_processed:60063744\n",
      "step:1223/1750 train_time:191144ms step_avg:156.29ms tokens_processed:60112896\n",
      "step:1224/1750 train_time:191303ms step_avg:156.29ms tokens_processed:60162048\n",
      "step:1225/1750 train_time:191464ms step_avg:156.30ms tokens_processed:60211200\n",
      "step:1226/1750 train_time:191625ms step_avg:156.30ms tokens_processed:60260352\n",
      "step:1227/1750 train_time:191784ms step_avg:156.30ms tokens_processed:60309504\n",
      "step:1228/1750 train_time:191945ms step_avg:156.31ms tokens_processed:60358656\n",
      "step:1229/1750 train_time:192104ms step_avg:156.31ms tokens_processed:60407808\n",
      "step:1230/1750 train_time:192264ms step_avg:156.31ms tokens_processed:60456960\n",
      "step:1231/1750 train_time:192424ms step_avg:156.32ms tokens_processed:60506112\n",
      "step:1232/1750 train_time:192583ms step_avg:156.32ms tokens_processed:60555264\n",
      "step:1233/1750 train_time:192746ms step_avg:156.32ms tokens_processed:60604416\n",
      "step:1234/1750 train_time:192905ms step_avg:156.33ms tokens_processed:60653568\n",
      "step:1235/1750 train_time:193065ms step_avg:156.33ms tokens_processed:60702720\n",
      "step:1236/1750 train_time:193226ms step_avg:156.33ms tokens_processed:60751872\n",
      "step:1237/1750 train_time:193384ms step_avg:156.33ms tokens_processed:60801024\n",
      "step:1238/1750 train_time:193544ms step_avg:156.34ms tokens_processed:60850176\n",
      "step:1239/1750 train_time:193704ms step_avg:156.34ms tokens_processed:60899328\n",
      "step:1240/1750 train_time:193865ms step_avg:156.34ms tokens_processed:60948480\n",
      "step:1241/1750 train_time:194025ms step_avg:156.35ms tokens_processed:60997632\n",
      "step:1242/1750 train_time:194185ms step_avg:156.35ms tokens_processed:61046784\n",
      "step:1243/1750 train_time:194344ms step_avg:156.35ms tokens_processed:61095936\n",
      "step:1244/1750 train_time:194503ms step_avg:156.35ms tokens_processed:61145088\n",
      "step:1245/1750 train_time:194664ms step_avg:156.36ms tokens_processed:61194240\n",
      "step:1246/1750 train_time:194823ms step_avg:156.36ms tokens_processed:61243392\n",
      "step:1247/1750 train_time:194982ms step_avg:156.36ms tokens_processed:61292544\n",
      "step:1248/1750 train_time:195143ms step_avg:156.36ms tokens_processed:61341696\n",
      "step:1249/1750 train_time:195304ms step_avg:156.37ms tokens_processed:61390848\n",
      "step:1250/1750 train_time:195464ms step_avg:156.37ms tokens_processed:61440000\n",
      "step:1251/1750 train_time:195623ms step_avg:156.37ms tokens_processed:61489152\n",
      "step:1252/1750 train_time:195784ms step_avg:156.38ms tokens_processed:61538304\n",
      "step:1253/1750 train_time:195943ms step_avg:156.38ms tokens_processed:61587456\n",
      "step:1254/1750 train_time:196105ms step_avg:156.38ms tokens_processed:61636608\n",
      "step:1255/1750 train_time:196265ms step_avg:156.39ms tokens_processed:61685760\n",
      "step:1256/1750 train_time:196426ms step_avg:156.39ms tokens_processed:61734912\n",
      "step:1257/1750 train_time:196586ms step_avg:156.39ms tokens_processed:61784064\n",
      "step:1258/1750 train_time:196746ms step_avg:156.40ms tokens_processed:61833216\n",
      "step:1259/1750 train_time:196906ms step_avg:156.40ms tokens_processed:61882368\n",
      "step:1260/1750 train_time:197066ms step_avg:156.40ms tokens_processed:61931520\n",
      "step:1261/1750 train_time:197228ms step_avg:156.41ms tokens_processed:61980672\n",
      "step:1262/1750 train_time:197387ms step_avg:156.41ms tokens_processed:62029824\n",
      "step:1263/1750 train_time:197546ms step_avg:156.41ms tokens_processed:62078976\n",
      "step:1264/1750 train_time:197707ms step_avg:156.41ms tokens_processed:62128128\n",
      "step:1265/1750 train_time:197867ms step_avg:156.42ms tokens_processed:62177280\n",
      "step:1266/1750 train_time:198028ms step_avg:156.42ms tokens_processed:62226432\n",
      "step:1267/1750 train_time:198189ms step_avg:156.42ms tokens_processed:62275584\n",
      "step:1268/1750 train_time:198348ms step_avg:156.43ms tokens_processed:62324736\n",
      "step:1269/1750 train_time:198508ms step_avg:156.43ms tokens_processed:62373888\n",
      "step:1270/1750 train_time:198667ms step_avg:156.43ms tokens_processed:62423040\n",
      "step:1271/1750 train_time:198828ms step_avg:156.43ms tokens_processed:62472192\n",
      "step:1272/1750 train_time:198991ms step_avg:156.44ms tokens_processed:62521344\n",
      "step:1273/1750 train_time:199151ms step_avg:156.44ms tokens_processed:62570496\n",
      "step:1274/1750 train_time:199311ms step_avg:156.44ms tokens_processed:62619648\n",
      "step:1275/1750 train_time:199471ms step_avg:156.45ms tokens_processed:62668800\n",
      "step:1276/1750 train_time:199632ms step_avg:156.45ms tokens_processed:62717952\n",
      "step:1277/1750 train_time:199792ms step_avg:156.45ms tokens_processed:62767104\n",
      "step:1278/1750 train_time:199953ms step_avg:156.46ms tokens_processed:62816256\n",
      "step:1279/1750 train_time:200113ms step_avg:156.46ms tokens_processed:62865408\n",
      "step:1280/1750 train_time:200273ms step_avg:156.46ms tokens_processed:62914560\n",
      "step:1281/1750 train_time:200435ms step_avg:156.47ms tokens_processed:62963712\n",
      "step:1282/1750 train_time:200594ms step_avg:156.47ms tokens_processed:63012864\n",
      "step:1283/1750 train_time:200753ms step_avg:156.47ms tokens_processed:63062016\n",
      "step:1284/1750 train_time:200916ms step_avg:156.48ms tokens_processed:63111168\n",
      "step:1285/1750 train_time:201075ms step_avg:156.48ms tokens_processed:63160320\n",
      "step:1286/1750 train_time:201237ms step_avg:156.48ms tokens_processed:63209472\n",
      "step:1287/1750 train_time:201398ms step_avg:156.49ms tokens_processed:63258624\n",
      "step:1288/1750 train_time:201556ms step_avg:156.49ms tokens_processed:63307776\n",
      "step:1289/1750 train_time:201717ms step_avg:156.49ms tokens_processed:63356928\n",
      "step:1290/1750 train_time:201876ms step_avg:156.49ms tokens_processed:63406080\n",
      "step:1291/1750 train_time:202037ms step_avg:156.50ms tokens_processed:63455232\n",
      "step:1292/1750 train_time:202199ms step_avg:156.50ms tokens_processed:63504384\n",
      "step:1293/1750 train_time:202359ms step_avg:156.50ms tokens_processed:63553536\n",
      "step:1294/1750 train_time:202521ms step_avg:156.51ms tokens_processed:63602688\n",
      "step:1295/1750 train_time:202680ms step_avg:156.51ms tokens_processed:63651840\n",
      "step:1296/1750 train_time:202840ms step_avg:156.51ms tokens_processed:63700992\n",
      "step:1297/1750 train_time:203001ms step_avg:156.52ms tokens_processed:63750144\n",
      "step:1298/1750 train_time:203162ms step_avg:156.52ms tokens_processed:63799296\n",
      "step:1299/1750 train_time:203323ms step_avg:156.52ms tokens_processed:63848448\n",
      "step:1300/1750 train_time:203481ms step_avg:156.52ms tokens_processed:63897600\n",
      "---validation---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(val_steps):\n\u001b[1;32m     34\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(val_loader)\n\u001b[0;32m---> 35\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlong_bm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshort_bm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m val_steps\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m val_loader\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:395\u001b[0m, in \u001b[0;36mOptimizedModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_has_any_global_hook():\n\u001b[1;32m    386\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:776\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, input_seq, target_seq, long_bm, short_bm)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mlr_mul \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m27.5\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalars\u001b[38;5;241m.\u001b[39mlr_mul \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_seq: Tensor, target_seq: Tensor, long_bm: BlockMask, short_bm: BlockMask):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_seq\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     40\u001b[0m     ve \u001b[38;5;241m=\u001b[39m [value_embed(input_seq) \u001b[38;5;28;01mfor\u001b[39;00m value_embed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embeds]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    params = scheduler(step)\n",
    "    # train_loader.seq_len = params['seq_len']\n",
    "    # train_loader.batch_size = params['batch_size']\n",
    "    \n",
    "    n_windows_long = params['dense']\n",
    "    n_windows_short = params['sparse']\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        \n",
    "        long_bm = get_blockmask(val_seq_len, n_windows_long)\n",
    "        short_bm = get_blockmask(val_seq_len, n_windows_short)\n",
    "\n",
    "        assert args.val_tokens % (val_batch_size * val_seq_len) == 0\n",
    "        val_steps = args.val_tokens // (val_batch_size * val_seq_len)\n",
    "\n",
    "        val_loader = EOSBatchFinder(args.val_files, val_seq_len, val_batch_size, align_to_bos=False).generator()\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, long_bm, short_bm)\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader_gen)\n",
    "    if step == 0: print(\"First inputs retrieved\")\n",
    "    tokens_processed += inputs.numel() * world_size\n",
    "\n",
    "    long_bm = get_blockmask(train_loader.seq_len, n_windows_long)\n",
    "    short_bm = get_blockmask(train_loader.seq_len, n_windows_short)\n",
    "\n",
    "    model(inputs, targets, long_bm, short_bm).backward()\n",
    "\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{tokens_processed}\",\n",
    "        console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1d466-0bbe-458b-8c8f-2edf33c5be28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---validation---\n",
      "step:0/1750 val_loss:10.8258 train_time:1ms step_avg:0.78ms\n",
      "---end of validation---\n",
      "First inputs retrieved torch.Size([8, 6144])\n",
      "step:1/1750 train_time:157ms step_avg:157.05ms tokens_processed:49152\n",
      "step:2/1750 train_time:293ms step_avg:146.59ms tokens_processed:98304\n",
      "step:3/1750 train_time:439ms step_avg:146.48ms tokens_processed:147456\n",
      "step:4/1750 train_time:589ms step_avg:147.36ms tokens_processed:196608\n",
      "step:5/1750 train_time:737ms step_avg:147.46ms tokens_processed:245760\n",
      "step:6/1750 train_time:887ms step_avg:147.80ms tokens_processed:294912\n",
      "step:7/1750 train_time:1037ms step_avg:148.10ms tokens_processed:344064\n",
      "step:8/1750 train_time:1192ms step_avg:149.03ms tokens_processed:393216\n",
      "step:9/1750 train_time:1345ms step_avg:149.42ms tokens_processed:442368\n",
      "step:10/1750 train_time:1495ms step_avg:149.54ms tokens_processed:491520\n",
      "step:11/1750 train_time:1646ms step_avg:149.60ms tokens_processed:540672\n",
      "step:12/1750 train_time:1796ms step_avg:149.63ms tokens_processed:589824\n",
      "step:13/1750 train_time:1947ms step_avg:149.79ms tokens_processed:638976\n",
      "step:14/1750 train_time:2097ms step_avg:149.78ms tokens_processed:688128\n",
      "step:15/1750 train_time:2249ms step_avg:149.94ms tokens_processed:737280\n",
      "step:16/1750 train_time:2401ms step_avg:150.04ms tokens_processed:786432\n",
      "step:17/1750 train_time:2552ms step_avg:150.14ms tokens_processed:835584\n",
      "step:18/1750 train_time:2704ms step_avg:150.21ms tokens_processed:884736\n",
      "step:19/1750 train_time:2854ms step_avg:150.22ms tokens_processed:933888\n",
      "step:20/1750 train_time:3006ms step_avg:150.31ms tokens_processed:983040\n",
      "step:21/1750 train_time:3160ms step_avg:150.48ms tokens_processed:1032192\n",
      "step:22/1750 train_time:3311ms step_avg:150.49ms tokens_processed:1081344\n",
      "step:23/1750 train_time:3461ms step_avg:150.48ms tokens_processed:1130496\n",
      "step:24/1750 train_time:3612ms step_avg:150.50ms tokens_processed:1179648\n",
      "step:25/1750 train_time:3762ms step_avg:150.50ms tokens_processed:1228800\n",
      "step:26/1750 train_time:3913ms step_avg:150.50ms tokens_processed:1277952\n",
      "step:27/1750 train_time:4064ms step_avg:150.53ms tokens_processed:1327104\n",
      "step:28/1750 train_time:4218ms step_avg:150.65ms tokens_processed:1376256\n",
      "step:29/1750 train_time:4368ms step_avg:150.63ms tokens_processed:1425408\n",
      "step:30/1750 train_time:4520ms step_avg:150.66ms tokens_processed:1474560\n",
      "step:31/1750 train_time:4671ms step_avg:150.69ms tokens_processed:1523712\n",
      "step:32/1750 train_time:4825ms step_avg:150.78ms tokens_processed:1572864\n",
      "step:33/1750 train_time:4975ms step_avg:150.77ms tokens_processed:1622016\n",
      "step:34/1750 train_time:5130ms step_avg:150.88ms tokens_processed:1671168\n",
      "step:35/1750 train_time:5279ms step_avg:150.84ms tokens_processed:1720320\n",
      "step:36/1750 train_time:5431ms step_avg:150.86ms tokens_processed:1769472\n",
      "step:37/1750 train_time:5584ms step_avg:150.91ms tokens_processed:1818624\n",
      "step:38/1750 train_time:5735ms step_avg:150.92ms tokens_processed:1867776\n",
      "step:39/1750 train_time:5888ms step_avg:150.96ms tokens_processed:1916928\n",
      "step:40/1750 train_time:6038ms step_avg:150.95ms tokens_processed:1966080\n",
      "step:41/1750 train_time:6190ms step_avg:150.97ms tokens_processed:2015232\n",
      "step:42/1750 train_time:6343ms step_avg:151.02ms tokens_processed:2064384\n",
      "step:43/1750 train_time:6495ms step_avg:151.04ms tokens_processed:2113536\n",
      "step:44/1750 train_time:6648ms step_avg:151.09ms tokens_processed:2162688\n",
      "step:45/1750 train_time:6798ms step_avg:151.06ms tokens_processed:2211840\n",
      "step:46/1750 train_time:6949ms step_avg:151.06ms tokens_processed:2260992\n",
      "step:47/1750 train_time:7100ms step_avg:151.07ms tokens_processed:2310144\n",
      "step:48/1750 train_time:7252ms step_avg:151.09ms tokens_processed:2359296\n",
      "step:49/1750 train_time:7405ms step_avg:151.11ms tokens_processed:2408448\n",
      "step:50/1750 train_time:7558ms step_avg:151.16ms tokens_processed:2457600\n",
      "step:51/1750 train_time:7709ms step_avg:151.15ms tokens_processed:2506752\n",
      "step:52/1750 train_time:7860ms step_avg:151.16ms tokens_processed:2555904\n",
      "step:53/1750 train_time:8013ms step_avg:151.18ms tokens_processed:2605056\n",
      "step:54/1750 train_time:8166ms step_avg:151.21ms tokens_processed:2654208\n",
      "step:55/1750 train_time:8316ms step_avg:151.19ms tokens_processed:2703360\n",
      "step:56/1750 train_time:8468ms step_avg:151.21ms tokens_processed:2752512\n",
      "step:57/1750 train_time:8619ms step_avg:151.22ms tokens_processed:2801664\n",
      "step:58/1750 train_time:8771ms step_avg:151.23ms tokens_processed:2850816\n",
      "step:59/1750 train_time:8926ms step_avg:151.29ms tokens_processed:2899968\n",
      "step:60/1750 train_time:9076ms step_avg:151.27ms tokens_processed:2949120\n",
      "step:61/1750 train_time:9228ms step_avg:151.28ms tokens_processed:2998272\n",
      "step:62/1750 train_time:9384ms step_avg:151.36ms tokens_processed:3047424\n",
      "step:63/1750 train_time:9537ms step_avg:151.39ms tokens_processed:3096576\n",
      "step:64/1750 train_time:9689ms step_avg:151.40ms tokens_processed:3145728\n",
      "step:65/1750 train_time:9842ms step_avg:151.41ms tokens_processed:3194880\n",
      "step:66/1750 train_time:9994ms step_avg:151.42ms tokens_processed:3244032\n",
      "step:67/1750 train_time:10146ms step_avg:151.43ms tokens_processed:3293184\n",
      "step:68/1750 train_time:10298ms step_avg:151.45ms tokens_processed:3342336\n",
      "step:69/1750 train_time:10449ms step_avg:151.43ms tokens_processed:3391488\n",
      "step:70/1750 train_time:10601ms step_avg:151.44ms tokens_processed:3440640\n",
      "step:71/1750 train_time:10753ms step_avg:151.45ms tokens_processed:3489792\n",
      "step:72/1750 train_time:10906ms step_avg:151.47ms tokens_processed:3538944\n",
      "step:73/1750 train_time:11059ms step_avg:151.50ms tokens_processed:3588096\n",
      "step:74/1750 train_time:11210ms step_avg:151.49ms tokens_processed:3637248\n",
      "step:75/1750 train_time:11363ms step_avg:151.50ms tokens_processed:3686400\n",
      "step:76/1750 train_time:11517ms step_avg:151.54ms tokens_processed:3735552\n",
      "step:77/1750 train_time:11667ms step_avg:151.53ms tokens_processed:3784704\n",
      "step:78/1750 train_time:11820ms step_avg:151.53ms tokens_processed:3833856\n",
      "step:79/1750 train_time:11972ms step_avg:151.55ms tokens_processed:3883008\n",
      "step:80/1750 train_time:12125ms step_avg:151.57ms tokens_processed:3932160\n",
      "step:81/1750 train_time:12277ms step_avg:151.57ms tokens_processed:3981312\n",
      "step:82/1750 train_time:12430ms step_avg:151.58ms tokens_processed:4030464\n",
      "step:83/1750 train_time:12582ms step_avg:151.59ms tokens_processed:4079616\n",
      "step:84/1750 train_time:12733ms step_avg:151.59ms tokens_processed:4128768\n",
      "step:85/1750 train_time:12886ms step_avg:151.60ms tokens_processed:4177920\n",
      "step:86/1750 train_time:13039ms step_avg:151.61ms tokens_processed:4227072\n",
      "step:87/1750 train_time:13191ms step_avg:151.62ms tokens_processed:4276224\n",
      "step:88/1750 train_time:13343ms step_avg:151.62ms tokens_processed:4325376\n",
      "step:89/1750 train_time:13496ms step_avg:151.64ms tokens_processed:4374528\n",
      "step:90/1750 train_time:13646ms step_avg:151.63ms tokens_processed:4423680\n",
      "step:91/1750 train_time:13798ms step_avg:151.62ms tokens_processed:4472832\n",
      "step:92/1750 train_time:13949ms step_avg:151.62ms tokens_processed:4521984\n",
      "step:93/1750 train_time:14102ms step_avg:151.63ms tokens_processed:4571136\n",
      "step:94/1750 train_time:14254ms step_avg:151.64ms tokens_processed:4620288\n",
      "step:95/1750 train_time:14406ms step_avg:151.65ms tokens_processed:4669440\n",
      "step:96/1750 train_time:14558ms step_avg:151.64ms tokens_processed:4718592\n",
      "step:97/1750 train_time:14711ms step_avg:151.66ms tokens_processed:4767744\n",
      "step:98/1750 train_time:14863ms step_avg:151.67ms tokens_processed:4816896\n",
      "step:99/1750 train_time:15016ms step_avg:151.67ms tokens_processed:4866048\n",
      "step:100/1750 train_time:15168ms step_avg:151.68ms tokens_processed:4915200\n",
      "---validation---\n",
      "step:100/1750 val_loss:5.9459 train_time:15176ms step_avg:151.76ms\n",
      "---end of validation---\n",
      "step:101/1750 train_time:15335ms step_avg:151.83ms tokens_processed:4964352\n",
      "step:102/1750 train_time:15487ms step_avg:151.83ms tokens_processed:5013504\n",
      "step:103/1750 train_time:15639ms step_avg:151.84ms tokens_processed:5062656\n",
      "step:104/1750 train_time:15793ms step_avg:151.85ms tokens_processed:5111808\n",
      "step:105/1750 train_time:15945ms step_avg:151.86ms tokens_processed:5160960\n",
      "step:106/1750 train_time:16099ms step_avg:151.88ms tokens_processed:5210112\n",
      "step:107/1750 train_time:16253ms step_avg:151.90ms tokens_processed:5259264\n",
      "step:108/1750 train_time:16405ms step_avg:151.89ms tokens_processed:5308416\n",
      "step:109/1750 train_time:16561ms step_avg:151.93ms tokens_processed:5357568\n",
      "step:110/1750 train_time:16713ms step_avg:151.93ms tokens_processed:5406720\n",
      "step:111/1750 train_time:16865ms step_avg:151.94ms tokens_processed:5455872\n",
      "step:112/1750 train_time:17018ms step_avg:151.95ms tokens_processed:5505024\n",
      "step:113/1750 train_time:17171ms step_avg:151.95ms tokens_processed:5554176\n",
      "step:114/1750 train_time:17323ms step_avg:151.95ms tokens_processed:5603328\n",
      "step:115/1750 train_time:17477ms step_avg:151.97ms tokens_processed:5652480\n",
      "step:116/1750 train_time:17629ms step_avg:151.98ms tokens_processed:5701632\n",
      "step:117/1750 train_time:17787ms step_avg:152.02ms tokens_processed:5750784\n",
      "step:118/1750 train_time:17937ms step_avg:152.01ms tokens_processed:5799936\n",
      "step:119/1750 train_time:18091ms step_avg:152.02ms tokens_processed:5849088\n",
      "step:120/1750 train_time:18243ms step_avg:152.02ms tokens_processed:5898240\n",
      "step:121/1750 train_time:18397ms step_avg:152.04ms tokens_processed:5947392\n",
      "step:122/1750 train_time:18550ms step_avg:152.05ms tokens_processed:5996544\n",
      "step:123/1750 train_time:18704ms step_avg:152.07ms tokens_processed:6045696\n",
      "step:124/1750 train_time:18857ms step_avg:152.07ms tokens_processed:6094848\n",
      "step:125/1750 train_time:19009ms step_avg:152.08ms tokens_processed:6144000\n",
      "step:126/1750 train_time:19163ms step_avg:152.09ms tokens_processed:6193152\n",
      "step:127/1750 train_time:19316ms step_avg:152.09ms tokens_processed:6242304\n",
      "step:128/1750 train_time:19468ms step_avg:152.10ms tokens_processed:6291456\n",
      "step:129/1750 train_time:19622ms step_avg:152.11ms tokens_processed:6340608\n",
      "step:130/1750 train_time:19776ms step_avg:152.12ms tokens_processed:6389760\n",
      "step:131/1750 train_time:19930ms step_avg:152.14ms tokens_processed:6438912\n",
      "step:132/1750 train_time:20081ms step_avg:152.13ms tokens_processed:6488064\n",
      "step:133/1750 train_time:20234ms step_avg:152.14ms tokens_processed:6537216\n",
      "step:134/1750 train_time:20387ms step_avg:152.14ms tokens_processed:6586368\n",
      "step:135/1750 train_time:20540ms step_avg:152.15ms tokens_processed:6635520\n",
      "step:136/1750 train_time:20693ms step_avg:152.16ms tokens_processed:6684672\n",
      "step:137/1750 train_time:20848ms step_avg:152.17ms tokens_processed:6733824\n",
      "step:138/1750 train_time:21001ms step_avg:152.18ms tokens_processed:6782976\n",
      "step:139/1750 train_time:21155ms step_avg:152.19ms tokens_processed:6832128\n",
      "step:140/1750 train_time:21306ms step_avg:152.19ms tokens_processed:6881280\n",
      "step:141/1750 train_time:21460ms step_avg:152.20ms tokens_processed:6930432\n",
      "step:142/1750 train_time:21615ms step_avg:152.21ms tokens_processed:6979584\n",
      "step:143/1750 train_time:21766ms step_avg:152.21ms tokens_processed:7028736\n",
      "step:144/1750 train_time:21919ms step_avg:152.21ms tokens_processed:7077888\n",
      "step:145/1750 train_time:22072ms step_avg:152.22ms tokens_processed:7127040\n",
      "step:146/1750 train_time:22224ms step_avg:152.22ms tokens_processed:7176192\n",
      "step:147/1750 train_time:22378ms step_avg:152.23ms tokens_processed:7225344\n",
      "step:148/1750 train_time:22530ms step_avg:152.23ms tokens_processed:7274496\n",
      "step:149/1750 train_time:22684ms step_avg:152.24ms tokens_processed:7323648\n",
      "step:150/1750 train_time:22839ms step_avg:152.26ms tokens_processed:7372800\n",
      "step:151/1750 train_time:22991ms step_avg:152.26ms tokens_processed:7421952\n",
      "step:152/1750 train_time:23145ms step_avg:152.27ms tokens_processed:7471104\n",
      "step:153/1750 train_time:23297ms step_avg:152.27ms tokens_processed:7520256\n",
      "step:154/1750 train_time:23451ms step_avg:152.28ms tokens_processed:7569408\n",
      "step:155/1750 train_time:23603ms step_avg:152.28ms tokens_processed:7618560\n",
      "step:156/1750 train_time:23757ms step_avg:152.29ms tokens_processed:7667712\n",
      "step:157/1750 train_time:23910ms step_avg:152.30ms tokens_processed:7716864\n",
      "step:158/1750 train_time:24065ms step_avg:152.31ms tokens_processed:7766016\n",
      "step:159/1750 train_time:24219ms step_avg:152.32ms tokens_processed:7815168\n",
      "step:160/1750 train_time:24374ms step_avg:152.34ms tokens_processed:7864320\n",
      "step:161/1750 train_time:24525ms step_avg:152.33ms tokens_processed:7913472\n",
      "step:162/1750 train_time:24679ms step_avg:152.34ms tokens_processed:7962624\n",
      "step:163/1750 train_time:24830ms step_avg:152.33ms tokens_processed:8011776\n",
      "step:164/1750 train_time:24985ms step_avg:152.35ms tokens_processed:8060928\n",
      "step:165/1750 train_time:25138ms step_avg:152.35ms tokens_processed:8110080\n",
      "step:166/1750 train_time:25292ms step_avg:152.36ms tokens_processed:8159232\n",
      "step:167/1750 train_time:25445ms step_avg:152.37ms tokens_processed:8208384\n",
      "step:168/1750 train_time:25599ms step_avg:152.37ms tokens_processed:8257536\n",
      "step:169/1750 train_time:25752ms step_avg:152.38ms tokens_processed:8306688\n",
      "step:170/1750 train_time:25903ms step_avg:152.37ms tokens_processed:8355840\n",
      "step:171/1750 train_time:26061ms step_avg:152.40ms tokens_processed:8404992\n",
      "step:172/1750 train_time:26213ms step_avg:152.40ms tokens_processed:8454144\n",
      "step:173/1750 train_time:26367ms step_avg:152.41ms tokens_processed:8503296\n",
      "step:174/1750 train_time:26520ms step_avg:152.41ms tokens_processed:8552448\n",
      "step:175/1750 train_time:26674ms step_avg:152.42ms tokens_processed:8601600\n",
      "step:176/1750 train_time:26825ms step_avg:152.41ms tokens_processed:8650752\n",
      "step:177/1750 train_time:26978ms step_avg:152.42ms tokens_processed:8699904\n",
      "step:178/1750 train_time:27133ms step_avg:152.44ms tokens_processed:8749056\n",
      "step:179/1750 train_time:27286ms step_avg:152.44ms tokens_processed:8798208\n",
      "step:180/1750 train_time:27440ms step_avg:152.44ms tokens_processed:8847360\n",
      "step:181/1750 train_time:27594ms step_avg:152.45ms tokens_processed:8896512\n",
      "step:182/1750 train_time:27747ms step_avg:152.45ms tokens_processed:8945664\n",
      "step:183/1750 train_time:27902ms step_avg:152.47ms tokens_processed:8994816\n",
      "step:184/1750 train_time:28053ms step_avg:152.46ms tokens_processed:9043968\n",
      "step:185/1750 train_time:28206ms step_avg:152.46ms tokens_processed:9093120\n",
      "step:186/1750 train_time:28359ms step_avg:152.47ms tokens_processed:9142272\n",
      "step:187/1750 train_time:28514ms step_avg:152.48ms tokens_processed:9191424\n",
      "step:188/1750 train_time:28666ms step_avg:152.48ms tokens_processed:9240576\n",
      "step:189/1750 train_time:28819ms step_avg:152.48ms tokens_processed:9289728\n",
      "step:190/1750 train_time:28972ms step_avg:152.49ms tokens_processed:9338880\n",
      "step:191/1750 train_time:29125ms step_avg:152.49ms tokens_processed:9388032\n",
      "step:192/1750 train_time:29278ms step_avg:152.49ms tokens_processed:9437184\n",
      "step:193/1750 train_time:29430ms step_avg:152.49ms tokens_processed:9486336\n",
      "step:194/1750 train_time:29584ms step_avg:152.50ms tokens_processed:9535488\n",
      "step:195/1750 train_time:29739ms step_avg:152.51ms tokens_processed:9584640\n",
      "step:196/1750 train_time:29894ms step_avg:152.52ms tokens_processed:9633792\n",
      "step:197/1750 train_time:30046ms step_avg:152.52ms tokens_processed:9682944\n",
      "step:198/1750 train_time:30199ms step_avg:152.52ms tokens_processed:9732096\n",
      "step:199/1750 train_time:30353ms step_avg:152.53ms tokens_processed:9781248\n",
      "step:200/1750 train_time:30505ms step_avg:152.53ms tokens_processed:9830400\n",
      "---validation---\n",
      "step:200/1750 val_loss:5.4610 train_time:30512ms step_avg:152.56ms\n",
      "---end of validation---\n",
      "step:201/1750 train_time:30665ms step_avg:152.56ms tokens_processed:9879552\n",
      "step:202/1750 train_time:30817ms step_avg:152.56ms tokens_processed:9928704\n",
      "step:203/1750 train_time:30971ms step_avg:152.56ms tokens_processed:9977856\n",
      "step:204/1750 train_time:31125ms step_avg:152.57ms tokens_processed:10027008\n",
      "step:205/1750 train_time:31276ms step_avg:152.57ms tokens_processed:10076160\n",
      "step:206/1750 train_time:31429ms step_avg:152.57ms tokens_processed:10125312\n",
      "step:207/1750 train_time:31584ms step_avg:152.58ms tokens_processed:10174464\n",
      "step:208/1750 train_time:31740ms step_avg:152.59ms tokens_processed:10223616\n",
      "step:209/1750 train_time:31892ms step_avg:152.59ms tokens_processed:10272768\n",
      "step:210/1750 train_time:32047ms step_avg:152.61ms tokens_processed:10321920\n",
      "step:211/1750 train_time:32200ms step_avg:152.61ms tokens_processed:10371072\n",
      "step:212/1750 train_time:32354ms step_avg:152.61ms tokens_processed:10420224\n",
      "step:213/1750 train_time:32506ms step_avg:152.61ms tokens_processed:10469376\n",
      "step:214/1750 train_time:32660ms step_avg:152.62ms tokens_processed:10518528\n",
      "step:215/1750 train_time:32814ms step_avg:152.62ms tokens_processed:10567680\n",
      "step:216/1750 train_time:32970ms step_avg:152.64ms tokens_processed:10616832\n",
      "step:217/1750 train_time:33123ms step_avg:152.64ms tokens_processed:10665984\n",
      "step:218/1750 train_time:33277ms step_avg:152.65ms tokens_processed:10715136\n",
      "step:219/1750 train_time:33430ms step_avg:152.65ms tokens_processed:10764288\n",
      "step:220/1750 train_time:33585ms step_avg:152.66ms tokens_processed:10813440\n",
      "step:221/1750 train_time:33740ms step_avg:152.67ms tokens_processed:10862592\n",
      "step:222/1750 train_time:33891ms step_avg:152.66ms tokens_processed:10911744\n",
      "step:223/1750 train_time:34050ms step_avg:152.69ms tokens_processed:10960896\n",
      "step:224/1750 train_time:34201ms step_avg:152.68ms tokens_processed:11010048\n",
      "step:225/1750 train_time:34352ms step_avg:152.68ms tokens_processed:11059200\n",
      "step:226/1750 train_time:34507ms step_avg:152.69ms tokens_processed:11108352\n",
      "step:227/1750 train_time:34659ms step_avg:152.68ms tokens_processed:11157504\n",
      "step:228/1750 train_time:34814ms step_avg:152.69ms tokens_processed:11206656\n",
      "step:229/1750 train_time:34968ms step_avg:152.70ms tokens_processed:11255808\n",
      "step:230/1750 train_time:35124ms step_avg:152.71ms tokens_processed:11304960\n",
      "step:231/1750 train_time:35277ms step_avg:152.71ms tokens_processed:11354112\n",
      "step:232/1750 train_time:35430ms step_avg:152.72ms tokens_processed:11403264\n",
      "step:233/1750 train_time:35585ms step_avg:152.73ms tokens_processed:11452416\n",
      "step:234/1750 train_time:35738ms step_avg:152.73ms tokens_processed:11501568\n",
      "step:235/1750 train_time:35892ms step_avg:152.73ms tokens_processed:11550720\n",
      "step:236/1750 train_time:36048ms step_avg:152.75ms tokens_processed:11599872\n",
      "step:237/1750 train_time:36201ms step_avg:152.75ms tokens_processed:11649024\n",
      "step:238/1750 train_time:36353ms step_avg:152.74ms tokens_processed:11698176\n",
      "step:239/1750 train_time:36506ms step_avg:152.75ms tokens_processed:11747328\n",
      "step:240/1750 train_time:36659ms step_avg:152.75ms tokens_processed:11796480\n",
      "step:241/1750 train_time:36815ms step_avg:152.76ms tokens_processed:11845632\n",
      "step:242/1750 train_time:36969ms step_avg:152.76ms tokens_processed:11894784\n",
      "step:243/1750 train_time:37123ms step_avg:152.77ms tokens_processed:11943936\n",
      "step:244/1750 train_time:37278ms step_avg:152.78ms tokens_processed:11993088\n",
      "step:245/1750 train_time:37436ms step_avg:152.80ms tokens_processed:12042240\n",
      "step:246/1750 train_time:37590ms step_avg:152.80ms tokens_processed:12091392\n",
      "step:247/1750 train_time:37743ms step_avg:152.81ms tokens_processed:12140544\n",
      "step:248/1750 train_time:37897ms step_avg:152.81ms tokens_processed:12189696\n",
      "step:249/1750 train_time:38051ms step_avg:152.81ms tokens_processed:12238848\n",
      "step:250/1750 train_time:38207ms step_avg:152.83ms tokens_processed:12288000\n",
      "step:251/1750 train_time:38358ms step_avg:152.82ms tokens_processed:12337152\n",
      "step:252/1750 train_time:38513ms step_avg:152.83ms tokens_processed:12386304\n",
      "step:253/1750 train_time:38669ms step_avg:152.84ms tokens_processed:12435456\n",
      "step:254/1750 train_time:38820ms step_avg:152.84ms tokens_processed:12484608\n",
      "step:255/1750 train_time:38974ms step_avg:152.84ms tokens_processed:12533760\n",
      "step:256/1750 train_time:39130ms step_avg:152.85ms tokens_processed:12582912\n",
      "step:257/1750 train_time:39286ms step_avg:152.86ms tokens_processed:12632064\n",
      "step:258/1750 train_time:39437ms step_avg:152.86ms tokens_processed:12681216\n",
      "step:259/1750 train_time:39593ms step_avg:152.87ms tokens_processed:12730368\n",
      "step:260/1750 train_time:39746ms step_avg:152.87ms tokens_processed:12779520\n",
      "step:261/1750 train_time:39900ms step_avg:152.87ms tokens_processed:12828672\n",
      "step:262/1750 train_time:40052ms step_avg:152.87ms tokens_processed:12877824\n",
      "step:263/1750 train_time:40211ms step_avg:152.89ms tokens_processed:12926976\n",
      "step:264/1750 train_time:40365ms step_avg:152.90ms tokens_processed:12976128\n",
      "step:265/1750 train_time:40521ms step_avg:152.91ms tokens_processed:13025280\n",
      "step:266/1750 train_time:40672ms step_avg:152.90ms tokens_processed:13074432\n",
      "step:267/1750 train_time:40827ms step_avg:152.91ms tokens_processed:13123584\n",
      "step:268/1750 train_time:40979ms step_avg:152.91ms tokens_processed:13172736\n",
      "step:269/1750 train_time:41133ms step_avg:152.91ms tokens_processed:13221888\n",
      "step:270/1750 train_time:41288ms step_avg:152.92ms tokens_processed:13271040\n",
      "step:271/1750 train_time:41443ms step_avg:152.93ms tokens_processed:13320192\n",
      "step:272/1750 train_time:41595ms step_avg:152.92ms tokens_processed:13369344\n",
      "step:273/1750 train_time:41749ms step_avg:152.93ms tokens_processed:13418496\n",
      "step:274/1750 train_time:41906ms step_avg:152.94ms tokens_processed:13467648\n",
      "step:275/1750 train_time:42056ms step_avg:152.93ms tokens_processed:13516800\n",
      "step:276/1750 train_time:42211ms step_avg:152.94ms tokens_processed:13565952\n",
      "step:277/1750 train_time:42366ms step_avg:152.95ms tokens_processed:13615104\n",
      "step:278/1750 train_time:42519ms step_avg:152.95ms tokens_processed:13664256\n",
      "step:279/1750 train_time:42674ms step_avg:152.96ms tokens_processed:13713408\n",
      "step:280/1750 train_time:42828ms step_avg:152.96ms tokens_processed:13762560\n",
      "step:281/1750 train_time:42983ms step_avg:152.96ms tokens_processed:13811712\n",
      "step:282/1750 train_time:43138ms step_avg:152.97ms tokens_processed:13860864\n",
      "step:283/1750 train_time:43292ms step_avg:152.98ms tokens_processed:13910016\n",
      "step:284/1750 train_time:43445ms step_avg:152.97ms tokens_processed:13959168\n",
      "step:285/1750 train_time:43599ms step_avg:152.98ms tokens_processed:14008320\n",
      "step:286/1750 train_time:43752ms step_avg:152.98ms tokens_processed:14057472\n",
      "step:287/1750 train_time:43908ms step_avg:152.99ms tokens_processed:14106624\n",
      "step:288/1750 train_time:44062ms step_avg:152.99ms tokens_processed:14155776\n",
      "step:289/1750 train_time:44217ms step_avg:153.00ms tokens_processed:14204928\n",
      "step:290/1750 train_time:44370ms step_avg:153.00ms tokens_processed:14254080\n",
      "step:291/1750 train_time:44525ms step_avg:153.01ms tokens_processed:14303232\n",
      "step:292/1750 train_time:44678ms step_avg:153.01ms tokens_processed:14352384\n",
      "step:293/1750 train_time:44838ms step_avg:153.03ms tokens_processed:14401536\n",
      "step:294/1750 train_time:44991ms step_avg:153.03ms tokens_processed:14450688\n",
      "step:295/1750 train_time:45145ms step_avg:153.04ms tokens_processed:14499840\n",
      "step:296/1750 train_time:45302ms step_avg:153.05ms tokens_processed:14548992\n",
      "step:297/1750 train_time:45453ms step_avg:153.04ms tokens_processed:14598144\n",
      "step:298/1750 train_time:45608ms step_avg:153.05ms tokens_processed:14647296\n",
      "step:299/1750 train_time:45762ms step_avg:153.05ms tokens_processed:14696448\n",
      "step:300/1750 train_time:45916ms step_avg:153.05ms tokens_processed:14745600\n",
      "---validation---\n",
      "step:300/1750 val_loss:5.1890 train_time:45923ms step_avg:153.08ms\n",
      "---end of validation---\n",
      "step:301/1750 train_time:46074ms step_avg:153.07ms tokens_processed:14794752\n",
      "step:302/1750 train_time:46227ms step_avg:153.07ms tokens_processed:14843904\n",
      "step:303/1750 train_time:46379ms step_avg:153.06ms tokens_processed:14893056\n",
      "step:304/1750 train_time:46532ms step_avg:153.06ms tokens_processed:14942208\n",
      "step:305/1750 train_time:46685ms step_avg:153.07ms tokens_processed:14991360\n",
      "step:306/1750 train_time:46838ms step_avg:153.07ms tokens_processed:15040512\n",
      "step:307/1750 train_time:46992ms step_avg:153.07ms tokens_processed:15089664\n",
      "step:308/1750 train_time:47145ms step_avg:153.07ms tokens_processed:15138816\n",
      "step:309/1750 train_time:47297ms step_avg:153.06ms tokens_processed:15187968\n",
      "step:310/1750 train_time:47449ms step_avg:153.06ms tokens_processed:15237120\n",
      "step:311/1750 train_time:47602ms step_avg:153.06ms tokens_processed:15286272\n",
      "step:312/1750 train_time:47757ms step_avg:153.07ms tokens_processed:15335424\n",
      "step:313/1750 train_time:47913ms step_avg:153.08ms tokens_processed:15384576\n",
      "step:314/1750 train_time:48064ms step_avg:153.07ms tokens_processed:15433728\n",
      "step:315/1750 train_time:48219ms step_avg:153.08ms tokens_processed:15482880\n",
      "step:316/1750 train_time:48371ms step_avg:153.07ms tokens_processed:15532032\n",
      "step:317/1750 train_time:48523ms step_avg:153.07ms tokens_processed:15581184\n",
      "step:318/1750 train_time:48676ms step_avg:153.07ms tokens_processed:15630336\n",
      "step:319/1750 train_time:48829ms step_avg:153.07ms tokens_processed:15679488\n",
      "step:320/1750 train_time:48983ms step_avg:153.07ms tokens_processed:15728640\n",
      "step:321/1750 train_time:49135ms step_avg:153.07ms tokens_processed:15777792\n",
      "step:322/1750 train_time:49288ms step_avg:153.07ms tokens_processed:15826944\n",
      "step:323/1750 train_time:49442ms step_avg:153.07ms tokens_processed:15876096\n",
      "step:324/1750 train_time:49595ms step_avg:153.07ms tokens_processed:15925248\n",
      "step:325/1750 train_time:49748ms step_avg:153.07ms tokens_processed:15974400\n",
      "step:326/1750 train_time:49900ms step_avg:153.07ms tokens_processed:16023552\n",
      "step:327/1750 train_time:50054ms step_avg:153.07ms tokens_processed:16072704\n",
      "step:328/1750 train_time:50207ms step_avg:153.07ms tokens_processed:16121856\n",
      "step:329/1750 train_time:50360ms step_avg:153.07ms tokens_processed:16171008\n",
      "step:330/1750 train_time:50514ms step_avg:153.07ms tokens_processed:16220160\n",
      "step:331/1750 train_time:50667ms step_avg:153.07ms tokens_processed:16269312\n",
      "step:332/1750 train_time:50819ms step_avg:153.07ms tokens_processed:16318464\n",
      "step:333/1750 train_time:50974ms step_avg:153.07ms tokens_processed:16367616\n",
      "step:334/1750 train_time:51126ms step_avg:153.07ms tokens_processed:16416768\n",
      "step:335/1750 train_time:51279ms step_avg:153.07ms tokens_processed:16465920\n",
      "step:336/1750 train_time:51432ms step_avg:153.07ms tokens_processed:16515072\n",
      "step:337/1750 train_time:51585ms step_avg:153.07ms tokens_processed:16564224\n",
      "step:338/1750 train_time:51738ms step_avg:153.07ms tokens_processed:16613376\n",
      "step:339/1750 train_time:51892ms step_avg:153.07ms tokens_processed:16662528\n",
      "step:340/1750 train_time:52046ms step_avg:153.08ms tokens_processed:16711680\n",
      "step:341/1750 train_time:52198ms step_avg:153.07ms tokens_processed:16760832\n",
      "step:342/1750 train_time:52354ms step_avg:153.08ms tokens_processed:16809984\n",
      "step:343/1750 train_time:52505ms step_avg:153.08ms tokens_processed:16859136\n",
      "step:344/1750 train_time:52660ms step_avg:153.08ms tokens_processed:16908288\n",
      "step:345/1750 train_time:52815ms step_avg:153.09ms tokens_processed:16957440\n",
      "step:346/1750 train_time:52966ms step_avg:153.08ms tokens_processed:17006592\n",
      "step:347/1750 train_time:53118ms step_avg:153.08ms tokens_processed:17055744\n",
      "step:348/1750 train_time:53271ms step_avg:153.08ms tokens_processed:17104896\n",
      "step:349/1750 train_time:53423ms step_avg:153.07ms tokens_processed:17154048\n",
      "step:350/1750 train_time:53577ms step_avg:153.08ms tokens_processed:17203200\n",
      "step:351/1750 train_time:53730ms step_avg:153.08ms tokens_processed:17252352\n",
      "step:352/1750 train_time:53883ms step_avg:153.08ms tokens_processed:17301504\n",
      "step:353/1750 train_time:54036ms step_avg:153.08ms tokens_processed:17350656\n",
      "step:354/1750 train_time:54192ms step_avg:153.08ms tokens_processed:17399808\n",
      "step:355/1750 train_time:54343ms step_avg:153.08ms tokens_processed:17448960\n",
      "step:356/1750 train_time:54496ms step_avg:153.08ms tokens_processed:17498112\n",
      "step:357/1750 train_time:54649ms step_avg:153.08ms tokens_processed:17547264\n",
      "step:358/1750 train_time:54801ms step_avg:153.08ms tokens_processed:17596416\n",
      "step:359/1750 train_time:54957ms step_avg:153.08ms tokens_processed:17645568\n",
      "step:360/1750 train_time:55109ms step_avg:153.08ms tokens_processed:17694720\n",
      "step:361/1750 train_time:55263ms step_avg:153.08ms tokens_processed:17743872\n",
      "step:362/1750 train_time:55418ms step_avg:153.09ms tokens_processed:17793024\n",
      "step:363/1750 train_time:55568ms step_avg:153.08ms tokens_processed:17842176\n",
      "step:364/1750 train_time:55726ms step_avg:153.09ms tokens_processed:17891328\n",
      "step:365/1750 train_time:55877ms step_avg:153.09ms tokens_processed:17940480\n",
      "step:366/1750 train_time:56028ms step_avg:153.08ms tokens_processed:17989632\n",
      "step:367/1750 train_time:56184ms step_avg:153.09ms tokens_processed:18038784\n",
      "step:368/1750 train_time:56335ms step_avg:153.08ms tokens_processed:18087936\n",
      "step:369/1750 train_time:56490ms step_avg:153.09ms tokens_processed:18137088\n",
      "step:370/1750 train_time:56643ms step_avg:153.09ms tokens_processed:18186240\n",
      "step:371/1750 train_time:56795ms step_avg:153.09ms tokens_processed:18235392\n",
      "step:372/1750 train_time:56948ms step_avg:153.09ms tokens_processed:18284544\n",
      "step:373/1750 train_time:57100ms step_avg:153.08ms tokens_processed:18333696\n",
      "step:374/1750 train_time:57256ms step_avg:153.09ms tokens_processed:18382848\n",
      "step:375/1750 train_time:57408ms step_avg:153.09ms tokens_processed:18432000\n",
      "step:376/1750 train_time:57561ms step_avg:153.09ms tokens_processed:18481152\n",
      "step:377/1750 train_time:57714ms step_avg:153.09ms tokens_processed:18530304\n",
      "step:378/1750 train_time:57866ms step_avg:153.09ms tokens_processed:18579456\n",
      "step:379/1750 train_time:58021ms step_avg:153.09ms tokens_processed:18628608\n",
      "step:380/1750 train_time:58173ms step_avg:153.09ms tokens_processed:18677760\n",
      "step:381/1750 train_time:58325ms step_avg:153.08ms tokens_processed:18726912\n",
      "step:382/1750 train_time:58481ms step_avg:153.09ms tokens_processed:18776064\n",
      "step:383/1750 train_time:58635ms step_avg:153.09ms tokens_processed:18825216\n",
      "step:384/1750 train_time:58786ms step_avg:153.09ms tokens_processed:18874368\n",
      "step:385/1750 train_time:58937ms step_avg:153.08ms tokens_processed:18923520\n",
      "step:386/1750 train_time:59093ms step_avg:153.09ms tokens_processed:18972672\n",
      "step:387/1750 train_time:59244ms step_avg:153.08ms tokens_processed:19021824\n",
      "step:388/1750 train_time:59397ms step_avg:153.09ms tokens_processed:19070976\n",
      "step:389/1750 train_time:59551ms step_avg:153.09ms tokens_processed:19120128\n",
      "step:390/1750 train_time:59707ms step_avg:153.09ms tokens_processed:19169280\n",
      "step:391/1750 train_time:59857ms step_avg:153.09ms tokens_processed:19218432\n",
      "step:392/1750 train_time:60012ms step_avg:153.09ms tokens_processed:19267584\n",
      "step:393/1750 train_time:60164ms step_avg:153.09ms tokens_processed:19316736\n",
      "step:394/1750 train_time:60318ms step_avg:153.09ms tokens_processed:19365888\n",
      "step:395/1750 train_time:60471ms step_avg:153.09ms tokens_processed:19415040\n",
      "step:396/1750 train_time:60624ms step_avg:153.09ms tokens_processed:19464192\n",
      "step:397/1750 train_time:60778ms step_avg:153.09ms tokens_processed:19513344\n",
      "step:398/1750 train_time:60935ms step_avg:153.10ms tokens_processed:19562496\n",
      "step:399/1750 train_time:61087ms step_avg:153.10ms tokens_processed:19611648\n",
      "step:400/1750 train_time:61238ms step_avg:153.09ms tokens_processed:19660800\n",
      "---validation---\n",
      "step:400/1750 val_loss:5.0092 train_time:61245ms step_avg:153.11ms\n",
      "---end of validation---\n",
      "step:401/1750 train_time:61396ms step_avg:153.11ms tokens_processed:19709952\n",
      "step:402/1750 train_time:61549ms step_avg:153.11ms tokens_processed:19759104\n",
      "step:403/1750 train_time:61702ms step_avg:153.11ms tokens_processed:19808256\n",
      "step:404/1750 train_time:61856ms step_avg:153.11ms tokens_processed:19857408\n",
      "step:405/1750 train_time:62007ms step_avg:153.10ms tokens_processed:19906560\n",
      "step:406/1750 train_time:62162ms step_avg:153.11ms tokens_processed:19955712\n",
      "step:407/1750 train_time:62316ms step_avg:153.11ms tokens_processed:20004864\n",
      "step:408/1750 train_time:62469ms step_avg:153.11ms tokens_processed:20054016\n",
      "step:409/1750 train_time:62622ms step_avg:153.11ms tokens_processed:20103168\n",
      "step:410/1750 train_time:62774ms step_avg:153.11ms tokens_processed:20152320\n",
      "step:411/1750 train_time:62928ms step_avg:153.11ms tokens_processed:20201472\n",
      "step:412/1750 train_time:63081ms step_avg:153.11ms tokens_processed:20250624\n",
      "step:413/1750 train_time:63236ms step_avg:153.11ms tokens_processed:20299776\n",
      "step:414/1750 train_time:63389ms step_avg:153.11ms tokens_processed:20348928\n",
      "step:415/1750 train_time:63544ms step_avg:153.12ms tokens_processed:20398080\n",
      "step:416/1750 train_time:63696ms step_avg:153.12ms tokens_processed:20447232\n",
      "step:417/1750 train_time:63851ms step_avg:153.12ms tokens_processed:20496384\n",
      "step:418/1750 train_time:64003ms step_avg:153.12ms tokens_processed:20545536\n",
      "step:419/1750 train_time:64156ms step_avg:153.12ms tokens_processed:20594688\n",
      "step:420/1750 train_time:64310ms step_avg:153.12ms tokens_processed:20643840\n",
      "step:421/1750 train_time:64463ms step_avg:153.12ms tokens_processed:20692992\n",
      "step:422/1750 train_time:64617ms step_avg:153.12ms tokens_processed:20742144\n",
      "step:423/1750 train_time:64770ms step_avg:153.12ms tokens_processed:20791296\n",
      "step:424/1750 train_time:64922ms step_avg:153.12ms tokens_processed:20840448\n",
      "step:425/1750 train_time:65074ms step_avg:153.12ms tokens_processed:20889600\n",
      "step:426/1750 train_time:65232ms step_avg:153.13ms tokens_processed:20938752\n",
      "step:427/1750 train_time:65382ms step_avg:153.12ms tokens_processed:20987904\n",
      "step:428/1750 train_time:65536ms step_avg:153.12ms tokens_processed:21037056\n",
      "step:429/1750 train_time:65691ms step_avg:153.13ms tokens_processed:21086208\n",
      "step:430/1750 train_time:65842ms step_avg:153.12ms tokens_processed:21135360\n",
      "step:431/1750 train_time:65996ms step_avg:153.12ms tokens_processed:21184512\n",
      "step:432/1750 train_time:66149ms step_avg:153.12ms tokens_processed:21233664\n",
      "step:433/1750 train_time:66302ms step_avg:153.12ms tokens_processed:21282816\n",
      "step:434/1750 train_time:66457ms step_avg:153.13ms tokens_processed:21331968\n",
      "step:435/1750 train_time:66610ms step_avg:153.13ms tokens_processed:21381120\n",
      "step:436/1750 train_time:66763ms step_avg:153.13ms tokens_processed:21430272\n",
      "step:437/1750 train_time:66917ms step_avg:153.13ms tokens_processed:21479424\n",
      "step:438/1750 train_time:67069ms step_avg:153.13ms tokens_processed:21528576\n",
      "step:439/1750 train_time:67221ms step_avg:153.12ms tokens_processed:21577728\n",
      "step:440/1750 train_time:67374ms step_avg:153.12ms tokens_processed:21626880\n",
      "step:441/1750 train_time:67528ms step_avg:153.12ms tokens_processed:21676032\n",
      "step:442/1750 train_time:67682ms step_avg:153.13ms tokens_processed:21725184\n",
      "step:443/1750 train_time:67836ms step_avg:153.13ms tokens_processed:21774336\n",
      "step:444/1750 train_time:67989ms step_avg:153.13ms tokens_processed:21823488\n",
      "step:445/1750 train_time:68143ms step_avg:153.13ms tokens_processed:21872640\n",
      "step:446/1750 train_time:68299ms step_avg:153.14ms tokens_processed:21921792\n",
      "step:447/1750 train_time:68449ms step_avg:153.13ms tokens_processed:21970944\n",
      "step:448/1750 train_time:68603ms step_avg:153.13ms tokens_processed:22020096\n",
      "step:449/1750 train_time:68757ms step_avg:153.13ms tokens_processed:22069248\n",
      "step:450/1750 train_time:68912ms step_avg:153.14ms tokens_processed:22118400\n",
      "step:451/1750 train_time:69064ms step_avg:153.14ms tokens_processed:22167552\n",
      "step:452/1750 train_time:69217ms step_avg:153.14ms tokens_processed:22216704\n",
      "step:453/1750 train_time:69369ms step_avg:153.13ms tokens_processed:22265856\n",
      "step:454/1750 train_time:69522ms step_avg:153.13ms tokens_processed:22315008\n",
      "step:455/1750 train_time:69675ms step_avg:153.13ms tokens_processed:22364160\n",
      "step:456/1750 train_time:69829ms step_avg:153.13ms tokens_processed:22413312\n",
      "step:457/1750 train_time:69982ms step_avg:153.13ms tokens_processed:22462464\n",
      "step:458/1750 train_time:70136ms step_avg:153.14ms tokens_processed:22511616\n",
      "step:459/1750 train_time:70289ms step_avg:153.14ms tokens_processed:22560768\n",
      "step:460/1750 train_time:70443ms step_avg:153.14ms tokens_processed:22609920\n",
      "step:461/1750 train_time:70598ms step_avg:153.14ms tokens_processed:22659072\n",
      "step:462/1750 train_time:70749ms step_avg:153.14ms tokens_processed:22708224\n",
      "step:463/1750 train_time:70903ms step_avg:153.14ms tokens_processed:22757376\n",
      "step:464/1750 train_time:71059ms step_avg:153.14ms tokens_processed:22806528\n",
      "step:465/1750 train_time:71210ms step_avg:153.14ms tokens_processed:22855680\n",
      "step:466/1750 train_time:71364ms step_avg:153.14ms tokens_processed:22904832\n",
      "step:467/1750 train_time:71517ms step_avg:153.14ms tokens_processed:22953984\n",
      "step:468/1750 train_time:71673ms step_avg:153.15ms tokens_processed:23003136\n",
      "step:469/1750 train_time:71825ms step_avg:153.15ms tokens_processed:23052288\n",
      "step:470/1750 train_time:71978ms step_avg:153.15ms tokens_processed:23101440\n",
      "step:471/1750 train_time:72132ms step_avg:153.15ms tokens_processed:23150592\n",
      "step:472/1750 train_time:72286ms step_avg:153.15ms tokens_processed:23199744\n",
      "step:473/1750 train_time:72440ms step_avg:153.15ms tokens_processed:23248896\n",
      "step:474/1750 train_time:72594ms step_avg:153.15ms tokens_processed:23298048\n",
      "step:475/1750 train_time:72747ms step_avg:153.15ms tokens_processed:23347200\n",
      "step:476/1750 train_time:72901ms step_avg:153.15ms tokens_processed:23396352\n",
      "step:477/1750 train_time:73054ms step_avg:153.15ms tokens_processed:23445504\n",
      "step:478/1750 train_time:73207ms step_avg:153.15ms tokens_processed:23494656\n",
      "step:479/1750 train_time:73363ms step_avg:153.16ms tokens_processed:23543808\n",
      "step:480/1750 train_time:73513ms step_avg:153.15ms tokens_processed:23592960\n",
      "step:481/1750 train_time:73667ms step_avg:153.15ms tokens_processed:23642112\n",
      "step:482/1750 train_time:73822ms step_avg:153.16ms tokens_processed:23691264\n",
      "step:483/1750 train_time:73974ms step_avg:153.15ms tokens_processed:23740416\n",
      "step:484/1750 train_time:74128ms step_avg:153.16ms tokens_processed:23789568\n",
      "step:485/1750 train_time:74281ms step_avg:153.16ms tokens_processed:23838720\n",
      "step:486/1750 train_time:74434ms step_avg:153.16ms tokens_processed:23887872\n",
      "step:487/1750 train_time:74587ms step_avg:153.16ms tokens_processed:23937024\n",
      "step:488/1750 train_time:74741ms step_avg:153.16ms tokens_processed:23986176\n",
      "step:489/1750 train_time:74897ms step_avg:153.16ms tokens_processed:24035328\n",
      "step:490/1750 train_time:75048ms step_avg:153.16ms tokens_processed:24084480\n",
      "step:491/1750 train_time:75202ms step_avg:153.16ms tokens_processed:24133632\n",
      "step:492/1750 train_time:75356ms step_avg:153.16ms tokens_processed:24182784\n",
      "step:493/1750 train_time:75508ms step_avg:153.16ms tokens_processed:24231936\n",
      "step:494/1750 train_time:75662ms step_avg:153.16ms tokens_processed:24281088\n",
      "step:495/1750 train_time:75816ms step_avg:153.16ms tokens_processed:24330240\n",
      "step:496/1750 train_time:75969ms step_avg:153.16ms tokens_processed:24379392\n",
      "step:497/1750 train_time:76122ms step_avg:153.16ms tokens_processed:24428544\n",
      "step:498/1750 train_time:76277ms step_avg:153.17ms tokens_processed:24477696\n",
      "step:499/1750 train_time:76429ms step_avg:153.16ms tokens_processed:24526848\n",
      "step:500/1750 train_time:76582ms step_avg:153.16ms tokens_processed:24576000\n",
      "---validation---\n",
      "step:500/1750 val_loss:4.8956 train_time:76591ms step_avg:153.18ms\n",
      "---end of validation---\n",
      "step:501/1750 train_time:76742ms step_avg:153.18ms tokens_processed:24625152\n",
      "step:502/1750 train_time:76896ms step_avg:153.18ms tokens_processed:24674304\n",
      "step:503/1750 train_time:77049ms step_avg:153.18ms tokens_processed:24723456\n",
      "step:504/1750 train_time:77202ms step_avg:153.18ms tokens_processed:24772608\n",
      "step:505/1750 train_time:77355ms step_avg:153.18ms tokens_processed:24821760\n",
      "step:506/1750 train_time:77509ms step_avg:153.18ms tokens_processed:24870912\n",
      "step:507/1750 train_time:77663ms step_avg:153.18ms tokens_processed:24920064\n",
      "step:508/1750 train_time:77817ms step_avg:153.18ms tokens_processed:24969216\n",
      "step:509/1750 train_time:77970ms step_avg:153.18ms tokens_processed:25018368\n",
      "step:510/1750 train_time:78125ms step_avg:153.19ms tokens_processed:25067520\n",
      "step:511/1750 train_time:78277ms step_avg:153.18ms tokens_processed:25116672\n",
      "step:512/1750 train_time:78431ms step_avg:153.18ms tokens_processed:25165824\n",
      "step:513/1750 train_time:78586ms step_avg:153.19ms tokens_processed:25214976\n",
      "step:514/1750 train_time:78743ms step_avg:153.20ms tokens_processed:25264128\n",
      "step:515/1750 train_time:78895ms step_avg:153.19ms tokens_processed:25313280\n",
      "step:516/1750 train_time:79049ms step_avg:153.19ms tokens_processed:25362432\n",
      "step:517/1750 train_time:79200ms step_avg:153.19ms tokens_processed:25411584\n",
      "step:518/1750 train_time:79355ms step_avg:153.19ms tokens_processed:25460736\n",
      "step:519/1750 train_time:79507ms step_avg:153.19ms tokens_processed:25509888\n",
      "step:520/1750 train_time:79664ms step_avg:153.20ms tokens_processed:25559040\n",
      "step:521/1750 train_time:79817ms step_avg:153.20ms tokens_processed:25608192\n",
      "step:522/1750 train_time:79971ms step_avg:153.20ms tokens_processed:25657344\n",
      "step:523/1750 train_time:80127ms step_avg:153.21ms tokens_processed:25706496\n",
      "step:524/1750 train_time:80277ms step_avg:153.20ms tokens_processed:25755648\n",
      "step:525/1750 train_time:80431ms step_avg:153.20ms tokens_processed:25804800\n",
      "step:526/1750 train_time:80585ms step_avg:153.20ms tokens_processed:25853952\n",
      "step:527/1750 train_time:80740ms step_avg:153.21ms tokens_processed:25903104\n",
      "step:528/1750 train_time:80892ms step_avg:153.20ms tokens_processed:25952256\n",
      "step:529/1750 train_time:81047ms step_avg:153.21ms tokens_processed:26001408\n",
      "step:530/1750 train_time:81201ms step_avg:153.21ms tokens_processed:26050560\n",
      "step:531/1750 train_time:81353ms step_avg:153.21ms tokens_processed:26099712\n",
      "step:532/1750 train_time:81508ms step_avg:153.21ms tokens_processed:26148864\n",
      "step:533/1750 train_time:81660ms step_avg:153.21ms tokens_processed:26198016\n",
      "step:534/1750 train_time:81819ms step_avg:153.22ms tokens_processed:26247168\n",
      "step:535/1750 train_time:81975ms step_avg:153.22ms tokens_processed:26296320\n",
      "step:536/1750 train_time:82127ms step_avg:153.22ms tokens_processed:26345472\n",
      "step:537/1750 train_time:82280ms step_avg:153.22ms tokens_processed:26394624\n",
      "step:538/1750 train_time:82434ms step_avg:153.22ms tokens_processed:26443776\n",
      "step:539/1750 train_time:82588ms step_avg:153.23ms tokens_processed:26492928\n",
      "step:540/1750 train_time:82742ms step_avg:153.23ms tokens_processed:26542080\n",
      "step:541/1750 train_time:82895ms step_avg:153.23ms tokens_processed:26591232\n",
      "step:542/1750 train_time:83049ms step_avg:153.23ms tokens_processed:26640384\n",
      "step:543/1750 train_time:83200ms step_avg:153.22ms tokens_processed:26689536\n",
      "step:544/1750 train_time:83355ms step_avg:153.23ms tokens_processed:26738688\n",
      "step:545/1750 train_time:83507ms step_avg:153.22ms tokens_processed:26787840\n",
      "step:546/1750 train_time:83662ms step_avg:153.23ms tokens_processed:26836992\n",
      "step:547/1750 train_time:83817ms step_avg:153.23ms tokens_processed:26886144\n",
      "step:548/1750 train_time:83969ms step_avg:153.23ms tokens_processed:26935296\n",
      "step:549/1750 train_time:84123ms step_avg:153.23ms tokens_processed:26984448\n",
      "step:550/1750 train_time:84276ms step_avg:153.23ms tokens_processed:27033600\n",
      "step:551/1750 train_time:84428ms step_avg:153.23ms tokens_processed:27082752\n",
      "step:552/1750 train_time:84582ms step_avg:153.23ms tokens_processed:27131904\n",
      "step:553/1750 train_time:84738ms step_avg:153.23ms tokens_processed:27181056\n",
      "step:554/1750 train_time:84892ms step_avg:153.23ms tokens_processed:27230208\n",
      "step:555/1750 train_time:85042ms step_avg:153.23ms tokens_processed:27279360\n",
      "step:556/1750 train_time:85195ms step_avg:153.23ms tokens_processed:27328512\n",
      "step:557/1750 train_time:85348ms step_avg:153.23ms tokens_processed:27377664\n",
      "step:558/1750 train_time:85502ms step_avg:153.23ms tokens_processed:27426816\n",
      "step:559/1750 train_time:85655ms step_avg:153.23ms tokens_processed:27475968\n",
      "step:560/1750 train_time:85810ms step_avg:153.23ms tokens_processed:27525120\n",
      "step:561/1750 train_time:85964ms step_avg:153.23ms tokens_processed:27574272\n",
      "step:562/1750 train_time:86116ms step_avg:153.23ms tokens_processed:27623424\n",
      "step:563/1750 train_time:86272ms step_avg:153.24ms tokens_processed:27672576\n",
      "step:564/1750 train_time:86424ms step_avg:153.23ms tokens_processed:27721728\n",
      "step:565/1750 train_time:86578ms step_avg:153.24ms tokens_processed:27770880\n",
      "step:566/1750 train_time:86732ms step_avg:153.24ms tokens_processed:27820032\n",
      "step:567/1750 train_time:86886ms step_avg:153.24ms tokens_processed:27869184\n",
      "step:568/1750 train_time:87042ms step_avg:153.24ms tokens_processed:27918336\n",
      "step:569/1750 train_time:87194ms step_avg:153.24ms tokens_processed:27967488\n",
      "step:570/1750 train_time:87351ms step_avg:153.25ms tokens_processed:28016640\n",
      "step:571/1750 train_time:87500ms step_avg:153.24ms tokens_processed:28065792\n",
      "step:572/1750 train_time:87654ms step_avg:153.24ms tokens_processed:28114944\n",
      "step:573/1750 train_time:87809ms step_avg:153.24ms tokens_processed:28164096\n",
      "step:574/1750 train_time:87962ms step_avg:153.24ms tokens_processed:28213248\n",
      "step:575/1750 train_time:88117ms step_avg:153.25ms tokens_processed:28262400\n",
      "step:576/1750 train_time:88269ms step_avg:153.24ms tokens_processed:28311552\n",
      "step:577/1750 train_time:88422ms step_avg:153.24ms tokens_processed:28360704\n",
      "step:578/1750 train_time:88574ms step_avg:153.24ms tokens_processed:28409856\n",
      "step:579/1750 train_time:88732ms step_avg:153.25ms tokens_processed:28459008\n",
      "step:580/1750 train_time:88882ms step_avg:153.25ms tokens_processed:28508160\n",
      "step:581/1750 train_time:89037ms step_avg:153.25ms tokens_processed:28557312\n",
      "step:582/1750 train_time:89190ms step_avg:153.25ms tokens_processed:28606464\n",
      "step:583/1750 train_time:89344ms step_avg:153.25ms tokens_processed:28655616\n",
      "step:584/1750 train_time:89498ms step_avg:153.25ms tokens_processed:28704768\n",
      "step:585/1750 train_time:89650ms step_avg:153.25ms tokens_processed:28753920\n",
      "step:586/1750 train_time:89803ms step_avg:153.25ms tokens_processed:28803072\n",
      "step:587/1750 train_time:89957ms step_avg:153.25ms tokens_processed:28852224\n",
      "step:588/1750 train_time:90110ms step_avg:153.25ms tokens_processed:28901376\n",
      "step:589/1750 train_time:90263ms step_avg:153.25ms tokens_processed:28950528\n",
      "step:590/1750 train_time:90417ms step_avg:153.25ms tokens_processed:28999680\n",
      "step:591/1750 train_time:90571ms step_avg:153.25ms tokens_processed:29048832\n",
      "step:592/1750 train_time:90727ms step_avg:153.25ms tokens_processed:29097984\n",
      "step:593/1750 train_time:90879ms step_avg:153.25ms tokens_processed:29147136\n",
      "step:594/1750 train_time:91032ms step_avg:153.25ms tokens_processed:29196288\n",
      "step:595/1750 train_time:91187ms step_avg:153.26ms tokens_processed:29245440\n",
      "step:596/1750 train_time:91341ms step_avg:153.26ms tokens_processed:29294592\n",
      "step:597/1750 train_time:91499ms step_avg:153.26ms tokens_processed:29343744\n",
      "step:598/1750 train_time:91654ms step_avg:153.27ms tokens_processed:29392896\n",
      "step:599/1750 train_time:91807ms step_avg:153.27ms tokens_processed:29442048\n",
      "step:600/1750 train_time:91962ms step_avg:153.27ms tokens_processed:29491200\n",
      "---validation---\n",
      "step:600/1750 val_loss:4.8065 train_time:91967ms step_avg:153.28ms\n",
      "---end of validation---\n",
      "step:601/1750 train_time:92119ms step_avg:153.28ms tokens_processed:29540352\n",
      "step:602/1750 train_time:92272ms step_avg:153.28ms tokens_processed:29589504\n",
      "step:603/1750 train_time:92427ms step_avg:153.28ms tokens_processed:29638656\n",
      "step:604/1750 train_time:92579ms step_avg:153.28ms tokens_processed:29687808\n",
      "step:605/1750 train_time:92731ms step_avg:153.28ms tokens_processed:29736960\n",
      "step:606/1750 train_time:92885ms step_avg:153.28ms tokens_processed:29786112\n",
      "step:607/1750 train_time:93040ms step_avg:153.28ms tokens_processed:29835264\n",
      "step:608/1750 train_time:93194ms step_avg:153.28ms tokens_processed:29884416\n",
      "step:609/1750 train_time:93347ms step_avg:153.28ms tokens_processed:29933568\n",
      "step:610/1750 train_time:93500ms step_avg:153.28ms tokens_processed:29982720\n",
      "step:611/1750 train_time:93654ms step_avg:153.28ms tokens_processed:30031872\n",
      "step:612/1750 train_time:93807ms step_avg:153.28ms tokens_processed:30081024\n",
      "step:613/1750 train_time:93961ms step_avg:153.28ms tokens_processed:30130176\n",
      "step:614/1750 train_time:94115ms step_avg:153.28ms tokens_processed:30179328\n",
      "step:615/1750 train_time:94270ms step_avg:153.28ms tokens_processed:30228480\n",
      "step:616/1750 train_time:94422ms step_avg:153.28ms tokens_processed:30277632\n",
      "step:617/1750 train_time:94576ms step_avg:153.28ms tokens_processed:30326784\n",
      "step:618/1750 train_time:94731ms step_avg:153.29ms tokens_processed:30375936\n",
      "step:619/1750 train_time:94884ms step_avg:153.29ms tokens_processed:30425088\n",
      "step:620/1750 train_time:95038ms step_avg:153.29ms tokens_processed:30474240\n",
      "step:621/1750 train_time:95195ms step_avg:153.29ms tokens_processed:30523392\n",
      "step:622/1750 train_time:95346ms step_avg:153.29ms tokens_processed:30572544\n",
      "step:623/1750 train_time:95499ms step_avg:153.29ms tokens_processed:30621696\n",
      "step:624/1750 train_time:95653ms step_avg:153.29ms tokens_processed:30670848\n",
      "step:625/1750 train_time:95806ms step_avg:153.29ms tokens_processed:30720000\n",
      "step:626/1750 train_time:95962ms step_avg:153.29ms tokens_processed:30769152\n",
      "step:627/1750 train_time:96114ms step_avg:153.29ms tokens_processed:30818304\n",
      "step:628/1750 train_time:96268ms step_avg:153.29ms tokens_processed:30867456\n",
      "step:629/1750 train_time:96422ms step_avg:153.29ms tokens_processed:30916608\n",
      "step:630/1750 train_time:96575ms step_avg:153.29ms tokens_processed:30965760\n",
      "step:631/1750 train_time:96731ms step_avg:153.30ms tokens_processed:31014912\n",
      "step:632/1750 train_time:96882ms step_avg:153.29ms tokens_processed:31064064\n",
      "step:633/1750 train_time:97036ms step_avg:153.30ms tokens_processed:31113216\n",
      "step:634/1750 train_time:97195ms step_avg:153.30ms tokens_processed:31162368\n",
      "step:635/1750 train_time:97347ms step_avg:153.30ms tokens_processed:31211520\n",
      "step:636/1750 train_time:97500ms step_avg:153.30ms tokens_processed:31260672\n",
      "step:637/1750 train_time:97653ms step_avg:153.30ms tokens_processed:31309824\n",
      "step:638/1750 train_time:97808ms step_avg:153.30ms tokens_processed:31358976\n",
      "step:639/1750 train_time:97961ms step_avg:153.30ms tokens_processed:31408128\n",
      "step:640/1750 train_time:98120ms step_avg:153.31ms tokens_processed:31457280\n",
      "step:641/1750 train_time:98273ms step_avg:153.31ms tokens_processed:31506432\n",
      "step:642/1750 train_time:98428ms step_avg:153.32ms tokens_processed:31555584\n",
      "step:643/1750 train_time:98580ms step_avg:153.31ms tokens_processed:31604736\n",
      "step:644/1750 train_time:98734ms step_avg:153.31ms tokens_processed:31653888\n",
      "step:645/1750 train_time:98890ms step_avg:153.32ms tokens_processed:31703040\n",
      "step:646/1750 train_time:99042ms step_avg:153.32ms tokens_processed:31752192\n",
      "step:647/1750 train_time:99196ms step_avg:153.32ms tokens_processed:31801344\n",
      "step:648/1750 train_time:99351ms step_avg:153.32ms tokens_processed:31850496\n",
      "step:649/1750 train_time:99502ms step_avg:153.32ms tokens_processed:31899648\n",
      "step:650/1750 train_time:99655ms step_avg:153.32ms tokens_processed:31948800\n",
      "step:651/1750 train_time:99809ms step_avg:153.32ms tokens_processed:31997952\n",
      "step:652/1750 train_time:99963ms step_avg:153.32ms tokens_processed:32047104\n",
      "step:653/1750 train_time:100116ms step_avg:153.32ms tokens_processed:32096256\n",
      "step:654/1750 train_time:100270ms step_avg:153.32ms tokens_processed:32145408\n",
      "step:655/1750 train_time:100424ms step_avg:153.32ms tokens_processed:32194560\n",
      "step:656/1750 train_time:100577ms step_avg:153.32ms tokens_processed:32243712\n",
      "step:657/1750 train_time:100732ms step_avg:153.32ms tokens_processed:32292864\n",
      "step:658/1750 train_time:100888ms step_avg:153.33ms tokens_processed:32342016\n",
      "step:659/1750 train_time:101039ms step_avg:153.32ms tokens_processed:32391168\n",
      "step:660/1750 train_time:101193ms step_avg:153.32ms tokens_processed:32440320\n",
      "step:661/1750 train_time:101345ms step_avg:153.32ms tokens_processed:32489472\n",
      "step:662/1750 train_time:101500ms step_avg:153.32ms tokens_processed:32538624\n",
      "step:663/1750 train_time:101653ms step_avg:153.32ms tokens_processed:32587776\n",
      "step:664/1750 train_time:101807ms step_avg:153.32ms tokens_processed:32636928\n",
      "step:665/1750 train_time:101961ms step_avg:153.33ms tokens_processed:32686080\n",
      "step:666/1750 train_time:102115ms step_avg:153.33ms tokens_processed:32735232\n",
      "step:667/1750 train_time:102271ms step_avg:153.33ms tokens_processed:32784384\n",
      "step:668/1750 train_time:102422ms step_avg:153.33ms tokens_processed:32833536\n",
      "step:669/1750 train_time:102575ms step_avg:153.33ms tokens_processed:32882688\n",
      "step:670/1750 train_time:102731ms step_avg:153.33ms tokens_processed:32931840\n",
      "step:671/1750 train_time:102882ms step_avg:153.33ms tokens_processed:32980992\n",
      "step:672/1750 train_time:103036ms step_avg:153.33ms tokens_processed:33030144\n",
      "step:673/1750 train_time:103191ms step_avg:153.33ms tokens_processed:33079296\n",
      "step:674/1750 train_time:103343ms step_avg:153.33ms tokens_processed:33128448\n",
      "step:675/1750 train_time:103497ms step_avg:153.33ms tokens_processed:33177600\n",
      "step:676/1750 train_time:103651ms step_avg:153.33ms tokens_processed:33226752\n",
      "step:677/1750 train_time:103804ms step_avg:153.33ms tokens_processed:33275904\n",
      "step:678/1750 train_time:103958ms step_avg:153.33ms tokens_processed:33325056\n",
      "step:679/1750 train_time:104114ms step_avg:153.33ms tokens_processed:33374208\n",
      "step:680/1750 train_time:104265ms step_avg:153.33ms tokens_processed:33423360\n",
      "step:681/1750 train_time:104422ms step_avg:153.34ms tokens_processed:33472512\n",
      "step:682/1750 train_time:104574ms step_avg:153.33ms tokens_processed:33521664\n",
      "step:683/1750 train_time:104730ms step_avg:153.34ms tokens_processed:33570816\n",
      "step:684/1750 train_time:104881ms step_avg:153.34ms tokens_processed:33619968\n",
      "step:685/1750 train_time:105036ms step_avg:153.34ms tokens_processed:33669120\n",
      "step:686/1750 train_time:105190ms step_avg:153.34ms tokens_processed:33718272\n",
      "step:687/1750 train_time:105344ms step_avg:153.34ms tokens_processed:33767424\n",
      "step:688/1750 train_time:105497ms step_avg:153.34ms tokens_processed:33816576\n",
      "step:689/1750 train_time:105652ms step_avg:153.34ms tokens_processed:33865728\n",
      "step:690/1750 train_time:105807ms step_avg:153.34ms tokens_processed:33914880\n",
      "step:691/1750 train_time:105960ms step_avg:153.34ms tokens_processed:33964032\n",
      "step:692/1750 train_time:106115ms step_avg:153.35ms tokens_processed:34013184\n",
      "step:693/1750 train_time:106266ms step_avg:153.34ms tokens_processed:34062336\n",
      "step:694/1750 train_time:106421ms step_avg:153.34ms tokens_processed:34111488\n",
      "step:695/1750 train_time:106575ms step_avg:153.35ms tokens_processed:34160640\n",
      "step:696/1750 train_time:106727ms step_avg:153.34ms tokens_processed:34209792\n",
      "step:697/1750 train_time:106881ms step_avg:153.34ms tokens_processed:34258944\n",
      "step:698/1750 train_time:107035ms step_avg:153.34ms tokens_processed:34308096\n",
      "step:699/1750 train_time:107189ms step_avg:153.35ms tokens_processed:34357248\n",
      "step:700/1750 train_time:107342ms step_avg:153.35ms tokens_processed:34406400\n",
      "---validation---\n",
      "step:700/1750 val_loss:4.7357 train_time:107350ms step_avg:153.36ms\n",
      "---end of validation---\n",
      "step:701/1750 train_time:107504ms step_avg:153.36ms tokens_processed:34455552\n",
      "step:702/1750 train_time:107653ms step_avg:153.35ms tokens_processed:34504704\n",
      "step:703/1750 train_time:107805ms step_avg:153.35ms tokens_processed:34553856\n",
      "step:704/1750 train_time:107959ms step_avg:153.35ms tokens_processed:34603008\n",
      "step:705/1750 train_time:108112ms step_avg:153.35ms tokens_processed:34652160\n",
      "step:706/1750 train_time:108268ms step_avg:153.35ms tokens_processed:34701312\n",
      "step:707/1750 train_time:108420ms step_avg:153.35ms tokens_processed:34750464\n",
      "step:708/1750 train_time:108576ms step_avg:153.36ms tokens_processed:34799616\n",
      "step:709/1750 train_time:108728ms step_avg:153.35ms tokens_processed:34848768\n",
      "step:710/1750 train_time:108881ms step_avg:153.35ms tokens_processed:34897920\n",
      "step:711/1750 train_time:109035ms step_avg:153.35ms tokens_processed:34947072\n",
      "step:712/1750 train_time:109193ms step_avg:153.36ms tokens_processed:34996224\n",
      "step:713/1750 train_time:109345ms step_avg:153.36ms tokens_processed:35045376\n",
      "step:714/1750 train_time:109498ms step_avg:153.36ms tokens_processed:35094528\n",
      "step:715/1750 train_time:109654ms step_avg:153.36ms tokens_processed:35143680\n",
      "step:716/1750 train_time:109805ms step_avg:153.36ms tokens_processed:35192832\n",
      "step:717/1750 train_time:109959ms step_avg:153.36ms tokens_processed:35241984\n",
      "step:718/1750 train_time:110115ms step_avg:153.36ms tokens_processed:35291136\n",
      "step:719/1750 train_time:110268ms step_avg:153.36ms tokens_processed:35340288\n",
      "step:720/1750 train_time:110421ms step_avg:153.36ms tokens_processed:35389440\n",
      "step:721/1750 train_time:110575ms step_avg:153.36ms tokens_processed:35438592\n",
      "step:722/1750 train_time:110728ms step_avg:153.36ms tokens_processed:35487744\n",
      "step:723/1750 train_time:110883ms step_avg:153.36ms tokens_processed:35536896\n",
      "step:724/1750 train_time:111035ms step_avg:153.36ms tokens_processed:35586048\n",
      "step:725/1750 train_time:111190ms step_avg:153.37ms tokens_processed:35635200\n",
      "step:726/1750 train_time:111345ms step_avg:153.37ms tokens_processed:35684352\n",
      "step:727/1750 train_time:111498ms step_avg:153.37ms tokens_processed:35733504\n",
      "step:728/1750 train_time:111653ms step_avg:153.37ms tokens_processed:35782656\n",
      "step:729/1750 train_time:111806ms step_avg:153.37ms tokens_processed:35831808\n",
      "step:730/1750 train_time:111959ms step_avg:153.37ms tokens_processed:35880960\n",
      "step:731/1750 train_time:112114ms step_avg:153.37ms tokens_processed:35930112\n",
      "step:732/1750 train_time:112267ms step_avg:153.37ms tokens_processed:35979264\n",
      "step:733/1750 train_time:112421ms step_avg:153.37ms tokens_processed:36028416\n",
      "step:734/1750 train_time:112574ms step_avg:153.37ms tokens_processed:36077568\n",
      "step:735/1750 train_time:112729ms step_avg:153.37ms tokens_processed:36126720\n",
      "step:736/1750 train_time:112884ms step_avg:153.37ms tokens_processed:36175872\n",
      "step:737/1750 train_time:113036ms step_avg:153.37ms tokens_processed:36225024\n",
      "step:738/1750 train_time:113191ms step_avg:153.38ms tokens_processed:36274176\n",
      "step:739/1750 train_time:113344ms step_avg:153.37ms tokens_processed:36323328\n",
      "step:740/1750 train_time:113500ms step_avg:153.38ms tokens_processed:36372480\n",
      "step:741/1750 train_time:113652ms step_avg:153.38ms tokens_processed:36421632\n",
      "step:742/1750 train_time:113805ms step_avg:153.38ms tokens_processed:36470784\n",
      "step:743/1750 train_time:113959ms step_avg:153.38ms tokens_processed:36519936\n",
      "step:744/1750 train_time:114113ms step_avg:153.38ms tokens_processed:36569088\n",
      "step:745/1750 train_time:114267ms step_avg:153.38ms tokens_processed:36618240\n",
      "step:746/1750 train_time:114420ms step_avg:153.38ms tokens_processed:36667392\n",
      "step:747/1750 train_time:114574ms step_avg:153.38ms tokens_processed:36716544\n",
      "step:748/1750 train_time:114728ms step_avg:153.38ms tokens_processed:36765696\n",
      "step:749/1750 train_time:114882ms step_avg:153.38ms tokens_processed:36814848\n",
      "step:750/1750 train_time:115037ms step_avg:153.38ms tokens_processed:36864000\n",
      "step:751/1750 train_time:115193ms step_avg:153.39ms tokens_processed:36913152\n",
      "step:752/1750 train_time:115345ms step_avg:153.38ms tokens_processed:36962304\n",
      "step:753/1750 train_time:115499ms step_avg:153.38ms tokens_processed:37011456\n",
      "step:754/1750 train_time:115653ms step_avg:153.39ms tokens_processed:37060608\n",
      "step:755/1750 train_time:115805ms step_avg:153.38ms tokens_processed:37109760\n",
      "step:756/1750 train_time:115959ms step_avg:153.39ms tokens_processed:37158912\n",
      "step:757/1750 train_time:116113ms step_avg:153.39ms tokens_processed:37208064\n",
      "step:758/1750 train_time:116268ms step_avg:153.39ms tokens_processed:37257216\n",
      "step:759/1750 train_time:116421ms step_avg:153.39ms tokens_processed:37306368\n",
      "step:760/1750 train_time:116575ms step_avg:153.39ms tokens_processed:37355520\n",
      "step:761/1750 train_time:116728ms step_avg:153.39ms tokens_processed:37404672\n",
      "step:762/1750 train_time:116881ms step_avg:153.39ms tokens_processed:37453824\n",
      "step:763/1750 train_time:117037ms step_avg:153.39ms tokens_processed:37502976\n",
      "step:764/1750 train_time:117190ms step_avg:153.39ms tokens_processed:37552128\n",
      "step:765/1750 train_time:117344ms step_avg:153.39ms tokens_processed:37601280\n",
      "step:766/1750 train_time:117497ms step_avg:153.39ms tokens_processed:37650432\n",
      "step:767/1750 train_time:117651ms step_avg:153.39ms tokens_processed:37699584\n",
      "step:768/1750 train_time:117802ms step_avg:153.39ms tokens_processed:37748736\n",
      "step:769/1750 train_time:117963ms step_avg:153.40ms tokens_processed:37797888\n",
      "step:770/1750 train_time:118124ms step_avg:153.41ms tokens_processed:37847040\n",
      "step:771/1750 train_time:118287ms step_avg:153.42ms tokens_processed:37896192\n",
      "step:772/1750 train_time:118446ms step_avg:153.43ms tokens_processed:37945344\n",
      "step:773/1750 train_time:118605ms step_avg:153.43ms tokens_processed:37994496\n",
      "step:774/1750 train_time:118765ms step_avg:153.44ms tokens_processed:38043648\n",
      "step:775/1750 train_time:118924ms step_avg:153.45ms tokens_processed:38092800\n",
      "step:776/1750 train_time:119086ms step_avg:153.46ms tokens_processed:38141952\n",
      "step:777/1750 train_time:119244ms step_avg:153.47ms tokens_processed:38191104\n",
      "step:778/1750 train_time:119405ms step_avg:153.48ms tokens_processed:38240256\n",
      "step:779/1750 train_time:119566ms step_avg:153.49ms tokens_processed:38289408\n",
      "step:780/1750 train_time:119724ms step_avg:153.49ms tokens_processed:38338560\n",
      "step:781/1750 train_time:119883ms step_avg:153.50ms tokens_processed:38387712\n",
      "step:782/1750 train_time:120044ms step_avg:153.51ms tokens_processed:38436864\n",
      "step:783/1750 train_time:120206ms step_avg:153.52ms tokens_processed:38486016\n",
      "step:784/1750 train_time:120366ms step_avg:153.53ms tokens_processed:38535168\n",
      "step:785/1750 train_time:120526ms step_avg:153.54ms tokens_processed:38584320\n",
      "step:786/1750 train_time:120685ms step_avg:153.54ms tokens_processed:38633472\n",
      "step:787/1750 train_time:120844ms step_avg:153.55ms tokens_processed:38682624\n",
      "step:788/1750 train_time:121005ms step_avg:153.56ms tokens_processed:38731776\n",
      "step:789/1750 train_time:121165ms step_avg:153.57ms tokens_processed:38780928\n",
      "step:790/1750 train_time:121325ms step_avg:153.58ms tokens_processed:38830080\n",
      "step:791/1750 train_time:121485ms step_avg:153.58ms tokens_processed:38879232\n",
      "step:792/1750 train_time:121644ms step_avg:153.59ms tokens_processed:38928384\n",
      "step:793/1750 train_time:121804ms step_avg:153.60ms tokens_processed:38977536\n",
      "step:794/1750 train_time:121966ms step_avg:153.61ms tokens_processed:39026688\n",
      "step:795/1750 train_time:122123ms step_avg:153.61ms tokens_processed:39075840\n",
      "step:796/1750 train_time:122286ms step_avg:153.63ms tokens_processed:39124992\n",
      "step:797/1750 train_time:122443ms step_avg:153.63ms tokens_processed:39174144\n",
      "step:798/1750 train_time:122603ms step_avg:153.64ms tokens_processed:39223296\n",
      "step:799/1750 train_time:122764ms step_avg:153.65ms tokens_processed:39272448\n",
      "step:800/1750 train_time:122925ms step_avg:153.66ms tokens_processed:39321600\n",
      "---validation---\n",
      "step:800/1750 val_loss:4.6069 train_time:122931ms step_avg:153.66ms\n",
      "---end of validation---\n",
      "step:801/1750 train_time:123088ms step_avg:153.67ms tokens_processed:39370752\n",
      "step:802/1750 train_time:123248ms step_avg:153.68ms tokens_processed:39419904\n",
      "step:803/1750 train_time:123409ms step_avg:153.69ms tokens_processed:39469056\n",
      "step:804/1750 train_time:123567ms step_avg:153.69ms tokens_processed:39518208\n",
      "step:805/1750 train_time:123727ms step_avg:153.70ms tokens_processed:39567360\n",
      "step:806/1750 train_time:123886ms step_avg:153.71ms tokens_processed:39616512\n",
      "step:807/1750 train_time:124049ms step_avg:153.72ms tokens_processed:39665664\n",
      "step:808/1750 train_time:124208ms step_avg:153.72ms tokens_processed:39714816\n",
      "step:809/1750 train_time:124369ms step_avg:153.73ms tokens_processed:39763968\n",
      "step:810/1750 train_time:124529ms step_avg:153.74ms tokens_processed:39813120\n",
      "step:811/1750 train_time:124688ms step_avg:153.75ms tokens_processed:39862272\n",
      "step:812/1750 train_time:124848ms step_avg:153.75ms tokens_processed:39911424\n",
      "step:813/1750 train_time:125007ms step_avg:153.76ms tokens_processed:39960576\n",
      "step:814/1750 train_time:125168ms step_avg:153.77ms tokens_processed:40009728\n",
      "step:815/1750 train_time:125331ms step_avg:153.78ms tokens_processed:40058880\n",
      "step:816/1750 train_time:125487ms step_avg:153.78ms tokens_processed:40108032\n",
      "step:817/1750 train_time:125649ms step_avg:153.79ms tokens_processed:40157184\n",
      "step:818/1750 train_time:125807ms step_avg:153.80ms tokens_processed:40206336\n",
      "step:819/1750 train_time:125968ms step_avg:153.81ms tokens_processed:40255488\n",
      "step:820/1750 train_time:126128ms step_avg:153.81ms tokens_processed:40304640\n",
      "step:821/1750 train_time:126288ms step_avg:153.82ms tokens_processed:40353792\n",
      "step:822/1750 train_time:126449ms step_avg:153.83ms tokens_processed:40402944\n",
      "step:823/1750 train_time:126608ms step_avg:153.84ms tokens_processed:40452096\n",
      "step:824/1750 train_time:126769ms step_avg:153.85ms tokens_processed:40501248\n",
      "step:825/1750 train_time:126928ms step_avg:153.85ms tokens_processed:40550400\n",
      "step:826/1750 train_time:127089ms step_avg:153.86ms tokens_processed:40599552\n",
      "step:827/1750 train_time:127248ms step_avg:153.87ms tokens_processed:40648704\n",
      "step:828/1750 train_time:127408ms step_avg:153.87ms tokens_processed:40697856\n",
      "step:829/1750 train_time:127569ms step_avg:153.88ms tokens_processed:40747008\n",
      "step:830/1750 train_time:127727ms step_avg:153.89ms tokens_processed:40796160\n",
      "step:831/1750 train_time:127887ms step_avg:153.90ms tokens_processed:40845312\n",
      "step:832/1750 train_time:128048ms step_avg:153.90ms tokens_processed:40894464\n",
      "step:833/1750 train_time:128207ms step_avg:153.91ms tokens_processed:40943616\n",
      "step:834/1750 train_time:128367ms step_avg:153.92ms tokens_processed:40992768\n",
      "step:835/1750 train_time:128528ms step_avg:153.93ms tokens_processed:41041920\n",
      "step:836/1750 train_time:128687ms step_avg:153.93ms tokens_processed:41091072\n",
      "step:837/1750 train_time:128848ms step_avg:153.94ms tokens_processed:41140224\n",
      "step:838/1750 train_time:129006ms step_avg:153.95ms tokens_processed:41189376\n",
      "step:839/1750 train_time:129167ms step_avg:153.95ms tokens_processed:41238528\n",
      "step:840/1750 train_time:129327ms step_avg:153.96ms tokens_processed:41287680\n",
      "step:841/1750 train_time:129487ms step_avg:153.97ms tokens_processed:41336832\n",
      "step:842/1750 train_time:129649ms step_avg:153.98ms tokens_processed:41385984\n",
      "step:843/1750 train_time:129808ms step_avg:153.98ms tokens_processed:41435136\n",
      "step:844/1750 train_time:129969ms step_avg:153.99ms tokens_processed:41484288\n",
      "step:845/1750 train_time:130128ms step_avg:154.00ms tokens_processed:41533440\n",
      "step:846/1750 train_time:130289ms step_avg:154.01ms tokens_processed:41582592\n",
      "step:847/1750 train_time:130448ms step_avg:154.01ms tokens_processed:41631744\n",
      "step:848/1750 train_time:130610ms step_avg:154.02ms tokens_processed:41680896\n",
      "step:849/1750 train_time:130770ms step_avg:154.03ms tokens_processed:41730048\n",
      "step:850/1750 train_time:130929ms step_avg:154.03ms tokens_processed:41779200\n",
      "step:851/1750 train_time:131090ms step_avg:154.04ms tokens_processed:41828352\n",
      "step:852/1750 train_time:131248ms step_avg:154.05ms tokens_processed:41877504\n",
      "step:853/1750 train_time:131410ms step_avg:154.06ms tokens_processed:41926656\n",
      "step:854/1750 train_time:131570ms step_avg:154.06ms tokens_processed:41975808\n",
      "step:855/1750 train_time:131730ms step_avg:154.07ms tokens_processed:42024960\n",
      "step:856/1750 train_time:131889ms step_avg:154.08ms tokens_processed:42074112\n",
      "step:857/1750 train_time:132049ms step_avg:154.08ms tokens_processed:42123264\n",
      "step:858/1750 train_time:132210ms step_avg:154.09ms tokens_processed:42172416\n",
      "step:859/1750 train_time:132370ms step_avg:154.10ms tokens_processed:42221568\n",
      "step:860/1750 train_time:132530ms step_avg:154.10ms tokens_processed:42270720\n",
      "step:861/1750 train_time:132689ms step_avg:154.11ms tokens_processed:42319872\n",
      "step:862/1750 train_time:132848ms step_avg:154.12ms tokens_processed:42369024\n",
      "step:863/1750 train_time:133008ms step_avg:154.12ms tokens_processed:42418176\n",
      "step:864/1750 train_time:133169ms step_avg:154.13ms tokens_processed:42467328\n",
      "step:865/1750 train_time:133329ms step_avg:154.14ms tokens_processed:42516480\n",
      "step:866/1750 train_time:133489ms step_avg:154.14ms tokens_processed:42565632\n",
      "step:867/1750 train_time:133651ms step_avg:154.15ms tokens_processed:42614784\n",
      "step:868/1750 train_time:133809ms step_avg:154.16ms tokens_processed:42663936\n",
      "step:869/1750 train_time:133970ms step_avg:154.17ms tokens_processed:42713088\n",
      "step:870/1750 train_time:134129ms step_avg:154.17ms tokens_processed:42762240\n",
      "step:871/1750 train_time:134290ms step_avg:154.18ms tokens_processed:42811392\n",
      "step:872/1750 train_time:134449ms step_avg:154.18ms tokens_processed:42860544\n",
      "step:873/1750 train_time:134609ms step_avg:154.19ms tokens_processed:42909696\n",
      "step:874/1750 train_time:134770ms step_avg:154.20ms tokens_processed:42958848\n",
      "step:875/1750 train_time:134931ms step_avg:154.21ms tokens_processed:43008000\n",
      "step:876/1750 train_time:135089ms step_avg:154.21ms tokens_processed:43057152\n",
      "step:877/1750 train_time:135248ms step_avg:154.22ms tokens_processed:43106304\n",
      "step:878/1750 train_time:135408ms step_avg:154.22ms tokens_processed:43155456\n",
      "step:879/1750 train_time:135568ms step_avg:154.23ms tokens_processed:43204608\n",
      "step:880/1750 train_time:135728ms step_avg:154.24ms tokens_processed:43253760\n",
      "step:881/1750 train_time:135887ms step_avg:154.24ms tokens_processed:43302912\n",
      "step:882/1750 train_time:136047ms step_avg:154.25ms tokens_processed:43352064\n",
      "step:883/1750 train_time:136208ms step_avg:154.26ms tokens_processed:43401216\n",
      "step:884/1750 train_time:136369ms step_avg:154.26ms tokens_processed:43450368\n",
      "step:885/1750 train_time:136528ms step_avg:154.27ms tokens_processed:43499520\n",
      "step:886/1750 train_time:136687ms step_avg:154.27ms tokens_processed:43548672\n",
      "step:887/1750 train_time:136848ms step_avg:154.28ms tokens_processed:43597824\n",
      "step:888/1750 train_time:137008ms step_avg:154.29ms tokens_processed:43646976\n",
      "step:889/1750 train_time:137169ms step_avg:154.30ms tokens_processed:43696128\n",
      "step:890/1750 train_time:137329ms step_avg:154.30ms tokens_processed:43745280\n",
      "step:891/1750 train_time:137488ms step_avg:154.31ms tokens_processed:43794432\n",
      "step:892/1750 train_time:137647ms step_avg:154.31ms tokens_processed:43843584\n",
      "step:893/1750 train_time:137809ms step_avg:154.32ms tokens_processed:43892736\n",
      "step:894/1750 train_time:137970ms step_avg:154.33ms tokens_processed:43941888\n",
      "step:895/1750 train_time:138129ms step_avg:154.33ms tokens_processed:43991040\n",
      "step:896/1750 train_time:138288ms step_avg:154.34ms tokens_processed:44040192\n",
      "step:897/1750 train_time:138448ms step_avg:154.35ms tokens_processed:44089344\n",
      "step:898/1750 train_time:138608ms step_avg:154.35ms tokens_processed:44138496\n",
      "step:899/1750 train_time:138774ms step_avg:154.37ms tokens_processed:44187648\n",
      "step:900/1750 train_time:138935ms step_avg:154.37ms tokens_processed:44236800\n",
      "---validation---\n",
      "step:900/1750 val_loss:4.5312 train_time:138941ms step_avg:154.38ms\n",
      "---end of validation---\n",
      "step:901/1750 train_time:139103ms step_avg:154.39ms tokens_processed:44285952\n",
      "step:902/1750 train_time:139262ms step_avg:154.39ms tokens_processed:44335104\n",
      "step:903/1750 train_time:139421ms step_avg:154.40ms tokens_processed:44384256\n",
      "step:904/1750 train_time:139581ms step_avg:154.40ms tokens_processed:44433408\n",
      "step:905/1750 train_time:139741ms step_avg:154.41ms tokens_processed:44482560\n",
      "step:906/1750 train_time:139899ms step_avg:154.41ms tokens_processed:44531712\n",
      "step:907/1750 train_time:140061ms step_avg:154.42ms tokens_processed:44580864\n",
      "step:908/1750 train_time:140221ms step_avg:154.43ms tokens_processed:44630016\n",
      "step:909/1750 train_time:140383ms step_avg:154.44ms tokens_processed:44679168\n",
      "step:910/1750 train_time:140540ms step_avg:154.44ms tokens_processed:44728320\n",
      "step:911/1750 train_time:140700ms step_avg:154.45ms tokens_processed:44777472\n",
      "step:912/1750 train_time:140862ms step_avg:154.45ms tokens_processed:44826624\n",
      "step:913/1750 train_time:141018ms step_avg:154.46ms tokens_processed:44875776\n",
      "step:914/1750 train_time:141178ms step_avg:154.46ms tokens_processed:44924928\n",
      "step:915/1750 train_time:141339ms step_avg:154.47ms tokens_processed:44974080\n",
      "step:916/1750 train_time:141498ms step_avg:154.47ms tokens_processed:45023232\n",
      "step:917/1750 train_time:141658ms step_avg:154.48ms tokens_processed:45072384\n",
      "step:918/1750 train_time:141817ms step_avg:154.48ms tokens_processed:45121536\n",
      "step:919/1750 train_time:141981ms step_avg:154.49ms tokens_processed:45170688\n",
      "step:920/1750 train_time:142138ms step_avg:154.50ms tokens_processed:45219840\n",
      "step:921/1750 train_time:142298ms step_avg:154.50ms tokens_processed:45268992\n",
      "step:922/1750 train_time:142459ms step_avg:154.51ms tokens_processed:45318144\n",
      "step:923/1750 train_time:142619ms step_avg:154.52ms tokens_processed:45367296\n",
      "step:924/1750 train_time:142780ms step_avg:154.52ms tokens_processed:45416448\n",
      "step:925/1750 train_time:142939ms step_avg:154.53ms tokens_processed:45465600\n",
      "step:926/1750 train_time:143098ms step_avg:154.53ms tokens_processed:45514752\n",
      "step:927/1750 train_time:143259ms step_avg:154.54ms tokens_processed:45563904\n",
      "step:928/1750 train_time:143417ms step_avg:154.54ms tokens_processed:45613056\n",
      "step:929/1750 train_time:143577ms step_avg:154.55ms tokens_processed:45662208\n",
      "step:930/1750 train_time:143737ms step_avg:154.56ms tokens_processed:45711360\n",
      "step:931/1750 train_time:143898ms step_avg:154.56ms tokens_processed:45760512\n",
      "step:932/1750 train_time:144058ms step_avg:154.57ms tokens_processed:45809664\n",
      "step:933/1750 train_time:144219ms step_avg:154.58ms tokens_processed:45858816\n",
      "step:934/1750 train_time:144378ms step_avg:154.58ms tokens_processed:45907968\n",
      "step:935/1750 train_time:144538ms step_avg:154.59ms tokens_processed:45957120\n",
      "step:936/1750 train_time:144700ms step_avg:154.59ms tokens_processed:46006272\n",
      "step:937/1750 train_time:144860ms step_avg:154.60ms tokens_processed:46055424\n",
      "step:938/1750 train_time:145018ms step_avg:154.60ms tokens_processed:46104576\n",
      "step:939/1750 train_time:145179ms step_avg:154.61ms tokens_processed:46153728\n",
      "step:940/1750 train_time:145338ms step_avg:154.62ms tokens_processed:46202880\n",
      "step:941/1750 train_time:145499ms step_avg:154.62ms tokens_processed:46252032\n",
      "step:942/1750 train_time:145660ms step_avg:154.63ms tokens_processed:46301184\n",
      "step:943/1750 train_time:145819ms step_avg:154.63ms tokens_processed:46350336\n",
      "step:944/1750 train_time:145978ms step_avg:154.64ms tokens_processed:46399488\n",
      "step:945/1750 train_time:146140ms step_avg:154.65ms tokens_processed:46448640\n",
      "step:946/1750 train_time:146299ms step_avg:154.65ms tokens_processed:46497792\n",
      "step:947/1750 train_time:146458ms step_avg:154.65ms tokens_processed:46546944\n",
      "step:948/1750 train_time:146618ms step_avg:154.66ms tokens_processed:46596096\n",
      "step:949/1750 train_time:146778ms step_avg:154.67ms tokens_processed:46645248\n",
      "step:950/1750 train_time:146938ms step_avg:154.67ms tokens_processed:46694400\n",
      "step:951/1750 train_time:147099ms step_avg:154.68ms tokens_processed:46743552\n",
      "step:952/1750 train_time:147258ms step_avg:154.68ms tokens_processed:46792704\n",
      "step:953/1750 train_time:147420ms step_avg:154.69ms tokens_processed:46841856\n",
      "step:954/1750 train_time:147579ms step_avg:154.69ms tokens_processed:46891008\n",
      "step:955/1750 train_time:147738ms step_avg:154.70ms tokens_processed:46940160\n",
      "step:956/1750 train_time:147898ms step_avg:154.71ms tokens_processed:46989312\n",
      "step:957/1750 train_time:148059ms step_avg:154.71ms tokens_processed:47038464\n",
      "step:958/1750 train_time:148218ms step_avg:154.72ms tokens_processed:47087616\n",
      "step:959/1750 train_time:148378ms step_avg:154.72ms tokens_processed:47136768\n",
      "step:960/1750 train_time:148541ms step_avg:154.73ms tokens_processed:47185920\n",
      "step:961/1750 train_time:148699ms step_avg:154.73ms tokens_processed:47235072\n",
      "step:962/1750 train_time:148861ms step_avg:154.74ms tokens_processed:47284224\n",
      "step:963/1750 train_time:149018ms step_avg:154.74ms tokens_processed:47333376\n",
      "step:964/1750 train_time:149180ms step_avg:154.75ms tokens_processed:47382528\n",
      "step:965/1750 train_time:149339ms step_avg:154.76ms tokens_processed:47431680\n",
      "step:966/1750 train_time:149502ms step_avg:154.76ms tokens_processed:47480832\n",
      "step:967/1750 train_time:149662ms step_avg:154.77ms tokens_processed:47529984\n",
      "step:968/1750 train_time:149822ms step_avg:154.77ms tokens_processed:47579136\n",
      "step:969/1750 train_time:149980ms step_avg:154.78ms tokens_processed:47628288\n",
      "step:970/1750 train_time:150141ms step_avg:154.78ms tokens_processed:47677440\n",
      "step:971/1750 train_time:150302ms step_avg:154.79ms tokens_processed:47726592\n",
      "step:972/1750 train_time:150461ms step_avg:154.80ms tokens_processed:47775744\n",
      "step:973/1750 train_time:150618ms step_avg:154.80ms tokens_processed:47824896\n",
      "step:974/1750 train_time:150778ms step_avg:154.80ms tokens_processed:47874048\n",
      "step:975/1750 train_time:150937ms step_avg:154.81ms tokens_processed:47923200\n",
      "step:976/1750 train_time:151097ms step_avg:154.81ms tokens_processed:47972352\n",
      "step:977/1750 train_time:151258ms step_avg:154.82ms tokens_processed:48021504\n",
      "step:978/1750 train_time:151420ms step_avg:154.83ms tokens_processed:48070656\n",
      "step:979/1750 train_time:151578ms step_avg:154.83ms tokens_processed:48119808\n",
      "step:980/1750 train_time:151739ms step_avg:154.84ms tokens_processed:48168960\n",
      "step:981/1750 train_time:151899ms step_avg:154.84ms tokens_processed:48218112\n",
      "step:982/1750 train_time:152058ms step_avg:154.85ms tokens_processed:48267264\n",
      "step:983/1750 train_time:152219ms step_avg:154.85ms tokens_processed:48316416\n",
      "step:984/1750 train_time:152379ms step_avg:154.86ms tokens_processed:48365568\n",
      "step:985/1750 train_time:152540ms step_avg:154.86ms tokens_processed:48414720\n",
      "step:986/1750 train_time:152699ms step_avg:154.87ms tokens_processed:48463872\n",
      "step:987/1750 train_time:152859ms step_avg:154.87ms tokens_processed:48513024\n",
      "step:988/1750 train_time:153019ms step_avg:154.88ms tokens_processed:48562176\n",
      "step:989/1750 train_time:153178ms step_avg:154.88ms tokens_processed:48611328\n",
      "step:990/1750 train_time:153339ms step_avg:154.89ms tokens_processed:48660480\n",
      "step:991/1750 train_time:153501ms step_avg:154.89ms tokens_processed:48709632\n",
      "step:992/1750 train_time:153660ms step_avg:154.90ms tokens_processed:48758784\n",
      "step:993/1750 train_time:153818ms step_avg:154.90ms tokens_processed:48807936\n",
      "step:994/1750 train_time:153977ms step_avg:154.91ms tokens_processed:48857088\n",
      "step:995/1750 train_time:154137ms step_avg:154.91ms tokens_processed:48906240\n",
      "step:996/1750 train_time:154297ms step_avg:154.92ms tokens_processed:48955392\n",
      "step:997/1750 train_time:154459ms step_avg:154.92ms tokens_processed:49004544\n",
      "step:998/1750 train_time:154620ms step_avg:154.93ms tokens_processed:49053696\n",
      "step:999/1750 train_time:154780ms step_avg:154.93ms tokens_processed:49102848\n",
      "step:1000/1750 train_time:154939ms step_avg:154.94ms tokens_processed:49152000\n",
      "---validation---\n",
      "step:1000/1750 val_loss:4.4736 train_time:154945ms step_avg:154.95ms\n",
      "---end of validation---\n",
      "step:1001/1750 train_time:155103ms step_avg:154.95ms tokens_processed:49201152\n",
      "step:1002/1750 train_time:155263ms step_avg:154.95ms tokens_processed:49250304\n",
      "step:1003/1750 train_time:155424ms step_avg:154.96ms tokens_processed:49299456\n",
      "step:1004/1750 train_time:155585ms step_avg:154.97ms tokens_processed:49348608\n",
      "step:1005/1750 train_time:155743ms step_avg:154.97ms tokens_processed:49397760\n",
      "step:1006/1750 train_time:155901ms step_avg:154.97ms tokens_processed:49446912\n",
      "step:1007/1750 train_time:156063ms step_avg:154.98ms tokens_processed:49496064\n",
      "step:1008/1750 train_time:156221ms step_avg:154.98ms tokens_processed:49545216\n",
      "step:1009/1750 train_time:156383ms step_avg:154.99ms tokens_processed:49594368\n",
      "step:1010/1750 train_time:156541ms step_avg:154.99ms tokens_processed:49643520\n",
      "step:1011/1750 train_time:156701ms step_avg:155.00ms tokens_processed:49692672\n",
      "step:1012/1750 train_time:156860ms step_avg:155.00ms tokens_processed:49741824\n",
      "step:1013/1750 train_time:157023ms step_avg:155.01ms tokens_processed:49790976\n",
      "step:1014/1750 train_time:157181ms step_avg:155.01ms tokens_processed:49840128\n",
      "step:1015/1750 train_time:157342ms step_avg:155.02ms tokens_processed:49889280\n",
      "step:1016/1750 train_time:157505ms step_avg:155.02ms tokens_processed:49938432\n",
      "step:1017/1750 train_time:157662ms step_avg:155.03ms tokens_processed:49987584\n",
      "step:1018/1750 train_time:157823ms step_avg:155.03ms tokens_processed:50036736\n",
      "step:1019/1750 train_time:157981ms step_avg:155.04ms tokens_processed:50085888\n",
      "step:1020/1750 train_time:158142ms step_avg:155.04ms tokens_processed:50135040\n",
      "step:1021/1750 train_time:158302ms step_avg:155.05ms tokens_processed:50184192\n",
      "step:1022/1750 train_time:158463ms step_avg:155.05ms tokens_processed:50233344\n",
      "step:1023/1750 train_time:158621ms step_avg:155.05ms tokens_processed:50282496\n",
      "step:1024/1750 train_time:158781ms step_avg:155.06ms tokens_processed:50331648\n",
      "step:1025/1750 train_time:158943ms step_avg:155.07ms tokens_processed:50380800\n",
      "step:1026/1750 train_time:159102ms step_avg:155.07ms tokens_processed:50429952\n",
      "step:1027/1750 train_time:159261ms step_avg:155.07ms tokens_processed:50479104\n",
      "step:1028/1750 train_time:159422ms step_avg:155.08ms tokens_processed:50528256\n",
      "step:1029/1750 train_time:159584ms step_avg:155.09ms tokens_processed:50577408\n",
      "step:1030/1750 train_time:159742ms step_avg:155.09ms tokens_processed:50626560\n",
      "step:1031/1750 train_time:159903ms step_avg:155.10ms tokens_processed:50675712\n",
      "step:1032/1750 train_time:160061ms step_avg:155.10ms tokens_processed:50724864\n",
      "step:1033/1750 train_time:160221ms step_avg:155.10ms tokens_processed:50774016\n",
      "step:1034/1750 train_time:160381ms step_avg:155.11ms tokens_processed:50823168\n",
      "step:1035/1750 train_time:160542ms step_avg:155.11ms tokens_processed:50872320\n",
      "step:1036/1750 train_time:160701ms step_avg:155.12ms tokens_processed:50921472\n",
      "step:1037/1750 train_time:160861ms step_avg:155.12ms tokens_processed:50970624\n",
      "step:1038/1750 train_time:161022ms step_avg:155.13ms tokens_processed:51019776\n",
      "step:1039/1750 train_time:161183ms step_avg:155.13ms tokens_processed:51068928\n",
      "step:1040/1750 train_time:161342ms step_avg:155.14ms tokens_processed:51118080\n",
      "step:1041/1750 train_time:161501ms step_avg:155.14ms tokens_processed:51167232\n",
      "step:1042/1750 train_time:161662ms step_avg:155.15ms tokens_processed:51216384\n",
      "step:1043/1750 train_time:161821ms step_avg:155.15ms tokens_processed:51265536\n",
      "step:1044/1750 train_time:161982ms step_avg:155.15ms tokens_processed:51314688\n",
      "step:1045/1750 train_time:162142ms step_avg:155.16ms tokens_processed:51363840\n",
      "step:1046/1750 train_time:162301ms step_avg:155.16ms tokens_processed:51412992\n",
      "step:1047/1750 train_time:162461ms step_avg:155.17ms tokens_processed:51462144\n",
      "step:1048/1750 train_time:162621ms step_avg:155.17ms tokens_processed:51511296\n",
      "step:1049/1750 train_time:162781ms step_avg:155.18ms tokens_processed:51560448\n",
      "step:1050/1750 train_time:162942ms step_avg:155.18ms tokens_processed:51609600\n",
      "step:1051/1750 train_time:163101ms step_avg:155.19ms tokens_processed:51658752\n",
      "step:1052/1750 train_time:163263ms step_avg:155.19ms tokens_processed:51707904\n",
      "step:1053/1750 train_time:163421ms step_avg:155.20ms tokens_processed:51757056\n",
      "step:1054/1750 train_time:163582ms step_avg:155.20ms tokens_processed:51806208\n",
      "step:1055/1750 train_time:163745ms step_avg:155.21ms tokens_processed:51855360\n",
      "step:1056/1750 train_time:163902ms step_avg:155.21ms tokens_processed:51904512\n",
      "step:1057/1750 train_time:164062ms step_avg:155.21ms tokens_processed:51953664\n",
      "step:1058/1750 train_time:164222ms step_avg:155.22ms tokens_processed:52002816\n",
      "step:1059/1750 train_time:164382ms step_avg:155.22ms tokens_processed:52051968\n",
      "step:1060/1750 train_time:164549ms step_avg:155.23ms tokens_processed:52101120\n",
      "step:1061/1750 train_time:164707ms step_avg:155.24ms tokens_processed:52150272\n",
      "step:1062/1750 train_time:164867ms step_avg:155.24ms tokens_processed:52199424\n",
      "step:1063/1750 train_time:165025ms step_avg:155.24ms tokens_processed:52248576\n",
      "step:1064/1750 train_time:165188ms step_avg:155.25ms tokens_processed:52297728\n",
      "step:1065/1750 train_time:165349ms step_avg:155.26ms tokens_processed:52346880\n",
      "step:1066/1750 train_time:165509ms step_avg:155.26ms tokens_processed:52396032\n",
      "step:1067/1750 train_time:165670ms step_avg:155.27ms tokens_processed:52445184\n",
      "step:1068/1750 train_time:165828ms step_avg:155.27ms tokens_processed:52494336\n",
      "step:1069/1750 train_time:165988ms step_avg:155.27ms tokens_processed:52543488\n",
      "step:1070/1750 train_time:166149ms step_avg:155.28ms tokens_processed:52592640\n",
      "step:1071/1750 train_time:166306ms step_avg:155.28ms tokens_processed:52641792\n",
      "step:1072/1750 train_time:166469ms step_avg:155.29ms tokens_processed:52690944\n",
      "step:1073/1750 train_time:166626ms step_avg:155.29ms tokens_processed:52740096\n",
      "step:1074/1750 train_time:166785ms step_avg:155.29ms tokens_processed:52789248\n",
      "step:1075/1750 train_time:166946ms step_avg:155.30ms tokens_processed:52838400\n",
      "step:1076/1750 train_time:167103ms step_avg:155.30ms tokens_processed:52887552\n",
      "step:1077/1750 train_time:167263ms step_avg:155.30ms tokens_processed:52936704\n",
      "step:1078/1750 train_time:167422ms step_avg:155.31ms tokens_processed:52985856\n",
      "step:1079/1750 train_time:167582ms step_avg:155.31ms tokens_processed:53035008\n",
      "step:1080/1750 train_time:167742ms step_avg:155.32ms tokens_processed:53084160\n",
      "step:1081/1750 train_time:167904ms step_avg:155.32ms tokens_processed:53133312\n",
      "step:1082/1750 train_time:168063ms step_avg:155.33ms tokens_processed:53182464\n",
      "step:1083/1750 train_time:168222ms step_avg:155.33ms tokens_processed:53231616\n",
      "step:1084/1750 train_time:168382ms step_avg:155.33ms tokens_processed:53280768\n",
      "step:1085/1750 train_time:168541ms step_avg:155.34ms tokens_processed:53329920\n",
      "step:1086/1750 train_time:168704ms step_avg:155.34ms tokens_processed:53379072\n",
      "step:1087/1750 train_time:168862ms step_avg:155.35ms tokens_processed:53428224\n",
      "step:1088/1750 train_time:169022ms step_avg:155.35ms tokens_processed:53477376\n",
      "step:1089/1750 train_time:169184ms step_avg:155.36ms tokens_processed:53526528\n",
      "step:1090/1750 train_time:169342ms step_avg:155.36ms tokens_processed:53575680\n",
      "step:1091/1750 train_time:169503ms step_avg:155.36ms tokens_processed:53624832\n",
      "step:1092/1750 train_time:169661ms step_avg:155.37ms tokens_processed:53673984\n",
      "step:1093/1750 train_time:169821ms step_avg:155.37ms tokens_processed:53723136\n",
      "step:1094/1750 train_time:169981ms step_avg:155.38ms tokens_processed:53772288\n",
      "step:1095/1750 train_time:170141ms step_avg:155.38ms tokens_processed:53821440\n",
      "step:1096/1750 train_time:170301ms step_avg:155.38ms tokens_processed:53870592\n",
      "step:1097/1750 train_time:170461ms step_avg:155.39ms tokens_processed:53919744\n",
      "step:1098/1750 train_time:170621ms step_avg:155.39ms tokens_processed:53968896\n",
      "step:1099/1750 train_time:170781ms step_avg:155.40ms tokens_processed:54018048\n",
      "step:1100/1750 train_time:170941ms step_avg:155.40ms tokens_processed:54067200\n",
      "---validation---\n",
      "step:1100/1750 val_loss:4.4032 train_time:170948ms step_avg:155.41ms\n",
      "---end of validation---\n",
      "step:1101/1750 train_time:171109ms step_avg:155.41ms tokens_processed:54116352\n",
      "step:1102/1750 train_time:171266ms step_avg:155.41ms tokens_processed:54165504\n",
      "step:1103/1750 train_time:171426ms step_avg:155.42ms tokens_processed:54214656\n",
      "step:1104/1750 train_time:171586ms step_avg:155.42ms tokens_processed:54263808\n",
      "step:1105/1750 train_time:171745ms step_avg:155.43ms tokens_processed:54312960\n",
      "step:1106/1750 train_time:171905ms step_avg:155.43ms tokens_processed:54362112\n",
      "step:1107/1750 train_time:172065ms step_avg:155.43ms tokens_processed:54411264\n",
      "step:1108/1750 train_time:172228ms step_avg:155.44ms tokens_processed:54460416\n",
      "step:1109/1750 train_time:172384ms step_avg:155.44ms tokens_processed:54509568\n",
      "step:1110/1750 train_time:172545ms step_avg:155.45ms tokens_processed:54558720\n",
      "step:1111/1750 train_time:172704ms step_avg:155.45ms tokens_processed:54607872\n",
      "step:1112/1750 train_time:172863ms step_avg:155.45ms tokens_processed:54657024\n",
      "step:1113/1750 train_time:173025ms step_avg:155.46ms tokens_processed:54706176\n",
      "step:1114/1750 train_time:173183ms step_avg:155.46ms tokens_processed:54755328\n",
      "step:1115/1750 train_time:173343ms step_avg:155.46ms tokens_processed:54804480\n",
      "step:1116/1750 train_time:173504ms step_avg:155.47ms tokens_processed:54853632\n",
      "step:1117/1750 train_time:173665ms step_avg:155.47ms tokens_processed:54902784\n",
      "step:1118/1750 train_time:173824ms step_avg:155.48ms tokens_processed:54951936\n",
      "step:1119/1750 train_time:173986ms step_avg:155.48ms tokens_processed:55001088\n",
      "step:1120/1750 train_time:174145ms step_avg:155.49ms tokens_processed:55050240\n",
      "step:1121/1750 train_time:174304ms step_avg:155.49ms tokens_processed:55099392\n",
      "step:1122/1750 train_time:174464ms step_avg:155.49ms tokens_processed:55148544\n",
      "step:1123/1750 train_time:174625ms step_avg:155.50ms tokens_processed:55197696\n",
      "step:1124/1750 train_time:174785ms step_avg:155.50ms tokens_processed:55246848\n",
      "step:1125/1750 train_time:174945ms step_avg:155.51ms tokens_processed:55296000\n",
      "step:1126/1750 train_time:175105ms step_avg:155.51ms tokens_processed:55345152\n",
      "step:1127/1750 train_time:175265ms step_avg:155.51ms tokens_processed:55394304\n",
      "step:1128/1750 train_time:175423ms step_avg:155.52ms tokens_processed:55443456\n",
      "step:1129/1750 train_time:175585ms step_avg:155.52ms tokens_processed:55492608\n",
      "step:1130/1750 train_time:175745ms step_avg:155.53ms tokens_processed:55541760\n",
      "step:1131/1750 train_time:175904ms step_avg:155.53ms tokens_processed:55590912\n",
      "step:1132/1750 train_time:176065ms step_avg:155.53ms tokens_processed:55640064\n",
      "step:1133/1750 train_time:176229ms step_avg:155.54ms tokens_processed:55689216\n",
      "step:1134/1750 train_time:176389ms step_avg:155.55ms tokens_processed:55738368\n",
      "step:1135/1750 train_time:176552ms step_avg:155.55ms tokens_processed:55787520\n",
      "step:1136/1750 train_time:176708ms step_avg:155.55ms tokens_processed:55836672\n",
      "step:1137/1750 train_time:176868ms step_avg:155.56ms tokens_processed:55885824\n",
      "step:1138/1750 train_time:177028ms step_avg:155.56ms tokens_processed:55934976\n",
      "step:1139/1750 train_time:177189ms step_avg:155.57ms tokens_processed:55984128\n",
      "step:1140/1750 train_time:177348ms step_avg:155.57ms tokens_processed:56033280\n",
      "step:1141/1750 train_time:177508ms step_avg:155.57ms tokens_processed:56082432\n",
      "step:1142/1750 train_time:177669ms step_avg:155.58ms tokens_processed:56131584\n",
      "step:1143/1750 train_time:177826ms step_avg:155.58ms tokens_processed:56180736\n",
      "step:1144/1750 train_time:177987ms step_avg:155.58ms tokens_processed:56229888\n",
      "step:1145/1750 train_time:178145ms step_avg:155.59ms tokens_processed:56279040\n",
      "step:1146/1750 train_time:178306ms step_avg:155.59ms tokens_processed:56328192\n",
      "step:1147/1750 train_time:178467ms step_avg:155.59ms tokens_processed:56377344\n",
      "step:1148/1750 train_time:178626ms step_avg:155.60ms tokens_processed:56426496\n",
      "step:1149/1750 train_time:178785ms step_avg:155.60ms tokens_processed:56475648\n",
      "step:1150/1750 train_time:178945ms step_avg:155.60ms tokens_processed:56524800\n",
      "step:1151/1750 train_time:179105ms step_avg:155.61ms tokens_processed:56573952\n",
      "step:1152/1750 train_time:179267ms step_avg:155.61ms tokens_processed:56623104\n",
      "step:1153/1750 train_time:179424ms step_avg:155.62ms tokens_processed:56672256\n",
      "step:1154/1750 train_time:179586ms step_avg:155.62ms tokens_processed:56721408\n",
      "step:1155/1750 train_time:179745ms step_avg:155.62ms tokens_processed:56770560\n",
      "step:1156/1750 train_time:179905ms step_avg:155.63ms tokens_processed:56819712\n",
      "step:1157/1750 train_time:180064ms step_avg:155.63ms tokens_processed:56868864\n",
      "step:1158/1750 train_time:180227ms step_avg:155.64ms tokens_processed:56918016\n",
      "step:1159/1750 train_time:180389ms step_avg:155.64ms tokens_processed:56967168\n",
      "step:1160/1750 train_time:180548ms step_avg:155.64ms tokens_processed:57016320\n",
      "step:1161/1750 train_time:180708ms step_avg:155.65ms tokens_processed:57065472\n",
      "step:1162/1750 train_time:180868ms step_avg:155.65ms tokens_processed:57114624\n",
      "step:1163/1750 train_time:181026ms step_avg:155.65ms tokens_processed:57163776\n",
      "step:1164/1750 train_time:181187ms step_avg:155.66ms tokens_processed:57212928\n",
      "step:1165/1750 train_time:181345ms step_avg:155.66ms tokens_processed:57262080\n",
      "step:1166/1750 train_time:181505ms step_avg:155.66ms tokens_processed:57311232\n",
      "step:1167/1750 train_time:181666ms step_avg:155.67ms tokens_processed:57360384\n",
      "step:1168/1750 train_time:181825ms step_avg:155.67ms tokens_processed:57409536\n",
      "step:1169/1750 train_time:181985ms step_avg:155.68ms tokens_processed:57458688\n",
      "step:1170/1750 train_time:182144ms step_avg:155.68ms tokens_processed:57507840\n",
      "step:1171/1750 train_time:182307ms step_avg:155.68ms tokens_processed:57556992\n",
      "step:1172/1750 train_time:182465ms step_avg:155.69ms tokens_processed:57606144\n",
      "step:1173/1750 train_time:182625ms step_avg:155.69ms tokens_processed:57655296\n",
      "step:1174/1750 train_time:182784ms step_avg:155.69ms tokens_processed:57704448\n",
      "step:1175/1750 train_time:182943ms step_avg:155.70ms tokens_processed:57753600\n",
      "step:1176/1750 train_time:183105ms step_avg:155.70ms tokens_processed:57802752\n",
      "step:1177/1750 train_time:183263ms step_avg:155.70ms tokens_processed:57851904\n",
      "step:1178/1750 train_time:183423ms step_avg:155.71ms tokens_processed:57901056\n",
      "step:1179/1750 train_time:183583ms step_avg:155.71ms tokens_processed:57950208\n",
      "step:1180/1750 train_time:183742ms step_avg:155.71ms tokens_processed:57999360\n",
      "step:1181/1750 train_time:183902ms step_avg:155.72ms tokens_processed:58048512\n",
      "step:1182/1750 train_time:184062ms step_avg:155.72ms tokens_processed:58097664\n",
      "step:1183/1750 train_time:184222ms step_avg:155.72ms tokens_processed:58146816\n",
      "step:1184/1750 train_time:184381ms step_avg:155.73ms tokens_processed:58195968\n",
      "step:1185/1750 train_time:184542ms step_avg:155.73ms tokens_processed:58245120\n",
      "step:1186/1750 train_time:184702ms step_avg:155.73ms tokens_processed:58294272\n",
      "step:1187/1750 train_time:184862ms step_avg:155.74ms tokens_processed:58343424\n",
      "step:1188/1750 train_time:185025ms step_avg:155.74ms tokens_processed:58392576\n",
      "step:1189/1750 train_time:185184ms step_avg:155.75ms tokens_processed:58441728\n",
      "step:1190/1750 train_time:185344ms step_avg:155.75ms tokens_processed:58490880\n",
      "step:1191/1750 train_time:185504ms step_avg:155.75ms tokens_processed:58540032\n",
      "step:1192/1750 train_time:185664ms step_avg:155.76ms tokens_processed:58589184\n",
      "step:1193/1750 train_time:185825ms step_avg:155.76ms tokens_processed:58638336\n",
      "step:1194/1750 train_time:185982ms step_avg:155.76ms tokens_processed:58687488\n",
      "step:1195/1750 train_time:186142ms step_avg:155.77ms tokens_processed:58736640\n",
      "step:1196/1750 train_time:186304ms step_avg:155.77ms tokens_processed:58785792\n",
      "step:1197/1750 train_time:186464ms step_avg:155.78ms tokens_processed:58834944\n",
      "step:1198/1750 train_time:186624ms step_avg:155.78ms tokens_processed:58884096\n",
      "step:1199/1750 train_time:186784ms step_avg:155.78ms tokens_processed:58933248\n",
      "step:1200/1750 train_time:186943ms step_avg:155.79ms tokens_processed:58982400\n",
      "---validation---\n"
     ]
    }
   ],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    params = scheduler(step)\n",
    "    \n",
    "    n_windows_long = params['dense']\n",
    "    n_windows_short = params['sparse']\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        \n",
    "        long_bm = get_blockmask(val_seq_len, n_windows_long)\n",
    "        short_bm = get_blockmask(val_seq_len, n_windows_short)\n",
    "\n",
    "        assert args.val_tokens % (val_batch_size * val_seq_len) == 0\n",
    "        val_steps = args.val_tokens // (val_batch_size * val_seq_len)\n",
    "\n",
    "        val_loader = EOSBatchFinder(args.val_files, val_seq_len, val_batch_size, align_to_bos=False).generator()\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, long_bm, short_bm)\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader_gen)\n",
    "    if step == 0: print(\"First inputs retrieved\", inputs.shape)\n",
    "    tokens_processed += inputs.numel() * world_size\n",
    "\n",
    "    long_bm = get_blockmask(train_loader.seq_len, n_windows_long)\n",
    "    short_bm = get_blockmask(train_loader.seq_len, n_windows_short)\n",
    "\n",
    "    model(inputs, targets, long_bm, short_bm).backward()\n",
    "\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{tokens_processed}\",\n",
    "        console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0f86-3515-4b2c-b6e7-ab96b5f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bc370-db63-4df0-b201-2354b66621fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a316db-a79c-4b98-ad06-5aa657360708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
