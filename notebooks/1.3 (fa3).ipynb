{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b897866-7135-4029-afd5-1beea13cfb24",
   "metadata": {},
   "source": [
    "Do something like\n",
    "\n",
    "```\n",
    "pip install -U pip wheel setuptools ninja numpy packaging\n",
    "pip install --pre \"torch==2.9.0.dev20250713+cu126\" --index-url https://download.pytorch.org/whl/nightly/cu126\n",
    "\n",
    "\n",
    "git clone https://github.com/guilhermeleobas/flash-attention.git\n",
    "cd flash-attention/hopper\n",
    "git switch guilhermeleobas/fa3-compile\n",
    "\n",
    "\n",
    "export MAX_JOBS=24\n",
    "export FLASH_ATTENTION_FORCE_BUILD=TRUE        # skip prebuilt wheel fetch\n",
    "export FLASH_ATTENTION_DISABLE_SM80=TRUE       # Hopper-only\n",
    "export FLASH_ATTENTION_DISABLE_FP16=TRUE       # leave BF16, FP8\n",
    "export FLASH_ATTENTION_DISABLE_HDIM64=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM96=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM192=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM256=TRUE\n",
    "\n",
    "python setup.py bdist_wheel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945e553f-dbff-48ba-8561-74f51bd11572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /workspace/flash-attention/hopper/dist/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (2.9.0.dev20250713+cu126)\n",
      "Collecting einops (from flash-attn-3==3.0.0b1)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (25.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (1.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.9 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.3.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.4.0+gitae848267 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.4.0+gitae848267)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.4.0+gitae848267->torch->flash-attn-3==3.0.0b1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->flash-attn-3==3.0.0b1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn-3==3.0.0b1) (2.1.5)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn-3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn-3]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-3-3.0.0b1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /workspace/flash-attention/hopper/dist/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f894d5-2b23-46f6-8622-90ed61ff381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2025.8.3)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, hf-xet, huggingface-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [huggingface-hub] [huggingface-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.8 huggingface-hub-0.34.4 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f8d4b3-1323-4760-bd21-c4272c2fee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.9.0.dev20250713+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, pytorch-triton, sympy, typing-extensions\n",
      "Required-by: flash_attn_3, torchaudio, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ed5d43-2a1e-4cd9-811c-03137f276fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: flash_attn_3\n",
      "Version: 3.0.0b1\n",
      "Summary: FlashAttention-3\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: einops, ninja, packaging, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show flash_attn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2598ca10-f9da-4891-ae94-31c298b99fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fineweb_val_000000.bin: 100%|█████████████████| 200M/200M [00:00<00:00, 465MB/s]\n",
      "fineweb_train_000001.bin: 100%|███████████████| 200M/200M [00:00<00:00, 337MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../data/cached_fineweb10B.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ebe6a-7562-4d31-9c14-ac8b44269678",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62975aa6-f3b7-489a-9131-ea69089074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f91470-f6e0-431d-a490-23c80f3c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a879e98-a216-4626-a4ea-8db22d99df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432bad-bbd3-4853-9a8d-2b11d8feda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969746f-cc86-4121-a01e-19868c2a6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c63379b-7f98-4b29-aa53-31af47bc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ccb5-4d25-4918-aeb5-1b5a1f44a0fe",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1dd716-9d1b-4086-ae9c-3cf032ec04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile ## ns\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f0f206-d925-4920-8f53-681265a44628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # Efficient systems-wise implementation of step developed by @YouJiacheng,\n",
    "        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,\n",
    "        # @ryanyang0, and @vagrawal.\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                if base_i + rank < len(params):\n",
    "                    grad = params[base_i + rank].grad\n",
    "                # This gives strange dynamo warnings\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * world_size\n",
    "            momentum = group[\"momentum\"]\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                if base_i + rank < len(params):\n",
    "                    p = params[base_i + rank]\n",
    "                    grad = p.grad\n",
    "                    eff_lr = group[\"lr\"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, \"lr_mul\", 1.0)\n",
    "                    eff_weight_decay = group[\"lr\"] * group[\"weight_decay\"] * getattr(p, \"wd_mul\", 1.0)\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
    "                    momentum_buffer = state[\"momentum_buffer\"]\n",
    "                    p.mul_(1 - eff_weight_decay)\n",
    "                    momentum_buffer.lerp_(grad, 1 - momentum)\n",
    "                    grad = grad.lerp_(momentum_buffer, momentum)\n",
    "                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)\n",
    "                    p.add_(other=v, alpha=-eff_lr)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbc0f81-97ec-4ed7-99df-78f34bada062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "        # DistributedAdam implementation by @vagrawal\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78ffb1-753b-4a6f-a57f-81781873c084",
   "metadata": {},
   "source": [
    "## Custom Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e2360d-f65a-451b-89c9-9020d252e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x.new_tensor(x_s, dtype=torch.float32),\n",
    "            scale_b=x.new_tensor(w_s, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)\n",
    "        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)\n",
    "        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)\n",
    "        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=grad_inv_s,\n",
    "            scale_b=w_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=x_inv_s,\n",
    "            scale_b=grad_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "        return grad_x, grad_w\n",
    "\n",
    "    return impl(g, x_f8, w_f8)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)\n",
    "\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    return grad_x, grad_w, None, None, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c129e66-be79-4122-857d-fe205176d0c6",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ec017e-065d-4f43-b092-9163c3aaf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43c37-4d2a-4fd6-bd58-63fcd1b5a54a",
   "metadata": {},
   "source": [
    "### CastedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4934dccb-7f32-442d-a7b8-4cc8d666d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastedLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.use_fp8 = use_fp8\n",
    "        self.x_s = x_s\n",
    "        self.w_s = w_s\n",
    "        self.grad_s = grad_s\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.use_fp8 and self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65922453-7b13-425f-a888-899f53d243ea",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c79cc0a-7a69-4d76-88a0-906ee65da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim // 4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim // 4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a536ae-14b2-4b5c-b0ee-f2ca0ad7045a",
   "metadata": {},
   "source": [
    "### Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2d179f-d588-4731-afc3-242e31ae487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn_interface\n",
    "from flash_attn_interface import flash_attn_qkvpacked_func, flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a21056-70ee-4b77-a2b8-8c33f427bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        self.attn_scale = 0.12\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, bm_size: int):\n",
    "        B, T = x.shape[:2]   # batch, seqlen\n",
    "        H, D = self.num_heads, self.head_dim\n",
    "\n",
    "        # (3, H*D, dim) -> (3*H*D, dim) for F.linear, then (B, T, 3, H, D)\n",
    "        qkv = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3, H, D)\n",
    "        q, k, v = qkv.unbind(dim=2)  # each (B, T, H, D)\n",
    "\n",
    "        # --- NO IN-PLACE WRITES ON VIEWS ---\n",
    "        q = self.rotary(norm(q))\n",
    "        k = self.rotary(norm(k))\n",
    "\n",
    "        # Mix ve into v without in-place ops (still in bf16/fp16 here)\n",
    "        if ve is not None:\n",
    "            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)\n",
    "        else:\n",
    "            v = lambdas[0] * v\n",
    "\n",
    "        # ---- FP8 (Hopper) with per-head amax scaling ----\n",
    "        # scales shape: [B, 1, H, 1] (broadcasted over S and D)\n",
    "        # dequant equation inside FA3: q_fp8 * q_descale, etc.\n",
    "        def quantize_fp8_per_head(t: Tensor, eps: float = 1e-6, fp8_max: float = 448.0):\n",
    "            # t: [B, T, H, D] (bf16/fp16)\n",
    "            amax = t.abs().amax(dim=(1, 3), keepdim=True).clamp_min(eps)  # [B,1,H,1]\n",
    "            scale = (amax / fp8_max).to(t.dtype)                           # dequant scale\n",
    "            t_fp8 = (t / scale).to(torch.float8_e4m3fn)\n",
    "            return t_fp8.contiguous(), scale.contiguous()\n",
    "\n",
    "        q_fp8, q_descale = quantize_fp8_per_head(q)\n",
    "        k_fp8, k_descale = quantize_fp8_per_head(k)\n",
    "        v_fp8, v_descale = quantize_fp8_per_head(v)\n",
    "\n",
    "        y = flash_attn_func(\n",
    "            q_fp8, k_fp8, v_fp8,\n",
    "            softmax_scale=self.attn_scale,     # or None for default 1/sqrt(D)\n",
    "            causal=True,\n",
    "            window_size=(bm_size, 0),\n",
    "            deterministic=False,\n",
    "            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n",
    "        )  # (B, T, H, D), dtype will be bf16/fp16 compute inside the kernel\n",
    "\n",
    "        # Project back\n",
    "        y = y.to(x.dtype).reshape(B, T, H * D)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "        \n",
    "    # def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, bm_size: int):\n",
    "    #     B, T = x.shape[:2]  # batch, seqlen\n",
    "    #     H, D = self.num_heads, self.head_dim\n",
    "    \n",
    "    #     # (3, H*D, dim) -> (3*H*D, dim) for F.linear, then (B, T, 3, H, D)\n",
    "    #     qkv = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)) \\\n",
    "    #             .view(B, T, 3, H, D)\n",
    "    \n",
    "    #     # Get q, k, v as separate tensors: each (B, T, H, D)\n",
    "    #     # (Using unbind avoids the extra squeeze step; chunk(3, dim=2) + squeeze(2) is also fine.)\n",
    "    #     q, k, v = qkv.unbind(dim=2)\n",
    "    \n",
    "    #     # --- NO IN-PLACE WRITES ON VIEWS ---\n",
    "    #     # Apply norm + rotary (produce new tensors instead of copy_)\n",
    "    #     q = self.rotary(norm(q))\n",
    "    #     k = self.rotary(norm(k))\n",
    "    \n",
    "    #     # Mix ve into v without in-place ops\n",
    "    #     if ve is not None:\n",
    "    #         v = lambdas[0] * v + lambdas[1] * ve.view_as(v)\n",
    "    #     else:\n",
    "    #         v = lambdas[0] * v\n",
    "    \n",
    "    #     # Call FlashAttention (ensure contiguous buffers, correct dtype)\n",
    "    #     y = flash_attn_func(\n",
    "    #         q.contiguous(),\n",
    "    #         k.contiguous(),\n",
    "    #         v.contiguous(),\n",
    "    #         softmax_scale=self.attn_scale,   # or None\n",
    "    #         causal=True,\n",
    "    #         window_size=(bm_size, 0),\n",
    "    #         deterministic=False,\n",
    "    #     )  # (B, T, H, D)\n",
    "    \n",
    "    #     y = y.reshape(B, T, H * D)\n",
    "    #     y = self.c_proj(y)\n",
    "    #     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6828-b90a-47c6-9759-73bbab68521c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "992d3a44-eca9-4645-b9b9-b678cd76fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = CastedLinear(dim, hdim)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(\n",
    "            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa25d-96b7-4026-904d-e9b7f6e7ce7a",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c18407-3a67-4e22-9f5f-7717fb3cffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, bm_size: int):\n",
    "        x = lambdas[0] * x + lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, sa_lambdas, bm_size)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84352c5e-79c7-4a81-a683-e6a3b1561e95",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d3a347c-fb0a-4614-8975-55df41aef6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e0bf666-e06a-4164-909b-4408c67b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897\n",
    "        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.\n",
    "        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.\n",
    "        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim ** 0.5) / 448, w_s=24 / 448,\n",
    "                                    grad_s=1 / 448)\n",
    "        self.lm_head.weight.detach().zero_()  # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        pad = (-num_layers * 5) % dist.get_world_size()\n",
    "        self.scalars = nn.Parameter(torch.cat([\n",
    "            torch.ones(num_layers),  # skip_weights\n",
    "            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],  # block lambdas\n",
    "            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],  # SA lambdas\n",
    "            torch.ones(pad),\n",
    "        ]))\n",
    "        # set learning rates\n",
    "        for param in self.embed.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        for param in self.value_embeds.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        self.lm_head.weight.lr_mul = 27.5\n",
    "        self.scalars.lr_mul = 5.0\n",
    "        \n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor):\n",
    "        assert input_seq.ndim == 1\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        long_bm, short_bm = 4 * BLOCK_SIZE, BLOCK_SIZE \n",
    "        bm_sizes = [long_bm, short_bm, short_bm, \n",
    "                       short_bm, long_bm, short_bm, \n",
    "                       short_bm, long_bm, short_bm, \n",
    "                       short_bm, short_bm, long_bm]\n",
    "        assert len(bm_sizes) == len(self.blocks)\n",
    "\n",
    "        x = x0 = norm(self.embed(input_seq)[None])  # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        skip_weights = self.scalars[:(len(self.blocks) // 2)]\n",
    "        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)\n",
    "        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)\n",
    "\n",
    "        n = len(self.blocks) // 2\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], bm_sizes[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "        logits = self.lm_head(x).float()\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1) ** 0.5))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq,\n",
    "                               reduction=\"sum\" if self.training else \"mean\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18b470-51e6-466a-a90b-20606dcac5dc",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e719acf6-7de5-4e4f-97db-20e6397ead89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d9c76-d429-4926-bf62-703b99fa056c",
   "metadata": {},
   "source": [
    "Previous approach:\n",
    "- builds boolean mask over max_batch_span tokens; linear in max_batch_span\n",
    "- new approach: build EOS index per shard. Log search on number of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1f0904-4288-4559-b8ca-20f6084d5bf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EOSBatchFinder:\n",
    "    def __init__(self, tokens: Tensor, eos_id: int = 50256):\n",
    "        # Precompute EOS positions once per shard \n",
    "        self.eos_idx = (tokens == eos_id).nonzero(as_tuple=True)[0].to(torch.int64).cpu()\n",
    "        self.i = 0      # pointer into eos_idx (start EOS for next step)\n",
    "        self.pos = 0    # logical stream position within this shard\n",
    "\n",
    "    def seek(self, pos: int):\n",
    "        # Set pointer to the first EOS >= pos\n",
    "        self.i = int(torch.searchsorted(self.eos_idx, pos))\n",
    "        if self.i >= int(self.eos_idx.numel()):\n",
    "            raise StopIteration(\"Seek past last EOS.\")\n",
    "        self.pos = pos\n",
    "\n",
    "    def next_batch(self, local_batch_size: int, world_size: int = 1):\n",
    "        n = int(self.eos_idx.numel())\n",
    "        if self.i >= n:\n",
    "            raise StopIteration(\"No more EOS in this shard.\")\n",
    "\n",
    "        starts = []\n",
    "        idx = self.i\n",
    "        cur = int(self.eos_idx[idx])  # absolute position (token index) of current start\n",
    "\n",
    "        # For each rank, find the first EOS >= cur + local_batch_size\n",
    "        for _ in range(world_size):\n",
    "            target = cur + local_batch_size\n",
    "            j = int(torch.searchsorted(self.eos_idx, target))\n",
    "            if j >= n:\n",
    "                raise StopIteration(\"Insufficient EOS ahead; hit tail of shard.\")\n",
    "            starts.append(cur)\n",
    "            idx = j\n",
    "            cur = int(self.eos_idx[idx])  # next segment starts at this end EOS\n",
    "\n",
    "        advance = int(self.eos_idx[idx] - self.pos)  # move stream to the last end\n",
    "        self.pos += advance\n",
    "        self.i = idx\n",
    "        return starts, advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f83a1c-a134-47ae-95e0-6386477a5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]\n",
    "    assert batch_size % world_size == 0\n",
    "    local_batch_size = batch_size // world_size\n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "\n",
    "    finder = EOSBatchFinder(tokens) if align_to_bos else None\n",
    "    if align_to_bos:\n",
    "        finder.seek(pos)\n",
    "    \n",
    "    max_batch_span = batch_size # only used when not align_to_bos\n",
    "    while True:\n",
    "        if pos + max_batch_span + 1 >= len(tokens):\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "            if align_to_bos:\n",
    "                finder = EOSBatchFinder(tokens)\n",
    "                finder.seek(pos)\n",
    "        if align_to_bos:\n",
    "            try:\n",
    "                batch_starts, batch_span = finder.next_batch(local_batch_size, world_size)\n",
    "                start_idx = batch_starts[rank]\n",
    "            except StopIteration:\n",
    "                # move to next shard on next loop\n",
    "                pos = len(tokens) + 1\n",
    "                continue\n",
    "        else:\n",
    "            batch_span = batch_size\n",
    "            start_idx = pos + rank * local_batch_size\n",
    "        buf = tokens[start_idx:][:local_batch_size + 1]\n",
    "        inputs = buf[:-1].to(device=\"cuda\", dtype=torch.int32, non_blocking=True) # no sync on host side;\n",
    "        targets = buf[1:].to(device=\"cuda\", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.\n",
    "        pos += batch_span\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6ff7-2376-4818-b0c6-63ea5c91a4c6",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d8c4d61-8438-4512-a4cd-476b74b5111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "031c79ec-83bf-4c7c-8bc1-d1f48488720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files = \"../data/fineweb10B/fineweb_train_*.bin\"  # input .bin to train on\n",
    "    val_files = \"../data/fineweb10B/fineweb_val_*.bin\"  # input .bin to eval validation loss on\n",
    "    val_tokens = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    train_seq_len = 48 * 1024  # FlexAttention sequence length\n",
    "    val_seq_len = 4 * 64 * 1024  # FlexAttention sequence length for validation\n",
    "    # optimization\n",
    "    num_iterations = 1750  # number of iterations to run\n",
    "    cooldown_frac = 0.45  # fraction of training spent cooling down the learning rate\n",
    "    # evaluation and logging\n",
    "    val_loss_every = 350  # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "\n",
    "    vocab_size = next_multiple_of_n(50257, n=128)\n",
    "    num_layers = 12\n",
    "    num_heads = 6\n",
    "    model_dim = 768\n",
    "\n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00850fb8-6c7a-47ae-8b3f-7f03570bc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['LOCAL_RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['GROUP_RANK'] = '0'\n",
    "os.environ['ROLE_RANK'] = '0'\n",
    "os.environ['ROLE_NAME'] = 'default'\n",
    "os.environ['ROLE_WORLD_SIZE'] = '1'\n",
    "os.environ['TORCHELASTIC_RESTART_COUNT'] = '0'\n",
    "os.environ['TORCHELASTIC_MAX_RESTARTS'] = '0'\n",
    "os.environ['TORCHELASTIC_RUN_ID'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "711588cc-0891-40ae-867f-d6aad225b656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "master_process = (rank == 0)  # this process will do logging, checkpointing etc.\n",
    "# assert world_size == 8  # this code is designed for 8xH100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e41bdf92-3c1d-49f6-bf5b-1f40cf019fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "703f7239-29f7-4991-8539-df8c1b4eeb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea574fff-3d48-4806-bc64-6cf7a9968b33",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c007ae54-89cc-448a-8380-634e61fe4ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/0494fc83-0457-4a1c-8a89-4a1f74417db8.txt\n"
     ]
    }
   ],
   "source": [
    "# begin logging\n",
    "logfile = None\n",
    "if master_process:\n",
    "    run_id = uuid.uuid4()\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logfile = f\"logs/{run_id}.txt\"\n",
    "    print(logfile)\n",
    "\n",
    "\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fddc10c5-9eed-45e3-9fd9-b4413a8d5994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 20 19:30:06 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                  Off |\n",
      "| N/A   37C    P0              66W / 350W |   1103MiB / 81559MiB |      2%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb0b9b-de7f-4052-a2fe-e6f9a69fc930",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "253fdb42-b56c-4a3b-bb0c-7ac5837d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = GPT(vocab_size=args.vocab_size, \n",
    "                       num_layers=args.num_layers, \n",
    "                       num_heads=args.num_heads, \n",
    "                       model_dim=args.model_dim,\n",
    "                       max_seq_len=max(args.train_seq_len, args.val_seq_len)\n",
    "                      ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c4760a4-1858-499b-b961-530cd04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a03b657a-b05f-4aa5-bf53-d07702385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12334b9f-193f-446b-a3e8-00a1479bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10,\n",
    "                      weight_decay=0.0)\n",
    "optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94c83fbd-88f8-45b2-83d5-608a5ce607db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"TORCH_LOGS\"] = \"recompiles,graph_breaks\"\n",
    "# os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15d99b33-47c2-4b24-9316-e591e946a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = torch.compile(model, dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809242-8dca-4f72-bc28-736aa58fc03c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0e811f2-8daa-4992-8271-1ca7f9410fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    if x < 1 - args.cooldown_frac:\n",
    "        return 1.0\n",
    "    else:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        return w * 1.0 + (1 - w) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e779f1-8889-4a9b-afcc-6a3172237e49",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc40a3d-7ad0-4a8c-a035-e14357268cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 100\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state\n",
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1ffe97c-e49c-46f1-a9b1-49a81efc762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4122a6e-94d4-47a7-8d2c-ca7378e8cd52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abcf44eea8147d281def61093bb803d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: This API is going to be deprecated, please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(warmup_steps)):\n",
    "    inputs, targets = next(train_loader)\n",
    "    model(inputs, targets).backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "model.load_state_dict(initial_state[\"model\"])\n",
    "for opt, opt_state in zip(optimizers, initial_state[\"optimizers\"]):\n",
    "    opt.load_state_dict(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a71d51cc-f59c-4591-ae56-5a36fe2d912c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del train_loader, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27d96a-9085-417b-b8e3-479220a3f2e1",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5496e90d-9c4e-43ce-92f6-b49362e624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)\n",
    "train_steps = args.num_iterations\n",
    "training_time_ms = 0\n",
    "tokens_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c30bd45-5c91-4e04-a04e-3fc4021306eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---validation---\n",
      "step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.34ms\n",
      "---end of validation---\n",
      "First inputs retrieved\n",
      "step:1/1750 train_time:246ms step_avg:246.27ms tokens_processed:49152\n",
      "step:2/1750 train_time:386ms step_avg:193.02ms tokens_processed:49152\n",
      "step:3/1750 train_time:537ms step_avg:178.92ms tokens_processed:49152\n",
      "step:4/1750 train_time:691ms step_avg:172.64ms tokens_processed:49152\n",
      "step:5/1750 train_time:843ms step_avg:168.68ms tokens_processed:49152\n",
      "step:6/1750 train_time:998ms step_avg:166.33ms tokens_processed:49152\n",
      "step:7/1750 train_time:1155ms step_avg:165.05ms tokens_processed:49152\n",
      "step:8/1750 train_time:1315ms step_avg:164.41ms tokens_processed:49152\n",
      "step:9/1750 train_time:1472ms step_avg:163.56ms tokens_processed:49152\n",
      "step:10/1750 train_time:1628ms step_avg:162.81ms tokens_processed:49152\n",
      "step:11/1750 train_time:1782ms step_avg:161.98ms tokens_processed:49152\n",
      "step:12/1750 train_time:1936ms step_avg:161.36ms tokens_processed:49152\n",
      "step:13/1750 train_time:2093ms step_avg:160.97ms tokens_processed:49152\n",
      "step:14/1750 train_time:2249ms step_avg:160.66ms tokens_processed:49152\n",
      "step:15/1750 train_time:2406ms step_avg:160.37ms tokens_processed:49152\n",
      "step:16/1750 train_time:2561ms step_avg:160.09ms tokens_processed:49152\n",
      "step:17/1750 train_time:2717ms step_avg:159.83ms tokens_processed:49152\n",
      "step:18/1750 train_time:2873ms step_avg:159.61ms tokens_processed:49152\n",
      "step:19/1750 train_time:3029ms step_avg:159.44ms tokens_processed:49152\n",
      "step:20/1750 train_time:3185ms step_avg:159.26ms tokens_processed:49152\n",
      "step:21/1750 train_time:3342ms step_avg:159.12ms tokens_processed:49152\n",
      "step:22/1750 train_time:3497ms step_avg:158.93ms tokens_processed:49152\n",
      "step:23/1750 train_time:3653ms step_avg:158.83ms tokens_processed:49152\n",
      "step:24/1750 train_time:3809ms step_avg:158.70ms tokens_processed:49152\n",
      "step:25/1750 train_time:3966ms step_avg:158.62ms tokens_processed:49152\n",
      "step:26/1750 train_time:4121ms step_avg:158.51ms tokens_processed:49152\n",
      "step:27/1750 train_time:4279ms step_avg:158.46ms tokens_processed:49152\n",
      "step:28/1750 train_time:4436ms step_avg:158.42ms tokens_processed:49152\n",
      "step:29/1750 train_time:4593ms step_avg:158.39ms tokens_processed:49152\n",
      "step:30/1750 train_time:4750ms step_avg:158.34ms tokens_processed:49152\n",
      "step:31/1750 train_time:4905ms step_avg:158.24ms tokens_processed:49152\n",
      "step:32/1750 train_time:5061ms step_avg:158.16ms tokens_processed:49152\n",
      "step:33/1750 train_time:5217ms step_avg:158.10ms tokens_processed:49152\n",
      "step:34/1750 train_time:5374ms step_avg:158.07ms tokens_processed:49152\n",
      "step:35/1750 train_time:5533ms step_avg:158.10ms tokens_processed:49152\n",
      "step:36/1750 train_time:5689ms step_avg:158.03ms tokens_processed:49152\n",
      "step:37/1750 train_time:5844ms step_avg:157.94ms tokens_processed:49152\n",
      "step:38/1750 train_time:5999ms step_avg:157.87ms tokens_processed:49152\n",
      "step:39/1750 train_time:6156ms step_avg:157.86ms tokens_processed:49152\n",
      "step:40/1750 train_time:6312ms step_avg:157.81ms tokens_processed:49152\n",
      "step:41/1750 train_time:6470ms step_avg:157.80ms tokens_processed:49152\n",
      "step:42/1750 train_time:6627ms step_avg:157.78ms tokens_processed:49152\n",
      "step:43/1750 train_time:6783ms step_avg:157.74ms tokens_processed:49152\n",
      "step:44/1750 train_time:6941ms step_avg:157.75ms tokens_processed:49152\n",
      "step:45/1750 train_time:7095ms step_avg:157.68ms tokens_processed:49152\n",
      "step:46/1750 train_time:7251ms step_avg:157.63ms tokens_processed:49152\n",
      "step:47/1750 train_time:7408ms step_avg:157.61ms tokens_processed:49152\n",
      "step:48/1750 train_time:7566ms step_avg:157.62ms tokens_processed:49152\n",
      "step:49/1750 train_time:7721ms step_avg:157.58ms tokens_processed:49152\n",
      "step:50/1750 train_time:7878ms step_avg:157.56ms tokens_processed:49152\n",
      "step:51/1750 train_time:8035ms step_avg:157.55ms tokens_processed:49152\n",
      "step:52/1750 train_time:8191ms step_avg:157.52ms tokens_processed:49152\n",
      "step:53/1750 train_time:8346ms step_avg:157.47ms tokens_processed:49152\n",
      "step:54/1750 train_time:8502ms step_avg:157.44ms tokens_processed:49152\n",
      "step:55/1750 train_time:8657ms step_avg:157.40ms tokens_processed:49152\n",
      "step:56/1750 train_time:8813ms step_avg:157.37ms tokens_processed:49152\n",
      "step:57/1750 train_time:8972ms step_avg:157.40ms tokens_processed:49152\n",
      "step:58/1750 train_time:9128ms step_avg:157.38ms tokens_processed:49152\n",
      "step:59/1750 train_time:9284ms step_avg:157.36ms tokens_processed:49152\n",
      "step:60/1750 train_time:9440ms step_avg:157.33ms tokens_processed:49152\n",
      "step:61/1750 train_time:9597ms step_avg:157.33ms tokens_processed:49152\n",
      "step:62/1750 train_time:9753ms step_avg:157.31ms tokens_processed:49152\n",
      "step:63/1750 train_time:9911ms step_avg:157.32ms tokens_processed:49152\n",
      "step:64/1750 train_time:10068ms step_avg:157.31ms tokens_processed:49152\n",
      "step:65/1750 train_time:10224ms step_avg:157.29ms tokens_processed:49152\n",
      "step:66/1750 train_time:10381ms step_avg:157.28ms tokens_processed:49152\n",
      "step:67/1750 train_time:10537ms step_avg:157.26ms tokens_processed:49152\n",
      "step:68/1750 train_time:10694ms step_avg:157.26ms tokens_processed:49152\n",
      "step:69/1750 train_time:10850ms step_avg:157.25ms tokens_processed:49152\n",
      "step:70/1750 train_time:11006ms step_avg:157.23ms tokens_processed:49152\n",
      "step:71/1750 train_time:11163ms step_avg:157.23ms tokens_processed:49152\n",
      "step:72/1750 train_time:11320ms step_avg:157.22ms tokens_processed:49152\n",
      "step:73/1750 train_time:11476ms step_avg:157.21ms tokens_processed:49152\n",
      "step:74/1750 train_time:11633ms step_avg:157.21ms tokens_processed:49152\n",
      "step:75/1750 train_time:11790ms step_avg:157.20ms tokens_processed:49152\n",
      "step:76/1750 train_time:11946ms step_avg:157.18ms tokens_processed:49152\n",
      "step:77/1750 train_time:12102ms step_avg:157.17ms tokens_processed:49152\n",
      "step:78/1750 train_time:12258ms step_avg:157.16ms tokens_processed:49152\n",
      "step:79/1750 train_time:12416ms step_avg:157.16ms tokens_processed:49152\n",
      "step:80/1750 train_time:12575ms step_avg:157.18ms tokens_processed:49152\n",
      "step:81/1750 train_time:12732ms step_avg:157.19ms tokens_processed:49152\n",
      "step:82/1750 train_time:12890ms step_avg:157.19ms tokens_processed:49152\n",
      "step:83/1750 train_time:13045ms step_avg:157.17ms tokens_processed:49152\n",
      "step:84/1750 train_time:13202ms step_avg:157.16ms tokens_processed:49152\n",
      "step:85/1750 train_time:13357ms step_avg:157.14ms tokens_processed:49152\n",
      "step:86/1750 train_time:13514ms step_avg:157.14ms tokens_processed:49152\n",
      "step:87/1750 train_time:13673ms step_avg:157.16ms tokens_processed:49152\n",
      "step:88/1750 train_time:13831ms step_avg:157.18ms tokens_processed:49152\n",
      "step:89/1750 train_time:13988ms step_avg:157.17ms tokens_processed:49152\n",
      "step:90/1750 train_time:14144ms step_avg:157.15ms tokens_processed:49152\n",
      "step:91/1750 train_time:14299ms step_avg:157.13ms tokens_processed:49152\n",
      "step:92/1750 train_time:14455ms step_avg:157.12ms tokens_processed:49152\n",
      "step:93/1750 train_time:14613ms step_avg:157.13ms tokens_processed:49152\n",
      "step:94/1750 train_time:14771ms step_avg:157.14ms tokens_processed:49152\n",
      "step:95/1750 train_time:14929ms step_avg:157.14ms tokens_processed:49152\n",
      "step:96/1750 train_time:15085ms step_avg:157.14ms tokens_processed:49152\n",
      "step:97/1750 train_time:15240ms step_avg:157.12ms tokens_processed:49152\n",
      "step:98/1750 train_time:15397ms step_avg:157.11ms tokens_processed:49152\n",
      "step:99/1750 train_time:15553ms step_avg:157.10ms tokens_processed:49152\n",
      "step:100/1750 train_time:15712ms step_avg:157.12ms tokens_processed:49152\n",
      "step:101/1750 train_time:15869ms step_avg:157.12ms tokens_processed:49152\n",
      "step:102/1750 train_time:16024ms step_avg:157.10ms tokens_processed:49152\n",
      "step:103/1750 train_time:16180ms step_avg:157.09ms tokens_processed:49152\n",
      "step:104/1750 train_time:16336ms step_avg:157.08ms tokens_processed:49152\n",
      "step:105/1750 train_time:16493ms step_avg:157.08ms tokens_processed:49152\n",
      "step:106/1750 train_time:16650ms step_avg:157.08ms tokens_processed:49152\n",
      "step:107/1750 train_time:16807ms step_avg:157.07ms tokens_processed:49152\n",
      "step:108/1750 train_time:16963ms step_avg:157.06ms tokens_processed:49152\n",
      "step:109/1750 train_time:17120ms step_avg:157.07ms tokens_processed:49152\n",
      "step:110/1750 train_time:17276ms step_avg:157.06ms tokens_processed:49152\n",
      "step:111/1750 train_time:17434ms step_avg:157.06ms tokens_processed:49152\n",
      "step:112/1750 train_time:17592ms step_avg:157.07ms tokens_processed:49152\n",
      "step:113/1750 train_time:17748ms step_avg:157.06ms tokens_processed:49152\n",
      "step:114/1750 train_time:17905ms step_avg:157.06ms tokens_processed:49152\n",
      "step:115/1750 train_time:18061ms step_avg:157.05ms tokens_processed:49152\n",
      "step:116/1750 train_time:18217ms step_avg:157.04ms tokens_processed:49152\n",
      "step:117/1750 train_time:18375ms step_avg:157.05ms tokens_processed:49152\n",
      "step:118/1750 train_time:18534ms step_avg:157.07ms tokens_processed:49152\n",
      "step:119/1750 train_time:18691ms step_avg:157.06ms tokens_processed:49152\n",
      "step:120/1750 train_time:18846ms step_avg:157.05ms tokens_processed:49152\n",
      "step:121/1750 train_time:19002ms step_avg:157.04ms tokens_processed:49152\n",
      "step:122/1750 train_time:19159ms step_avg:157.04ms tokens_processed:49152\n",
      "step:123/1750 train_time:19316ms step_avg:157.04ms tokens_processed:49152\n",
      "step:124/1750 train_time:19474ms step_avg:157.05ms tokens_processed:49152\n",
      "step:125/1750 train_time:19632ms step_avg:157.06ms tokens_processed:49152\n",
      "step:126/1750 train_time:19789ms step_avg:157.06ms tokens_processed:49152\n",
      "step:127/1750 train_time:19945ms step_avg:157.05ms tokens_processed:49152\n",
      "step:128/1750 train_time:20101ms step_avg:157.04ms tokens_processed:49152\n",
      "step:129/1750 train_time:20257ms step_avg:157.03ms tokens_processed:49152\n",
      "step:130/1750 train_time:20414ms step_avg:157.03ms tokens_processed:49152\n",
      "step:131/1750 train_time:20573ms step_avg:157.05ms tokens_processed:49152\n",
      "step:132/1750 train_time:20732ms step_avg:157.06ms tokens_processed:49152\n",
      "step:133/1750 train_time:20889ms step_avg:157.06ms tokens_processed:49152\n",
      "step:134/1750 train_time:21044ms step_avg:157.04ms tokens_processed:49152\n",
      "step:135/1750 train_time:21201ms step_avg:157.04ms tokens_processed:49152\n",
      "step:136/1750 train_time:21358ms step_avg:157.04ms tokens_processed:49152\n",
      "step:137/1750 train_time:21515ms step_avg:157.04ms tokens_processed:49152\n",
      "step:138/1750 train_time:21674ms step_avg:157.06ms tokens_processed:49152\n",
      "step:139/1750 train_time:21832ms step_avg:157.06ms tokens_processed:49152\n",
      "step:140/1750 train_time:21989ms step_avg:157.06ms tokens_processed:49152\n",
      "step:141/1750 train_time:22144ms step_avg:157.05ms tokens_processed:49152\n",
      "step:142/1750 train_time:22300ms step_avg:157.04ms tokens_processed:49152\n",
      "step:143/1750 train_time:22458ms step_avg:157.05ms tokens_processed:49152\n",
      "step:144/1750 train_time:22614ms step_avg:157.04ms tokens_processed:49152\n",
      "step:145/1750 train_time:22773ms step_avg:157.06ms tokens_processed:49152\n",
      "step:146/1750 train_time:22932ms step_avg:157.07ms tokens_processed:49152\n",
      "step:147/1750 train_time:23089ms step_avg:157.07ms tokens_processed:49152\n",
      "step:148/1750 train_time:23244ms step_avg:157.05ms tokens_processed:49152\n",
      "step:149/1750 train_time:23400ms step_avg:157.05ms tokens_processed:49152\n",
      "step:150/1750 train_time:23557ms step_avg:157.05ms tokens_processed:49152\n",
      "step:151/1750 train_time:23715ms step_avg:157.05ms tokens_processed:49152\n",
      "step:152/1750 train_time:23874ms step_avg:157.07ms tokens_processed:49152\n",
      "step:153/1750 train_time:24032ms step_avg:157.07ms tokens_processed:49152\n",
      "step:154/1750 train_time:24189ms step_avg:157.07ms tokens_processed:49152\n",
      "step:155/1750 train_time:24345ms step_avg:157.07ms tokens_processed:49152\n",
      "step:156/1750 train_time:24501ms step_avg:157.06ms tokens_processed:49152\n",
      "step:157/1750 train_time:24658ms step_avg:157.05ms tokens_processed:49152\n",
      "step:158/1750 train_time:24816ms step_avg:157.06ms tokens_processed:49152\n",
      "step:159/1750 train_time:24977ms step_avg:157.09ms tokens_processed:49152\n",
      "step:160/1750 train_time:25134ms step_avg:157.09ms tokens_processed:49152\n",
      "step:161/1750 train_time:25291ms step_avg:157.09ms tokens_processed:49152\n",
      "step:162/1750 train_time:25447ms step_avg:157.08ms tokens_processed:49152\n",
      "step:163/1750 train_time:25602ms step_avg:157.07ms tokens_processed:49152\n",
      "step:164/1750 train_time:25759ms step_avg:157.07ms tokens_processed:49152\n",
      "step:165/1750 train_time:25917ms step_avg:157.07ms tokens_processed:49152\n",
      "step:166/1750 train_time:26076ms step_avg:157.08ms tokens_processed:49152\n",
      "step:167/1750 train_time:26234ms step_avg:157.09ms tokens_processed:49152\n",
      "step:168/1750 train_time:26391ms step_avg:157.09ms tokens_processed:49152\n",
      "step:169/1750 train_time:26546ms step_avg:157.08ms tokens_processed:49152\n",
      "step:170/1750 train_time:26703ms step_avg:157.07ms tokens_processed:49152\n",
      "step:171/1750 train_time:26860ms step_avg:157.07ms tokens_processed:49152\n",
      "step:172/1750 train_time:27017ms step_avg:157.08ms tokens_processed:49152\n",
      "step:173/1750 train_time:27176ms step_avg:157.09ms tokens_processed:49152\n",
      "step:174/1750 train_time:27335ms step_avg:157.10ms tokens_processed:49152\n",
      "step:175/1750 train_time:27494ms step_avg:157.11ms tokens_processed:49152\n",
      "step:176/1750 train_time:27648ms step_avg:157.09ms tokens_processed:49152\n",
      "step:177/1750 train_time:27803ms step_avg:157.08ms tokens_processed:49152\n",
      "step:178/1750 train_time:27958ms step_avg:157.07ms tokens_processed:49152\n",
      "step:179/1750 train_time:28115ms step_avg:157.07ms tokens_processed:49152\n",
      "step:180/1750 train_time:28273ms step_avg:157.07ms tokens_processed:49152\n",
      "step:181/1750 train_time:28431ms step_avg:157.08ms tokens_processed:49152\n",
      "step:182/1750 train_time:28586ms step_avg:157.07ms tokens_processed:49152\n",
      "step:183/1750 train_time:28741ms step_avg:157.06ms tokens_processed:49152\n",
      "step:184/1750 train_time:28898ms step_avg:157.06ms tokens_processed:49152\n",
      "step:185/1750 train_time:29053ms step_avg:157.04ms tokens_processed:49152\n",
      "step:186/1750 train_time:29209ms step_avg:157.04ms tokens_processed:49152\n",
      "step:187/1750 train_time:29366ms step_avg:157.04ms tokens_processed:49152\n",
      "step:188/1750 train_time:29522ms step_avg:157.03ms tokens_processed:49152\n",
      "step:189/1750 train_time:29678ms step_avg:157.03ms tokens_processed:49152\n",
      "step:190/1750 train_time:29834ms step_avg:157.02ms tokens_processed:49152\n",
      "step:191/1750 train_time:29990ms step_avg:157.02ms tokens_processed:49152\n",
      "step:192/1750 train_time:30145ms step_avg:157.01ms tokens_processed:49152\n",
      "step:193/1750 train_time:30301ms step_avg:157.00ms tokens_processed:49152\n",
      "step:194/1750 train_time:30457ms step_avg:157.00ms tokens_processed:49152\n",
      "step:195/1750 train_time:30614ms step_avg:156.99ms tokens_processed:49152\n",
      "step:196/1750 train_time:30770ms step_avg:156.99ms tokens_processed:49152\n",
      "step:197/1750 train_time:30926ms step_avg:156.98ms tokens_processed:49152\n",
      "step:198/1750 train_time:31081ms step_avg:156.98ms tokens_processed:49152\n",
      "step:199/1750 train_time:31238ms step_avg:156.97ms tokens_processed:49152\n",
      "step:200/1750 train_time:31394ms step_avg:156.97ms tokens_processed:49152\n",
      "step:201/1750 train_time:31550ms step_avg:156.97ms tokens_processed:49152\n",
      "step:202/1750 train_time:31706ms step_avg:156.96ms tokens_processed:49152\n",
      "step:203/1750 train_time:31862ms step_avg:156.96ms tokens_processed:49152\n",
      "step:204/1750 train_time:32018ms step_avg:156.95ms tokens_processed:49152\n",
      "step:205/1750 train_time:32175ms step_avg:156.95ms tokens_processed:49152\n",
      "step:206/1750 train_time:32332ms step_avg:156.95ms tokens_processed:49152\n",
      "step:207/1750 train_time:32489ms step_avg:156.95ms tokens_processed:49152\n",
      "step:208/1750 train_time:32644ms step_avg:156.94ms tokens_processed:49152\n",
      "step:209/1750 train_time:32800ms step_avg:156.94ms tokens_processed:49152\n",
      "step:210/1750 train_time:32955ms step_avg:156.93ms tokens_processed:49152\n",
      "step:211/1750 train_time:33112ms step_avg:156.93ms tokens_processed:49152\n",
      "step:212/1750 train_time:33268ms step_avg:156.93ms tokens_processed:49152\n",
      "step:213/1750 train_time:33425ms step_avg:156.93ms tokens_processed:49152\n",
      "step:214/1750 train_time:33581ms step_avg:156.92ms tokens_processed:49152\n",
      "step:215/1750 train_time:33737ms step_avg:156.92ms tokens_processed:49152\n",
      "step:216/1750 train_time:33893ms step_avg:156.91ms tokens_processed:49152\n",
      "step:217/1750 train_time:34048ms step_avg:156.90ms tokens_processed:49152\n",
      "step:218/1750 train_time:34204ms step_avg:156.90ms tokens_processed:49152\n",
      "step:219/1750 train_time:34360ms step_avg:156.90ms tokens_processed:49152\n",
      "step:220/1750 train_time:34517ms step_avg:156.89ms tokens_processed:49152\n",
      "step:221/1750 train_time:34674ms step_avg:156.90ms tokens_processed:49152\n",
      "step:222/1750 train_time:34831ms step_avg:156.90ms tokens_processed:49152\n",
      "step:223/1750 train_time:34986ms step_avg:156.89ms tokens_processed:49152\n",
      "step:224/1750 train_time:35142ms step_avg:156.88ms tokens_processed:49152\n",
      "step:225/1750 train_time:35297ms step_avg:156.88ms tokens_processed:49152\n",
      "step:226/1750 train_time:35452ms step_avg:156.87ms tokens_processed:49152\n",
      "step:227/1750 train_time:35609ms step_avg:156.87ms tokens_processed:49152\n",
      "step:228/1750 train_time:35766ms step_avg:156.87ms tokens_processed:49152\n",
      "step:229/1750 train_time:35922ms step_avg:156.87ms tokens_processed:49152\n",
      "step:230/1750 train_time:36078ms step_avg:156.86ms tokens_processed:49152\n",
      "step:231/1750 train_time:36235ms step_avg:156.86ms tokens_processed:49152\n",
      "step:232/1750 train_time:36391ms step_avg:156.86ms tokens_processed:49152\n",
      "step:233/1750 train_time:36546ms step_avg:156.85ms tokens_processed:49152\n",
      "step:234/1750 train_time:36701ms step_avg:156.84ms tokens_processed:49152\n",
      "step:235/1750 train_time:36857ms step_avg:156.84ms tokens_processed:49152\n",
      "step:236/1750 train_time:37013ms step_avg:156.84ms tokens_processed:49152\n",
      "step:237/1750 train_time:37171ms step_avg:156.84ms tokens_processed:49152\n",
      "step:238/1750 train_time:37327ms step_avg:156.84ms tokens_processed:49152\n",
      "step:239/1750 train_time:37483ms step_avg:156.83ms tokens_processed:49152\n",
      "step:240/1750 train_time:37639ms step_avg:156.83ms tokens_processed:49152\n",
      "step:241/1750 train_time:37796ms step_avg:156.83ms tokens_processed:49152\n",
      "step:242/1750 train_time:37952ms step_avg:156.83ms tokens_processed:49152\n",
      "step:243/1750 train_time:38107ms step_avg:156.82ms tokens_processed:49152\n",
      "step:244/1750 train_time:38265ms step_avg:156.82ms tokens_processed:49152\n",
      "step:245/1750 train_time:38421ms step_avg:156.82ms tokens_processed:49152\n",
      "step:246/1750 train_time:38578ms step_avg:156.82ms tokens_processed:49152\n",
      "step:247/1750 train_time:38734ms step_avg:156.82ms tokens_processed:49152\n",
      "step:248/1750 train_time:38891ms step_avg:156.82ms tokens_processed:49152\n",
      "step:249/1750 train_time:39045ms step_avg:156.81ms tokens_processed:49152\n",
      "step:250/1750 train_time:39201ms step_avg:156.81ms tokens_processed:49152\n",
      "step:251/1750 train_time:39357ms step_avg:156.80ms tokens_processed:49152\n",
      "step:252/1750 train_time:39514ms step_avg:156.80ms tokens_processed:49152\n",
      "step:253/1750 train_time:39672ms step_avg:156.81ms tokens_processed:49152\n",
      "step:254/1750 train_time:39827ms step_avg:156.80ms tokens_processed:49152\n",
      "step:255/1750 train_time:39984ms step_avg:156.80ms tokens_processed:49152\n",
      "step:256/1750 train_time:40139ms step_avg:156.79ms tokens_processed:49152\n",
      "step:257/1750 train_time:40297ms step_avg:156.80ms tokens_processed:49152\n",
      "step:258/1750 train_time:40453ms step_avg:156.79ms tokens_processed:49152\n",
      "step:259/1750 train_time:40609ms step_avg:156.79ms tokens_processed:49152\n",
      "step:260/1750 train_time:40766ms step_avg:156.79ms tokens_processed:49152\n",
      "step:261/1750 train_time:40922ms step_avg:156.79ms tokens_processed:49152\n",
      "step:262/1750 train_time:41079ms step_avg:156.79ms tokens_processed:49152\n",
      "step:263/1750 train_time:41235ms step_avg:156.79ms tokens_processed:49152\n",
      "step:264/1750 train_time:41391ms step_avg:156.79ms tokens_processed:49152\n",
      "step:265/1750 train_time:41547ms step_avg:156.78ms tokens_processed:49152\n",
      "step:266/1750 train_time:41703ms step_avg:156.78ms tokens_processed:49152\n",
      "step:267/1750 train_time:41859ms step_avg:156.78ms tokens_processed:49152\n",
      "step:268/1750 train_time:42016ms step_avg:156.77ms tokens_processed:49152\n",
      "step:269/1750 train_time:42173ms step_avg:156.78ms tokens_processed:49152\n",
      "step:270/1750 train_time:42331ms step_avg:156.78ms tokens_processed:49152\n",
      "step:271/1750 train_time:42488ms step_avg:156.78ms tokens_processed:49152\n",
      "step:272/1750 train_time:42642ms step_avg:156.77ms tokens_processed:49152\n",
      "step:273/1750 train_time:42798ms step_avg:156.77ms tokens_processed:49152\n",
      "step:274/1750 train_time:42954ms step_avg:156.77ms tokens_processed:49152\n",
      "step:275/1750 train_time:43111ms step_avg:156.77ms tokens_processed:49152\n",
      "step:276/1750 train_time:43268ms step_avg:156.77ms tokens_processed:49152\n",
      "step:277/1750 train_time:43424ms step_avg:156.77ms tokens_processed:49152\n",
      "step:278/1750 train_time:43581ms step_avg:156.77ms tokens_processed:49152\n",
      "step:279/1750 train_time:43738ms step_avg:156.77ms tokens_processed:49152\n",
      "step:280/1750 train_time:43895ms step_avg:156.77ms tokens_processed:49152\n",
      "step:281/1750 train_time:44049ms step_avg:156.76ms tokens_processed:49152\n",
      "step:282/1750 train_time:44204ms step_avg:156.75ms tokens_processed:49152\n",
      "step:283/1750 train_time:44361ms step_avg:156.75ms tokens_processed:49152\n",
      "step:284/1750 train_time:44517ms step_avg:156.75ms tokens_processed:49152\n",
      "step:285/1750 train_time:44674ms step_avg:156.75ms tokens_processed:49152\n",
      "step:286/1750 train_time:44832ms step_avg:156.75ms tokens_processed:49152\n",
      "step:287/1750 train_time:44988ms step_avg:156.75ms tokens_processed:49152\n",
      "step:288/1750 train_time:45143ms step_avg:156.75ms tokens_processed:49152\n",
      "step:289/1750 train_time:45299ms step_avg:156.74ms tokens_processed:49152\n",
      "step:290/1750 train_time:45455ms step_avg:156.74ms tokens_processed:49152\n",
      "step:291/1750 train_time:45612ms step_avg:156.74ms tokens_processed:49152\n",
      "step:292/1750 train_time:45769ms step_avg:156.74ms tokens_processed:49152\n",
      "step:293/1750 train_time:45925ms step_avg:156.74ms tokens_processed:49152\n",
      "step:294/1750 train_time:46082ms step_avg:156.74ms tokens_processed:49152\n",
      "step:295/1750 train_time:46239ms step_avg:156.74ms tokens_processed:49152\n",
      "step:296/1750 train_time:46395ms step_avg:156.74ms tokens_processed:49152\n",
      "step:297/1750 train_time:46551ms step_avg:156.74ms tokens_processed:49152\n",
      "step:298/1750 train_time:46708ms step_avg:156.74ms tokens_processed:49152\n",
      "step:299/1750 train_time:46864ms step_avg:156.74ms tokens_processed:49152\n",
      "step:300/1750 train_time:47021ms step_avg:156.74ms tokens_processed:49152\n",
      "step:301/1750 train_time:47178ms step_avg:156.74ms tokens_processed:49152\n",
      "step:302/1750 train_time:47334ms step_avg:156.73ms tokens_processed:49152\n",
      "step:303/1750 train_time:47490ms step_avg:156.73ms tokens_processed:49152\n",
      "step:304/1750 train_time:47645ms step_avg:156.73ms tokens_processed:49152\n",
      "step:305/1750 train_time:47801ms step_avg:156.73ms tokens_processed:49152\n",
      "step:306/1750 train_time:47957ms step_avg:156.72ms tokens_processed:49152\n",
      "step:307/1750 train_time:48114ms step_avg:156.72ms tokens_processed:49152\n",
      "step:308/1750 train_time:48271ms step_avg:156.72ms tokens_processed:49152\n",
      "step:309/1750 train_time:48429ms step_avg:156.73ms tokens_processed:49152\n",
      "step:310/1750 train_time:48584ms step_avg:156.72ms tokens_processed:49152\n",
      "step:311/1750 train_time:48739ms step_avg:156.72ms tokens_processed:49152\n",
      "step:312/1750 train_time:48896ms step_avg:156.72ms tokens_processed:49152\n",
      "step:313/1750 train_time:49053ms step_avg:156.72ms tokens_processed:49152\n",
      "step:314/1750 train_time:49209ms step_avg:156.72ms tokens_processed:49152\n",
      "step:315/1750 train_time:49366ms step_avg:156.72ms tokens_processed:49152\n",
      "step:316/1750 train_time:49522ms step_avg:156.71ms tokens_processed:49152\n",
      "step:317/1750 train_time:49678ms step_avg:156.71ms tokens_processed:49152\n",
      "step:318/1750 train_time:49836ms step_avg:156.72ms tokens_processed:49152\n",
      "step:319/1750 train_time:49992ms step_avg:156.71ms tokens_processed:49152\n",
      "step:320/1750 train_time:50147ms step_avg:156.71ms tokens_processed:49152\n",
      "step:321/1750 train_time:50304ms step_avg:156.71ms tokens_processed:49152\n",
      "step:322/1750 train_time:50460ms step_avg:156.71ms tokens_processed:49152\n",
      "step:323/1750 train_time:50616ms step_avg:156.70ms tokens_processed:49152\n",
      "step:324/1750 train_time:50773ms step_avg:156.71ms tokens_processed:49152\n",
      "step:325/1750 train_time:50929ms step_avg:156.70ms tokens_processed:49152\n",
      "step:326/1750 train_time:51085ms step_avg:156.70ms tokens_processed:49152\n",
      "step:327/1750 train_time:51242ms step_avg:156.70ms tokens_processed:49152\n",
      "step:328/1750 train_time:51399ms step_avg:156.70ms tokens_processed:49152\n",
      "step:329/1750 train_time:51555ms step_avg:156.70ms tokens_processed:49152\n",
      "step:330/1750 train_time:51712ms step_avg:156.70ms tokens_processed:49152\n",
      "step:331/1750 train_time:51869ms step_avg:156.70ms tokens_processed:49152\n",
      "step:332/1750 train_time:52024ms step_avg:156.70ms tokens_processed:49152\n",
      "step:333/1750 train_time:52180ms step_avg:156.70ms tokens_processed:49152\n",
      "step:334/1750 train_time:52337ms step_avg:156.70ms tokens_processed:49152\n",
      "step:335/1750 train_time:52494ms step_avg:156.70ms tokens_processed:49152\n",
      "step:336/1750 train_time:52650ms step_avg:156.70ms tokens_processed:49152\n",
      "step:337/1750 train_time:52806ms step_avg:156.69ms tokens_processed:49152\n",
      "step:338/1750 train_time:52961ms step_avg:156.69ms tokens_processed:49152\n",
      "step:339/1750 train_time:53117ms step_avg:156.69ms tokens_processed:49152\n",
      "step:340/1750 train_time:53275ms step_avg:156.69ms tokens_processed:49152\n",
      "step:341/1750 train_time:53433ms step_avg:156.69ms tokens_processed:49152\n",
      "step:342/1750 train_time:53589ms step_avg:156.69ms tokens_processed:49152\n",
      "step:343/1750 train_time:53745ms step_avg:156.69ms tokens_processed:49152\n",
      "step:344/1750 train_time:53900ms step_avg:156.69ms tokens_processed:49152\n",
      "step:345/1750 train_time:54057ms step_avg:156.69ms tokens_processed:49152\n",
      "step:346/1750 train_time:54213ms step_avg:156.68ms tokens_processed:49152\n",
      "step:347/1750 train_time:54371ms step_avg:156.69ms tokens_processed:49152\n",
      "step:348/1750 train_time:54527ms step_avg:156.69ms tokens_processed:49152\n",
      "step:349/1750 train_time:54683ms step_avg:156.69ms tokens_processed:49152\n",
      "step:350/1750 train_time:54839ms step_avg:156.68ms tokens_processed:49152\n",
      "---validation---\n",
      "step:350/1750 val_loss:4.6737 train_time:54847ms step_avg:156.71ms\n",
      "---end of validation---\n",
      "step:351/1750 train_time:54997ms step_avg:156.69ms tokens_processed:49152\n",
      "step:352/1750 train_time:55153ms step_avg:156.68ms tokens_processed:49152\n",
      "step:353/1750 train_time:55310ms step_avg:156.68ms tokens_processed:49152\n",
      "step:354/1750 train_time:55467ms step_avg:156.69ms tokens_processed:49152\n",
      "step:355/1750 train_time:55623ms step_avg:156.69ms tokens_processed:49152\n",
      "step:356/1750 train_time:55781ms step_avg:156.69ms tokens_processed:49152\n",
      "step:357/1750 train_time:55937ms step_avg:156.69ms tokens_processed:49152\n",
      "step:358/1750 train_time:56094ms step_avg:156.69ms tokens_processed:49152\n",
      "step:359/1750 train_time:56251ms step_avg:156.69ms tokens_processed:49152\n",
      "step:360/1750 train_time:56407ms step_avg:156.69ms tokens_processed:49152\n",
      "step:361/1750 train_time:56564ms step_avg:156.69ms tokens_processed:49152\n",
      "step:362/1750 train_time:56720ms step_avg:156.68ms tokens_processed:49152\n",
      "step:363/1750 train_time:56877ms step_avg:156.69ms tokens_processed:49152\n",
      "step:364/1750 train_time:57033ms step_avg:156.68ms tokens_processed:49152\n",
      "step:365/1750 train_time:57192ms step_avg:156.69ms tokens_processed:49152\n",
      "step:366/1750 train_time:57347ms step_avg:156.69ms tokens_processed:49152\n",
      "step:367/1750 train_time:57504ms step_avg:156.69ms tokens_processed:49152\n",
      "step:368/1750 train_time:57660ms step_avg:156.68ms tokens_processed:49152\n",
      "step:369/1750 train_time:57816ms step_avg:156.68ms tokens_processed:49152\n",
      "step:370/1750 train_time:57973ms step_avg:156.68ms tokens_processed:49152\n",
      "step:371/1750 train_time:58131ms step_avg:156.69ms tokens_processed:49152\n",
      "step:372/1750 train_time:58289ms step_avg:156.69ms tokens_processed:49152\n",
      "step:373/1750 train_time:58445ms step_avg:156.69ms tokens_processed:49152\n",
      "step:374/1750 train_time:58601ms step_avg:156.69ms tokens_processed:49152\n",
      "step:375/1750 train_time:58757ms step_avg:156.68ms tokens_processed:49152\n",
      "step:376/1750 train_time:58913ms step_avg:156.68ms tokens_processed:49152\n",
      "step:377/1750 train_time:59071ms step_avg:156.69ms tokens_processed:49152\n",
      "step:378/1750 train_time:59230ms step_avg:156.69ms tokens_processed:49152\n",
      "step:379/1750 train_time:59388ms step_avg:156.70ms tokens_processed:49152\n",
      "step:380/1750 train_time:59543ms step_avg:156.69ms tokens_processed:49152\n",
      "step:381/1750 train_time:59698ms step_avg:156.69ms tokens_processed:49152\n",
      "step:382/1750 train_time:59854ms step_avg:156.69ms tokens_processed:49152\n",
      "step:383/1750 train_time:60011ms step_avg:156.69ms tokens_processed:49152\n",
      "step:384/1750 train_time:60170ms step_avg:156.69ms tokens_processed:49152\n",
      "step:385/1750 train_time:60327ms step_avg:156.69ms tokens_processed:49152\n",
      "step:386/1750 train_time:60484ms step_avg:156.69ms tokens_processed:49152\n",
      "step:387/1750 train_time:60639ms step_avg:156.69ms tokens_processed:49152\n",
      "step:388/1750 train_time:60795ms step_avg:156.69ms tokens_processed:49152\n",
      "step:389/1750 train_time:60950ms step_avg:156.68ms tokens_processed:49152\n",
      "step:390/1750 train_time:61108ms step_avg:156.69ms tokens_processed:49152\n",
      "step:391/1750 train_time:61266ms step_avg:156.69ms tokens_processed:49152\n",
      "step:392/1750 train_time:61424ms step_avg:156.69ms tokens_processed:49152\n",
      "step:393/1750 train_time:61579ms step_avg:156.69ms tokens_processed:49152\n",
      "step:394/1750 train_time:61734ms step_avg:156.69ms tokens_processed:49152\n",
      "step:395/1750 train_time:61891ms step_avg:156.69ms tokens_processed:49152\n",
      "step:396/1750 train_time:62048ms step_avg:156.69ms tokens_processed:49152\n",
      "step:397/1750 train_time:62204ms step_avg:156.69ms tokens_processed:49152\n",
      "step:398/1750 train_time:62362ms step_avg:156.69ms tokens_processed:49152\n",
      "step:399/1750 train_time:62519ms step_avg:156.69ms tokens_processed:49152\n",
      "step:400/1750 train_time:62676ms step_avg:156.69ms tokens_processed:49152\n",
      "step:401/1750 train_time:62833ms step_avg:156.69ms tokens_processed:49152\n",
      "step:402/1750 train_time:62989ms step_avg:156.69ms tokens_processed:49152\n",
      "step:403/1750 train_time:63146ms step_avg:156.69ms tokens_processed:49152\n",
      "step:404/1750 train_time:63303ms step_avg:156.69ms tokens_processed:49152\n",
      "step:405/1750 train_time:63459ms step_avg:156.69ms tokens_processed:49152\n",
      "step:406/1750 train_time:63616ms step_avg:156.69ms tokens_processed:49152\n",
      "step:407/1750 train_time:63773ms step_avg:156.69ms tokens_processed:49152\n",
      "step:408/1750 train_time:63931ms step_avg:156.69ms tokens_processed:49152\n",
      "step:409/1750 train_time:64087ms step_avg:156.69ms tokens_processed:49152\n",
      "step:410/1750 train_time:64244ms step_avg:156.69ms tokens_processed:49152\n",
      "step:411/1750 train_time:64400ms step_avg:156.69ms tokens_processed:49152\n",
      "step:412/1750 train_time:64556ms step_avg:156.69ms tokens_processed:49152\n",
      "step:413/1750 train_time:64713ms step_avg:156.69ms tokens_processed:49152\n",
      "step:414/1750 train_time:64871ms step_avg:156.69ms tokens_processed:49152\n",
      "step:415/1750 train_time:65028ms step_avg:156.69ms tokens_processed:49152\n",
      "step:416/1750 train_time:65184ms step_avg:156.69ms tokens_processed:49152\n",
      "step:417/1750 train_time:65341ms step_avg:156.69ms tokens_processed:49152\n",
      "step:418/1750 train_time:65497ms step_avg:156.69ms tokens_processed:49152\n",
      "step:419/1750 train_time:65653ms step_avg:156.69ms tokens_processed:49152\n",
      "step:420/1750 train_time:65810ms step_avg:156.69ms tokens_processed:49152\n",
      "step:421/1750 train_time:65969ms step_avg:156.70ms tokens_processed:49152\n",
      "step:422/1750 train_time:66127ms step_avg:156.70ms tokens_processed:49152\n",
      "step:423/1750 train_time:66283ms step_avg:156.70ms tokens_processed:49152\n",
      "step:424/1750 train_time:66438ms step_avg:156.69ms tokens_processed:49152\n",
      "step:425/1750 train_time:66595ms step_avg:156.69ms tokens_processed:49152\n",
      "step:426/1750 train_time:66752ms step_avg:156.69ms tokens_processed:49152\n",
      "step:427/1750 train_time:66908ms step_avg:156.69ms tokens_processed:49152\n",
      "step:428/1750 train_time:67067ms step_avg:156.70ms tokens_processed:49152\n",
      "step:429/1750 train_time:67224ms step_avg:156.70ms tokens_processed:49152\n",
      "step:430/1750 train_time:67380ms step_avg:156.70ms tokens_processed:49152\n",
      "step:431/1750 train_time:67536ms step_avg:156.70ms tokens_processed:49152\n",
      "step:432/1750 train_time:67693ms step_avg:156.70ms tokens_processed:49152\n",
      "step:433/1750 train_time:67850ms step_avg:156.70ms tokens_processed:49152\n",
      "step:434/1750 train_time:68007ms step_avg:156.70ms tokens_processed:49152\n",
      "step:435/1750 train_time:68165ms step_avg:156.70ms tokens_processed:49152\n",
      "step:436/1750 train_time:68321ms step_avg:156.70ms tokens_processed:49152\n",
      "step:437/1750 train_time:68478ms step_avg:156.70ms tokens_processed:49152\n",
      "step:438/1750 train_time:68634ms step_avg:156.70ms tokens_processed:49152\n",
      "step:439/1750 train_time:68791ms step_avg:156.70ms tokens_processed:49152\n",
      "step:440/1750 train_time:68948ms step_avg:156.70ms tokens_processed:49152\n",
      "step:441/1750 train_time:69104ms step_avg:156.70ms tokens_processed:49152\n",
      "step:442/1750 train_time:69261ms step_avg:156.70ms tokens_processed:49152\n",
      "step:443/1750 train_time:69417ms step_avg:156.70ms tokens_processed:49152\n",
      "step:444/1750 train_time:69575ms step_avg:156.70ms tokens_processed:49152\n",
      "step:445/1750 train_time:69731ms step_avg:156.70ms tokens_processed:49152\n",
      "step:446/1750 train_time:69888ms step_avg:156.70ms tokens_processed:49152\n",
      "step:447/1750 train_time:70044ms step_avg:156.70ms tokens_processed:49152\n",
      "step:448/1750 train_time:70200ms step_avg:156.70ms tokens_processed:49152\n",
      "step:449/1750 train_time:70357ms step_avg:156.70ms tokens_processed:49152\n",
      "step:450/1750 train_time:70513ms step_avg:156.70ms tokens_processed:49152\n",
      "step:451/1750 train_time:70671ms step_avg:156.70ms tokens_processed:49152\n",
      "step:452/1750 train_time:70829ms step_avg:156.70ms tokens_processed:49152\n",
      "step:453/1750 train_time:70987ms step_avg:156.70ms tokens_processed:49152\n",
      "step:454/1750 train_time:71143ms step_avg:156.70ms tokens_processed:49152\n",
      "step:455/1750 train_time:71299ms step_avg:156.70ms tokens_processed:49152\n",
      "step:456/1750 train_time:71455ms step_avg:156.70ms tokens_processed:49152\n",
      "step:457/1750 train_time:71612ms step_avg:156.70ms tokens_processed:49152\n",
      "step:458/1750 train_time:71769ms step_avg:156.70ms tokens_processed:49152\n",
      "step:459/1750 train_time:71928ms step_avg:156.71ms tokens_processed:49152\n",
      "step:460/1750 train_time:72085ms step_avg:156.71ms tokens_processed:49152\n",
      "step:461/1750 train_time:72240ms step_avg:156.70ms tokens_processed:49152\n",
      "step:462/1750 train_time:72396ms step_avg:156.70ms tokens_processed:49152\n",
      "step:463/1750 train_time:72552ms step_avg:156.70ms tokens_processed:49152\n",
      "step:464/1750 train_time:72709ms step_avg:156.70ms tokens_processed:49152\n",
      "step:465/1750 train_time:72868ms step_avg:156.71ms tokens_processed:49152\n",
      "step:466/1750 train_time:73026ms step_avg:156.71ms tokens_processed:49152\n",
      "step:467/1750 train_time:73181ms step_avg:156.70ms tokens_processed:49152\n",
      "step:468/1750 train_time:73337ms step_avg:156.70ms tokens_processed:49152\n",
      "step:469/1750 train_time:73495ms step_avg:156.70ms tokens_processed:49152\n",
      "step:470/1750 train_time:73650ms step_avg:156.70ms tokens_processed:49152\n",
      "step:471/1750 train_time:73808ms step_avg:156.70ms tokens_processed:49152\n",
      "step:472/1750 train_time:73965ms step_avg:156.71ms tokens_processed:49152\n",
      "step:473/1750 train_time:74121ms step_avg:156.70ms tokens_processed:49152\n",
      "step:474/1750 train_time:74277ms step_avg:156.70ms tokens_processed:49152\n",
      "step:475/1750 train_time:74433ms step_avg:156.70ms tokens_processed:49152\n",
      "step:476/1750 train_time:74590ms step_avg:156.70ms tokens_processed:49152\n",
      "step:477/1750 train_time:74746ms step_avg:156.70ms tokens_processed:49152\n",
      "step:478/1750 train_time:74904ms step_avg:156.70ms tokens_processed:49152\n",
      "step:479/1750 train_time:75060ms step_avg:156.70ms tokens_processed:49152\n",
      "step:480/1750 train_time:75217ms step_avg:156.70ms tokens_processed:49152\n",
      "step:481/1750 train_time:75375ms step_avg:156.70ms tokens_processed:49152\n",
      "step:482/1750 train_time:75532ms step_avg:156.70ms tokens_processed:49152\n",
      "step:483/1750 train_time:75689ms step_avg:156.71ms tokens_processed:49152\n",
      "step:484/1750 train_time:75844ms step_avg:156.70ms tokens_processed:49152\n",
      "step:485/1750 train_time:76001ms step_avg:156.70ms tokens_processed:49152\n",
      "step:486/1750 train_time:76157ms step_avg:156.70ms tokens_processed:49152\n",
      "step:487/1750 train_time:76315ms step_avg:156.70ms tokens_processed:49152\n",
      "step:488/1750 train_time:76473ms step_avg:156.71ms tokens_processed:49152\n",
      "step:489/1750 train_time:76631ms step_avg:156.71ms tokens_processed:49152\n",
      "step:490/1750 train_time:76788ms step_avg:156.71ms tokens_processed:49152\n",
      "step:491/1750 train_time:76944ms step_avg:156.71ms tokens_processed:49152\n",
      "step:492/1750 train_time:77099ms step_avg:156.71ms tokens_processed:49152\n",
      "step:493/1750 train_time:77255ms step_avg:156.70ms tokens_processed:49152\n",
      "step:494/1750 train_time:77413ms step_avg:156.71ms tokens_processed:49152\n",
      "step:495/1750 train_time:77571ms step_avg:156.71ms tokens_processed:49152\n",
      "step:496/1750 train_time:77730ms step_avg:156.71ms tokens_processed:49152\n",
      "step:497/1750 train_time:77887ms step_avg:156.72ms tokens_processed:49152\n",
      "step:498/1750 train_time:78042ms step_avg:156.71ms tokens_processed:49152\n",
      "step:499/1750 train_time:78198ms step_avg:156.71ms tokens_processed:49152\n",
      "step:500/1750 train_time:78354ms step_avg:156.71ms tokens_processed:49152\n",
      "step:501/1750 train_time:78510ms step_avg:156.71ms tokens_processed:49152\n",
      "step:502/1750 train_time:78669ms step_avg:156.71ms tokens_processed:49152\n",
      "step:503/1750 train_time:78829ms step_avg:156.72ms tokens_processed:49152\n",
      "step:504/1750 train_time:78984ms step_avg:156.71ms tokens_processed:49152\n",
      "step:505/1750 train_time:79139ms step_avg:156.71ms tokens_processed:49152\n",
      "step:506/1750 train_time:79296ms step_avg:156.71ms tokens_processed:49152\n",
      "step:507/1750 train_time:79452ms step_avg:156.71ms tokens_processed:49152\n",
      "step:508/1750 train_time:79610ms step_avg:156.71ms tokens_processed:49152\n",
      "step:509/1750 train_time:79769ms step_avg:156.72ms tokens_processed:49152\n",
      "step:510/1750 train_time:79927ms step_avg:156.72ms tokens_processed:49152\n",
      "step:511/1750 train_time:80083ms step_avg:156.72ms tokens_processed:49152\n",
      "step:512/1750 train_time:80238ms step_avg:156.72ms tokens_processed:49152\n",
      "step:513/1750 train_time:80395ms step_avg:156.71ms tokens_processed:49152\n",
      "step:514/1750 train_time:80551ms step_avg:156.71ms tokens_processed:49152\n",
      "step:515/1750 train_time:80709ms step_avg:156.72ms tokens_processed:49152\n",
      "step:516/1750 train_time:80867ms step_avg:156.72ms tokens_processed:49152\n",
      "step:517/1750 train_time:81024ms step_avg:156.72ms tokens_processed:49152\n",
      "step:518/1750 train_time:81179ms step_avg:156.72ms tokens_processed:49152\n",
      "step:519/1750 train_time:81336ms step_avg:156.72ms tokens_processed:49152\n",
      "step:520/1750 train_time:81493ms step_avg:156.72ms tokens_processed:49152\n",
      "step:521/1750 train_time:81650ms step_avg:156.72ms tokens_processed:49152\n",
      "step:522/1750 train_time:81807ms step_avg:156.72ms tokens_processed:49152\n",
      "step:523/1750 train_time:81964ms step_avg:156.72ms tokens_processed:49152\n",
      "step:524/1750 train_time:82120ms step_avg:156.72ms tokens_processed:49152\n",
      "step:525/1750 train_time:82276ms step_avg:156.72ms tokens_processed:49152\n",
      "step:526/1750 train_time:82433ms step_avg:156.72ms tokens_processed:49152\n",
      "step:527/1750 train_time:82590ms step_avg:156.72ms tokens_processed:49152\n",
      "step:528/1750 train_time:82747ms step_avg:156.72ms tokens_processed:49152\n",
      "step:529/1750 train_time:82904ms step_avg:156.72ms tokens_processed:49152\n",
      "step:530/1750 train_time:83061ms step_avg:156.72ms tokens_processed:49152\n",
      "step:531/1750 train_time:83216ms step_avg:156.72ms tokens_processed:49152\n",
      "step:532/1750 train_time:83373ms step_avg:156.72ms tokens_processed:49152\n",
      "step:533/1750 train_time:83531ms step_avg:156.72ms tokens_processed:49152\n",
      "step:534/1750 train_time:83688ms step_avg:156.72ms tokens_processed:49152\n",
      "step:535/1750 train_time:83844ms step_avg:156.72ms tokens_processed:49152\n",
      "step:536/1750 train_time:84000ms step_avg:156.72ms tokens_processed:49152\n",
      "step:537/1750 train_time:84155ms step_avg:156.71ms tokens_processed:49152\n",
      "step:538/1750 train_time:84313ms step_avg:156.71ms tokens_processed:49152\n",
      "step:539/1750 train_time:84471ms step_avg:156.72ms tokens_processed:49152\n",
      "step:540/1750 train_time:84628ms step_avg:156.72ms tokens_processed:49152\n",
      "step:541/1750 train_time:84786ms step_avg:156.72ms tokens_processed:49152\n",
      "step:542/1750 train_time:84941ms step_avg:156.72ms tokens_processed:49152\n",
      "step:543/1750 train_time:85098ms step_avg:156.72ms tokens_processed:49152\n",
      "step:544/1750 train_time:85253ms step_avg:156.71ms tokens_processed:49152\n",
      "step:545/1750 train_time:85410ms step_avg:156.71ms tokens_processed:49152\n",
      "step:546/1750 train_time:85568ms step_avg:156.72ms tokens_processed:49152\n",
      "step:547/1750 train_time:85725ms step_avg:156.72ms tokens_processed:49152\n",
      "step:548/1750 train_time:85881ms step_avg:156.72ms tokens_processed:49152\n",
      "step:549/1750 train_time:86037ms step_avg:156.72ms tokens_processed:49152\n",
      "step:550/1750 train_time:86194ms step_avg:156.72ms tokens_processed:49152\n",
      "step:551/1750 train_time:86350ms step_avg:156.72ms tokens_processed:49152\n",
      "step:552/1750 train_time:86508ms step_avg:156.72ms tokens_processed:49152\n",
      "step:553/1750 train_time:86666ms step_avg:156.72ms tokens_processed:49152\n",
      "step:554/1750 train_time:86822ms step_avg:156.72ms tokens_processed:49152\n",
      "step:555/1750 train_time:86979ms step_avg:156.72ms tokens_processed:49152\n",
      "step:556/1750 train_time:87134ms step_avg:156.72ms tokens_processed:49152\n",
      "step:557/1750 train_time:87292ms step_avg:156.72ms tokens_processed:49152\n",
      "step:558/1750 train_time:87449ms step_avg:156.72ms tokens_processed:49152\n",
      "step:559/1750 train_time:87606ms step_avg:156.72ms tokens_processed:49152\n",
      "step:560/1750 train_time:87762ms step_avg:156.72ms tokens_processed:49152\n",
      "step:561/1750 train_time:87919ms step_avg:156.72ms tokens_processed:49152\n",
      "step:562/1750 train_time:88075ms step_avg:156.72ms tokens_processed:49152\n",
      "step:563/1750 train_time:88232ms step_avg:156.72ms tokens_processed:49152\n",
      "step:564/1750 train_time:88391ms step_avg:156.72ms tokens_processed:49152\n",
      "step:565/1750 train_time:88547ms step_avg:156.72ms tokens_processed:49152\n",
      "step:566/1750 train_time:88703ms step_avg:156.72ms tokens_processed:49152\n",
      "step:567/1750 train_time:88860ms step_avg:156.72ms tokens_processed:49152\n",
      "step:568/1750 train_time:89016ms step_avg:156.72ms tokens_processed:49152\n",
      "step:569/1750 train_time:89174ms step_avg:156.72ms tokens_processed:49152\n",
      "step:570/1750 train_time:89333ms step_avg:156.72ms tokens_processed:49152\n",
      "step:571/1750 train_time:89491ms step_avg:156.73ms tokens_processed:49152\n",
      "step:572/1750 train_time:89647ms step_avg:156.73ms tokens_processed:49152\n",
      "step:573/1750 train_time:89804ms step_avg:156.73ms tokens_processed:49152\n",
      "step:574/1750 train_time:89960ms step_avg:156.72ms tokens_processed:49152\n",
      "step:575/1750 train_time:90116ms step_avg:156.72ms tokens_processed:49152\n",
      "step:576/1750 train_time:90274ms step_avg:156.73ms tokens_processed:49152\n",
      "step:577/1750 train_time:90433ms step_avg:156.73ms tokens_processed:49152\n",
      "step:578/1750 train_time:90590ms step_avg:156.73ms tokens_processed:49152\n",
      "step:579/1750 train_time:90746ms step_avg:156.73ms tokens_processed:49152\n",
      "step:580/1750 train_time:90901ms step_avg:156.73ms tokens_processed:49152\n",
      "step:581/1750 train_time:91058ms step_avg:156.73ms tokens_processed:49152\n",
      "step:582/1750 train_time:91215ms step_avg:156.73ms tokens_processed:49152\n",
      "step:583/1750 train_time:91373ms step_avg:156.73ms tokens_processed:49152\n",
      "step:584/1750 train_time:91532ms step_avg:156.73ms tokens_processed:49152\n",
      "step:585/1750 train_time:91691ms step_avg:156.74ms tokens_processed:49152\n",
      "step:586/1750 train_time:91846ms step_avg:156.73ms tokens_processed:49152\n",
      "step:587/1750 train_time:92001ms step_avg:156.73ms tokens_processed:49152\n",
      "step:588/1750 train_time:92157ms step_avg:156.73ms tokens_processed:49152\n",
      "step:589/1750 train_time:92315ms step_avg:156.73ms tokens_processed:49152\n",
      "step:590/1750 train_time:92472ms step_avg:156.73ms tokens_processed:49152\n",
      "step:591/1750 train_time:92631ms step_avg:156.74ms tokens_processed:49152\n",
      "step:592/1750 train_time:92788ms step_avg:156.74ms tokens_processed:49152\n",
      "step:593/1750 train_time:92943ms step_avg:156.73ms tokens_processed:49152\n",
      "step:594/1750 train_time:93099ms step_avg:156.73ms tokens_processed:49152\n",
      "step:595/1750 train_time:93254ms step_avg:156.73ms tokens_processed:49152\n",
      "step:596/1750 train_time:93411ms step_avg:156.73ms tokens_processed:49152\n",
      "step:597/1750 train_time:93570ms step_avg:156.73ms tokens_processed:49152\n",
      "step:598/1750 train_time:93729ms step_avg:156.74ms tokens_processed:49152\n",
      "step:599/1750 train_time:93885ms step_avg:156.74ms tokens_processed:49152\n",
      "step:600/1750 train_time:94040ms step_avg:156.73ms tokens_processed:49152\n",
      "step:601/1750 train_time:94197ms step_avg:156.73ms tokens_processed:49152\n",
      "step:602/1750 train_time:94352ms step_avg:156.73ms tokens_processed:49152\n",
      "step:603/1750 train_time:94509ms step_avg:156.73ms tokens_processed:49152\n",
      "step:604/1750 train_time:94668ms step_avg:156.74ms tokens_processed:49152\n",
      "step:605/1750 train_time:94828ms step_avg:156.74ms tokens_processed:49152\n",
      "step:606/1750 train_time:94984ms step_avg:156.74ms tokens_processed:49152\n",
      "step:607/1750 train_time:95139ms step_avg:156.74ms tokens_processed:49152\n",
      "step:608/1750 train_time:95295ms step_avg:156.73ms tokens_processed:49152\n",
      "step:609/1750 train_time:95451ms step_avg:156.73ms tokens_processed:49152\n",
      "step:610/1750 train_time:95608ms step_avg:156.73ms tokens_processed:49152\n",
      "step:611/1750 train_time:95767ms step_avg:156.74ms tokens_processed:49152\n",
      "step:612/1750 train_time:95924ms step_avg:156.74ms tokens_processed:49152\n",
      "step:613/1750 train_time:96079ms step_avg:156.74ms tokens_processed:49152\n",
      "step:614/1750 train_time:96235ms step_avg:156.73ms tokens_processed:49152\n",
      "step:615/1750 train_time:96392ms step_avg:156.73ms tokens_processed:49152\n",
      "step:616/1750 train_time:96549ms step_avg:156.74ms tokens_processed:49152\n",
      "step:617/1750 train_time:96706ms step_avg:156.74ms tokens_processed:49152\n",
      "step:618/1750 train_time:96863ms step_avg:156.74ms tokens_processed:49152\n",
      "step:619/1750 train_time:97018ms step_avg:156.73ms tokens_processed:49152\n",
      "step:620/1750 train_time:97175ms step_avg:156.73ms tokens_processed:49152\n",
      "step:621/1750 train_time:97333ms step_avg:156.74ms tokens_processed:49152\n",
      "step:622/1750 train_time:97490ms step_avg:156.74ms tokens_processed:49152\n",
      "step:623/1750 train_time:97648ms step_avg:156.74ms tokens_processed:49152\n",
      "step:624/1750 train_time:97804ms step_avg:156.74ms tokens_processed:49152\n",
      "step:625/1750 train_time:97960ms step_avg:156.74ms tokens_processed:49152\n",
      "step:626/1750 train_time:98116ms step_avg:156.73ms tokens_processed:49152\n",
      "step:627/1750 train_time:98273ms step_avg:156.74ms tokens_processed:49152\n",
      "step:628/1750 train_time:98431ms step_avg:156.74ms tokens_processed:49152\n",
      "step:629/1750 train_time:98588ms step_avg:156.74ms tokens_processed:49152\n",
      "step:630/1750 train_time:98743ms step_avg:156.74ms tokens_processed:49152\n",
      "step:631/1750 train_time:98899ms step_avg:156.73ms tokens_processed:49152\n",
      "step:632/1750 train_time:99055ms step_avg:156.73ms tokens_processed:49152\n",
      "step:633/1750 train_time:99211ms step_avg:156.73ms tokens_processed:49152\n",
      "step:634/1750 train_time:99370ms step_avg:156.73ms tokens_processed:49152\n",
      "step:635/1750 train_time:99528ms step_avg:156.74ms tokens_processed:49152\n",
      "step:636/1750 train_time:99684ms step_avg:156.74ms tokens_processed:49152\n",
      "step:637/1750 train_time:99839ms step_avg:156.73ms tokens_processed:49152\n",
      "step:638/1750 train_time:99996ms step_avg:156.73ms tokens_processed:49152\n",
      "step:639/1750 train_time:100151ms step_avg:156.73ms tokens_processed:49152\n",
      "step:640/1750 train_time:100309ms step_avg:156.73ms tokens_processed:49152\n",
      "step:641/1750 train_time:100468ms step_avg:156.74ms tokens_processed:49152\n",
      "step:642/1750 train_time:100626ms step_avg:156.74ms tokens_processed:49152\n",
      "step:643/1750 train_time:100782ms step_avg:156.74ms tokens_processed:49152\n",
      "step:644/1750 train_time:100937ms step_avg:156.73ms tokens_processed:49152\n",
      "step:645/1750 train_time:101094ms step_avg:156.74ms tokens_processed:49152\n",
      "step:646/1750 train_time:101250ms step_avg:156.73ms tokens_processed:49152\n",
      "step:647/1750 train_time:101408ms step_avg:156.74ms tokens_processed:49152\n",
      "step:648/1750 train_time:101565ms step_avg:156.74ms tokens_processed:49152\n",
      "step:649/1750 train_time:101722ms step_avg:156.74ms tokens_processed:49152\n",
      "step:650/1750 train_time:101878ms step_avg:156.74ms tokens_processed:49152\n",
      "step:651/1750 train_time:102034ms step_avg:156.73ms tokens_processed:49152\n",
      "step:652/1750 train_time:102191ms step_avg:156.73ms tokens_processed:49152\n",
      "step:653/1750 train_time:102348ms step_avg:156.73ms tokens_processed:49152\n",
      "step:654/1750 train_time:102505ms step_avg:156.74ms tokens_processed:49152\n",
      "step:655/1750 train_time:102662ms step_avg:156.74ms tokens_processed:49152\n",
      "step:656/1750 train_time:102818ms step_avg:156.73ms tokens_processed:49152\n",
      "step:657/1750 train_time:102975ms step_avg:156.73ms tokens_processed:49152\n",
      "step:658/1750 train_time:103132ms step_avg:156.74ms tokens_processed:49152\n",
      "step:659/1750 train_time:103290ms step_avg:156.74ms tokens_processed:49152\n",
      "step:660/1750 train_time:103445ms step_avg:156.74ms tokens_processed:49152\n",
      "step:661/1750 train_time:103602ms step_avg:156.73ms tokens_processed:49152\n",
      "step:662/1750 train_time:103758ms step_avg:156.73ms tokens_processed:49152\n",
      "step:663/1750 train_time:103915ms step_avg:156.73ms tokens_processed:49152\n",
      "step:664/1750 train_time:104072ms step_avg:156.74ms tokens_processed:49152\n",
      "step:665/1750 train_time:104230ms step_avg:156.74ms tokens_processed:49152\n",
      "step:666/1750 train_time:104387ms step_avg:156.74ms tokens_processed:49152\n",
      "step:667/1750 train_time:104542ms step_avg:156.73ms tokens_processed:49152\n",
      "step:668/1750 train_time:104699ms step_avg:156.74ms tokens_processed:49152\n",
      "step:669/1750 train_time:104855ms step_avg:156.73ms tokens_processed:49152\n",
      "step:670/1750 train_time:105012ms step_avg:156.73ms tokens_processed:49152\n",
      "step:671/1750 train_time:105170ms step_avg:156.74ms tokens_processed:49152\n",
      "step:672/1750 train_time:105327ms step_avg:156.74ms tokens_processed:49152\n",
      "step:673/1750 train_time:105483ms step_avg:156.74ms tokens_processed:49152\n",
      "step:674/1750 train_time:105639ms step_avg:156.73ms tokens_processed:49152\n",
      "step:675/1750 train_time:105796ms step_avg:156.73ms tokens_processed:49152\n",
      "step:676/1750 train_time:105952ms step_avg:156.73ms tokens_processed:49152\n",
      "step:677/1750 train_time:106110ms step_avg:156.74ms tokens_processed:49152\n",
      "step:678/1750 train_time:106267ms step_avg:156.74ms tokens_processed:49152\n",
      "step:679/1750 train_time:106423ms step_avg:156.74ms tokens_processed:49152\n",
      "step:680/1750 train_time:106580ms step_avg:156.73ms tokens_processed:49152\n",
      "step:681/1750 train_time:106736ms step_avg:156.73ms tokens_processed:49152\n",
      "step:682/1750 train_time:106892ms step_avg:156.73ms tokens_processed:49152\n",
      "step:683/1750 train_time:107048ms step_avg:156.73ms tokens_processed:49152\n",
      "step:684/1750 train_time:107205ms step_avg:156.73ms tokens_processed:49152\n",
      "step:685/1750 train_time:107362ms step_avg:156.73ms tokens_processed:49152\n",
      "step:686/1750 train_time:107519ms step_avg:156.73ms tokens_processed:49152\n",
      "step:687/1750 train_time:107676ms step_avg:156.73ms tokens_processed:49152\n",
      "step:688/1750 train_time:107833ms step_avg:156.73ms tokens_processed:49152\n",
      "step:689/1750 train_time:107990ms step_avg:156.73ms tokens_processed:49152\n",
      "step:690/1750 train_time:108146ms step_avg:156.73ms tokens_processed:49152\n",
      "step:691/1750 train_time:108303ms step_avg:156.73ms tokens_processed:49152\n",
      "step:692/1750 train_time:108459ms step_avg:156.73ms tokens_processed:49152\n",
      "step:693/1750 train_time:108615ms step_avg:156.73ms tokens_processed:49152\n",
      "step:694/1750 train_time:108773ms step_avg:156.73ms tokens_processed:49152\n",
      "step:695/1750 train_time:108931ms step_avg:156.74ms tokens_processed:49152\n",
      "step:696/1750 train_time:109088ms step_avg:156.74ms tokens_processed:49152\n",
      "step:697/1750 train_time:109244ms step_avg:156.73ms tokens_processed:49152\n",
      "step:698/1750 train_time:109400ms step_avg:156.73ms tokens_processed:49152\n",
      "step:699/1750 train_time:109556ms step_avg:156.73ms tokens_processed:49152\n",
      "step:700/1750 train_time:109713ms step_avg:156.73ms tokens_processed:49152\n",
      "---validation---\n",
      "step:700/1750 val_loss:4.3548 train_time:109721ms step_avg:156.74ms\n",
      "---end of validation---\n",
      "step:701/1750 train_time:109871ms step_avg:156.73ms tokens_processed:49152\n",
      "step:702/1750 train_time:110027ms step_avg:156.73ms tokens_processed:49152\n",
      "step:703/1750 train_time:110184ms step_avg:156.73ms tokens_processed:49152\n",
      "step:704/1750 train_time:110339ms step_avg:156.73ms tokens_processed:49152\n",
      "step:705/1750 train_time:110495ms step_avg:156.73ms tokens_processed:49152\n",
      "step:706/1750 train_time:110653ms step_avg:156.73ms tokens_processed:49152\n",
      "step:707/1750 train_time:110811ms step_avg:156.73ms tokens_processed:49152\n",
      "step:708/1750 train_time:110967ms step_avg:156.73ms tokens_processed:49152\n",
      "step:709/1750 train_time:111125ms step_avg:156.73ms tokens_processed:49152\n",
      "step:710/1750 train_time:111282ms step_avg:156.73ms tokens_processed:49152\n",
      "step:711/1750 train_time:111437ms step_avg:156.73ms tokens_processed:49152\n",
      "step:712/1750 train_time:111594ms step_avg:156.73ms tokens_processed:49152\n",
      "step:713/1750 train_time:111752ms step_avg:156.73ms tokens_processed:49152\n",
      "step:714/1750 train_time:111909ms step_avg:156.73ms tokens_processed:49152\n",
      "step:715/1750 train_time:112066ms step_avg:156.74ms tokens_processed:49152\n",
      "step:716/1750 train_time:112224ms step_avg:156.74ms tokens_processed:49152\n",
      "step:717/1750 train_time:112379ms step_avg:156.74ms tokens_processed:49152\n",
      "step:718/1750 train_time:112535ms step_avg:156.73ms tokens_processed:49152\n",
      "step:719/1750 train_time:112693ms step_avg:156.74ms tokens_processed:49152\n",
      "step:720/1750 train_time:112850ms step_avg:156.74ms tokens_processed:49152\n",
      "step:721/1750 train_time:113007ms step_avg:156.74ms tokens_processed:49152\n",
      "step:722/1750 train_time:113164ms step_avg:156.74ms tokens_processed:49152\n",
      "step:723/1750 train_time:113321ms step_avg:156.74ms tokens_processed:49152\n",
      "step:724/1750 train_time:113476ms step_avg:156.73ms tokens_processed:49152\n",
      "step:725/1750 train_time:113634ms step_avg:156.74ms tokens_processed:49152\n",
      "step:726/1750 train_time:113791ms step_avg:156.74ms tokens_processed:49152\n",
      "step:727/1750 train_time:113948ms step_avg:156.74ms tokens_processed:49152\n",
      "step:728/1750 train_time:114105ms step_avg:156.74ms tokens_processed:49152\n",
      "step:729/1750 train_time:114261ms step_avg:156.74ms tokens_processed:49152\n",
      "step:730/1750 train_time:114417ms step_avg:156.74ms tokens_processed:49152\n",
      "step:731/1750 train_time:114574ms step_avg:156.74ms tokens_processed:49152\n",
      "step:732/1750 train_time:114733ms step_avg:156.74ms tokens_processed:49152\n",
      "step:733/1750 train_time:114889ms step_avg:156.74ms tokens_processed:49152\n",
      "step:734/1750 train_time:115047ms step_avg:156.74ms tokens_processed:49152\n",
      "step:735/1750 train_time:115203ms step_avg:156.74ms tokens_processed:49152\n",
      "step:736/1750 train_time:115358ms step_avg:156.74ms tokens_processed:49152\n",
      "step:737/1750 train_time:115514ms step_avg:156.73ms tokens_processed:49152\n",
      "step:738/1750 train_time:115671ms step_avg:156.74ms tokens_processed:49152\n",
      "step:739/1750 train_time:115829ms step_avg:156.74ms tokens_processed:49152\n",
      "step:740/1750 train_time:115987ms step_avg:156.74ms tokens_processed:49152\n",
      "step:741/1750 train_time:116146ms step_avg:156.74ms tokens_processed:49152\n",
      "step:742/1750 train_time:116300ms step_avg:156.74ms tokens_processed:49152\n",
      "step:743/1750 train_time:116456ms step_avg:156.74ms tokens_processed:49152\n",
      "step:744/1750 train_time:116612ms step_avg:156.74ms tokens_processed:49152\n",
      "step:745/1750 train_time:116769ms step_avg:156.74ms tokens_processed:49152\n",
      "step:746/1750 train_time:116928ms step_avg:156.74ms tokens_processed:49152\n",
      "step:747/1750 train_time:117086ms step_avg:156.74ms tokens_processed:49152\n",
      "step:748/1750 train_time:117243ms step_avg:156.74ms tokens_processed:49152\n",
      "step:749/1750 train_time:117398ms step_avg:156.74ms tokens_processed:49152\n",
      "step:750/1750 train_time:117555ms step_avg:156.74ms tokens_processed:49152\n",
      "step:751/1750 train_time:117711ms step_avg:156.74ms tokens_processed:49152\n",
      "step:752/1750 train_time:117868ms step_avg:156.74ms tokens_processed:49152\n",
      "step:753/1750 train_time:118027ms step_avg:156.74ms tokens_processed:49152\n",
      "step:754/1750 train_time:118185ms step_avg:156.74ms tokens_processed:49152\n",
      "step:755/1750 train_time:118341ms step_avg:156.74ms tokens_processed:49152\n",
      "step:756/1750 train_time:118495ms step_avg:156.74ms tokens_processed:49152\n",
      "step:757/1750 train_time:118653ms step_avg:156.74ms tokens_processed:49152\n",
      "step:758/1750 train_time:118808ms step_avg:156.74ms tokens_processed:49152\n",
      "step:759/1750 train_time:118967ms step_avg:156.74ms tokens_processed:49152\n",
      "step:760/1750 train_time:119125ms step_avg:156.74ms tokens_processed:49152\n",
      "step:761/1750 train_time:119283ms step_avg:156.74ms tokens_processed:49152\n",
      "step:762/1750 train_time:119439ms step_avg:156.74ms tokens_processed:49152\n",
      "step:763/1750 train_time:119594ms step_avg:156.74ms tokens_processed:49152\n",
      "step:764/1750 train_time:119752ms step_avg:156.74ms tokens_processed:49152\n",
      "step:765/1750 train_time:119908ms step_avg:156.74ms tokens_processed:49152\n",
      "step:766/1750 train_time:120067ms step_avg:156.74ms tokens_processed:49152\n",
      "step:767/1750 train_time:120224ms step_avg:156.75ms tokens_processed:49152\n",
      "step:768/1750 train_time:120381ms step_avg:156.75ms tokens_processed:49152\n",
      "step:769/1750 train_time:120537ms step_avg:156.75ms tokens_processed:49152\n",
      "step:770/1750 train_time:120694ms step_avg:156.75ms tokens_processed:49152\n",
      "step:771/1750 train_time:120852ms step_avg:156.75ms tokens_processed:49152\n",
      "step:772/1750 train_time:121008ms step_avg:156.75ms tokens_processed:49152\n",
      "step:773/1750 train_time:121166ms step_avg:156.75ms tokens_processed:49152\n",
      "step:774/1750 train_time:121323ms step_avg:156.75ms tokens_processed:49152\n",
      "step:775/1750 train_time:121480ms step_avg:156.75ms tokens_processed:49152\n",
      "step:776/1750 train_time:121636ms step_avg:156.75ms tokens_processed:49152\n",
      "step:777/1750 train_time:121792ms step_avg:156.75ms tokens_processed:49152\n",
      "step:778/1750 train_time:121949ms step_avg:156.75ms tokens_processed:49152\n",
      "step:779/1750 train_time:122105ms step_avg:156.75ms tokens_processed:49152\n",
      "step:780/1750 train_time:122262ms step_avg:156.75ms tokens_processed:49152\n",
      "step:781/1750 train_time:122419ms step_avg:156.75ms tokens_processed:49152\n",
      "step:782/1750 train_time:122575ms step_avg:156.75ms tokens_processed:49152\n",
      "step:783/1750 train_time:122733ms step_avg:156.75ms tokens_processed:49152\n",
      "step:784/1750 train_time:122891ms step_avg:156.75ms tokens_processed:49152\n",
      "step:785/1750 train_time:123048ms step_avg:156.75ms tokens_processed:49152\n",
      "step:786/1750 train_time:123205ms step_avg:156.75ms tokens_processed:49152\n",
      "step:787/1750 train_time:123361ms step_avg:156.75ms tokens_processed:49152\n",
      "step:788/1750 train_time:123518ms step_avg:156.75ms tokens_processed:49152\n",
      "step:789/1750 train_time:123675ms step_avg:156.75ms tokens_processed:49152\n",
      "step:790/1750 train_time:123832ms step_avg:156.75ms tokens_processed:49152\n",
      "step:791/1750 train_time:123989ms step_avg:156.75ms tokens_processed:49152\n",
      "step:792/1750 train_time:124147ms step_avg:156.75ms tokens_processed:49152\n",
      "step:793/1750 train_time:124303ms step_avg:156.75ms tokens_processed:49152\n",
      "step:794/1750 train_time:124459ms step_avg:156.75ms tokens_processed:49152\n",
      "step:795/1750 train_time:124615ms step_avg:156.75ms tokens_processed:49152\n",
      "step:796/1750 train_time:124771ms step_avg:156.75ms tokens_processed:49152\n",
      "step:797/1750 train_time:124929ms step_avg:156.75ms tokens_processed:49152\n",
      "step:798/1750 train_time:125088ms step_avg:156.75ms tokens_processed:49152\n",
      "step:799/1750 train_time:125245ms step_avg:156.75ms tokens_processed:49152\n",
      "step:800/1750 train_time:125401ms step_avg:156.75ms tokens_processed:49152\n",
      "step:801/1750 train_time:125557ms step_avg:156.75ms tokens_processed:49152\n",
      "step:802/1750 train_time:125713ms step_avg:156.75ms tokens_processed:49152\n",
      "step:803/1750 train_time:125869ms step_avg:156.75ms tokens_processed:49152\n",
      "step:804/1750 train_time:126027ms step_avg:156.75ms tokens_processed:49152\n",
      "step:805/1750 train_time:126187ms step_avg:156.75ms tokens_processed:49152\n",
      "step:806/1750 train_time:126343ms step_avg:156.75ms tokens_processed:49152\n",
      "step:807/1750 train_time:126498ms step_avg:156.75ms tokens_processed:49152\n",
      "step:808/1750 train_time:126654ms step_avg:156.75ms tokens_processed:49152\n",
      "step:809/1750 train_time:126812ms step_avg:156.75ms tokens_processed:49152\n",
      "step:810/1750 train_time:126968ms step_avg:156.75ms tokens_processed:49152\n",
      "step:811/1750 train_time:127126ms step_avg:156.75ms tokens_processed:49152\n",
      "step:812/1750 train_time:127284ms step_avg:156.75ms tokens_processed:49152\n",
      "step:813/1750 train_time:127440ms step_avg:156.75ms tokens_processed:49152\n",
      "step:814/1750 train_time:127595ms step_avg:156.75ms tokens_processed:49152\n",
      "step:815/1750 train_time:127752ms step_avg:156.75ms tokens_processed:49152\n",
      "step:816/1750 train_time:127908ms step_avg:156.75ms tokens_processed:49152\n",
      "step:817/1750 train_time:128066ms step_avg:156.75ms tokens_processed:49152\n",
      "step:818/1750 train_time:128224ms step_avg:156.75ms tokens_processed:49152\n",
      "step:819/1750 train_time:128379ms step_avg:156.75ms tokens_processed:49152\n",
      "step:820/1750 train_time:128536ms step_avg:156.75ms tokens_processed:49152\n",
      "step:821/1750 train_time:128693ms step_avg:156.75ms tokens_processed:49152\n",
      "step:822/1750 train_time:128850ms step_avg:156.75ms tokens_processed:49152\n",
      "step:823/1750 train_time:129006ms step_avg:156.75ms tokens_processed:49152\n",
      "step:824/1750 train_time:129163ms step_avg:156.75ms tokens_processed:49152\n",
      "step:825/1750 train_time:129320ms step_avg:156.75ms tokens_processed:49152\n",
      "step:826/1750 train_time:129476ms step_avg:156.75ms tokens_processed:49152\n",
      "step:827/1750 train_time:129634ms step_avg:156.75ms tokens_processed:49152\n",
      "step:828/1750 train_time:129791ms step_avg:156.75ms tokens_processed:49152\n",
      "step:829/1750 train_time:129948ms step_avg:156.75ms tokens_processed:49152\n",
      "step:830/1750 train_time:130105ms step_avg:156.75ms tokens_processed:49152\n",
      "step:831/1750 train_time:130262ms step_avg:156.75ms tokens_processed:49152\n",
      "step:832/1750 train_time:130417ms step_avg:156.75ms tokens_processed:49152\n",
      "step:833/1750 train_time:130574ms step_avg:156.75ms tokens_processed:49152\n",
      "step:834/1750 train_time:130732ms step_avg:156.75ms tokens_processed:49152\n",
      "step:835/1750 train_time:130890ms step_avg:156.75ms tokens_processed:49152\n",
      "step:836/1750 train_time:131048ms step_avg:156.76ms tokens_processed:49152\n",
      "step:837/1750 train_time:131204ms step_avg:156.76ms tokens_processed:49152\n",
      "step:838/1750 train_time:131360ms step_avg:156.75ms tokens_processed:49152\n",
      "step:839/1750 train_time:131516ms step_avg:156.75ms tokens_processed:49152\n",
      "step:840/1750 train_time:131673ms step_avg:156.75ms tokens_processed:49152\n",
      "step:841/1750 train_time:131831ms step_avg:156.75ms tokens_processed:49152\n",
      "step:842/1750 train_time:131988ms step_avg:156.76ms tokens_processed:49152\n",
      "step:843/1750 train_time:132147ms step_avg:156.76ms tokens_processed:49152\n",
      "step:844/1750 train_time:132302ms step_avg:156.76ms tokens_processed:49152\n",
      "step:845/1750 train_time:132458ms step_avg:156.75ms tokens_processed:49152\n",
      "step:846/1750 train_time:132614ms step_avg:156.75ms tokens_processed:49152\n",
      "step:847/1750 train_time:132771ms step_avg:156.75ms tokens_processed:49152\n",
      "step:848/1750 train_time:132929ms step_avg:156.76ms tokens_processed:49152\n",
      "step:849/1750 train_time:133087ms step_avg:156.76ms tokens_processed:49152\n",
      "step:850/1750 train_time:133245ms step_avg:156.76ms tokens_processed:49152\n",
      "step:851/1750 train_time:133401ms step_avg:156.76ms tokens_processed:49152\n",
      "step:852/1750 train_time:133557ms step_avg:156.76ms tokens_processed:49152\n",
      "step:853/1750 train_time:133712ms step_avg:156.75ms tokens_processed:49152\n",
      "step:854/1750 train_time:133868ms step_avg:156.75ms tokens_processed:49152\n",
      "step:855/1750 train_time:134027ms step_avg:156.76ms tokens_processed:49152\n",
      "step:856/1750 train_time:134186ms step_avg:156.76ms tokens_processed:49152\n",
      "step:857/1750 train_time:134342ms step_avg:156.76ms tokens_processed:49152\n",
      "step:858/1750 train_time:134498ms step_avg:156.76ms tokens_processed:49152\n",
      "step:859/1750 train_time:134656ms step_avg:156.76ms tokens_processed:49152\n",
      "step:860/1750 train_time:134812ms step_avg:156.76ms tokens_processed:49152\n",
      "step:861/1750 train_time:134969ms step_avg:156.76ms tokens_processed:49152\n",
      "step:862/1750 train_time:135127ms step_avg:156.76ms tokens_processed:49152\n",
      "step:863/1750 train_time:135285ms step_avg:156.76ms tokens_processed:49152\n",
      "step:864/1750 train_time:135441ms step_avg:156.76ms tokens_processed:49152\n",
      "step:865/1750 train_time:135597ms step_avg:156.76ms tokens_processed:49152\n",
      "step:866/1750 train_time:135753ms step_avg:156.76ms tokens_processed:49152\n",
      "step:867/1750 train_time:135909ms step_avg:156.76ms tokens_processed:49152\n",
      "step:868/1750 train_time:136067ms step_avg:156.76ms tokens_processed:49152\n",
      "step:869/1750 train_time:136225ms step_avg:156.76ms tokens_processed:49152\n",
      "step:870/1750 train_time:136383ms step_avg:156.76ms tokens_processed:49152\n",
      "step:871/1750 train_time:136540ms step_avg:156.76ms tokens_processed:49152\n",
      "step:872/1750 train_time:136695ms step_avg:156.76ms tokens_processed:49152\n",
      "step:873/1750 train_time:136853ms step_avg:156.76ms tokens_processed:49152\n",
      "step:874/1750 train_time:137009ms step_avg:156.76ms tokens_processed:49152\n",
      "step:875/1750 train_time:137166ms step_avg:156.76ms tokens_processed:49152\n",
      "step:876/1750 train_time:137323ms step_avg:156.76ms tokens_processed:49152\n",
      "step:877/1750 train_time:137481ms step_avg:156.76ms tokens_processed:49152\n",
      "step:878/1750 train_time:137636ms step_avg:156.76ms tokens_processed:49152\n",
      "step:879/1750 train_time:137793ms step_avg:156.76ms tokens_processed:49152\n",
      "step:880/1750 train_time:137950ms step_avg:156.76ms tokens_processed:49152\n",
      "step:881/1750 train_time:138107ms step_avg:156.76ms tokens_processed:49152\n",
      "step:882/1750 train_time:138264ms step_avg:156.76ms tokens_processed:49152\n",
      "step:883/1750 train_time:138420ms step_avg:156.76ms tokens_processed:49152\n",
      "step:884/1750 train_time:138577ms step_avg:156.76ms tokens_processed:49152\n",
      "step:885/1750 train_time:138734ms step_avg:156.76ms tokens_processed:49152\n",
      "step:886/1750 train_time:138892ms step_avg:156.76ms tokens_processed:49152\n",
      "step:887/1750 train_time:139048ms step_avg:156.76ms tokens_processed:49152\n",
      "step:888/1750 train_time:139205ms step_avg:156.76ms tokens_processed:49152\n",
      "step:889/1750 train_time:139361ms step_avg:156.76ms tokens_processed:49152\n",
      "step:890/1750 train_time:139518ms step_avg:156.76ms tokens_processed:49152\n",
      "step:891/1750 train_time:139674ms step_avg:156.76ms tokens_processed:49152\n",
      "step:892/1750 train_time:139832ms step_avg:156.76ms tokens_processed:49152\n",
      "step:893/1750 train_time:139989ms step_avg:156.76ms tokens_processed:49152\n",
      "step:894/1750 train_time:140147ms step_avg:156.76ms tokens_processed:49152\n",
      "step:895/1750 train_time:140303ms step_avg:156.76ms tokens_processed:49152\n",
      "step:896/1750 train_time:140459ms step_avg:156.76ms tokens_processed:49152\n",
      "step:897/1750 train_time:140614ms step_avg:156.76ms tokens_processed:49152\n",
      "step:898/1750 train_time:140772ms step_avg:156.76ms tokens_processed:49152\n",
      "step:899/1750 train_time:140929ms step_avg:156.76ms tokens_processed:49152\n",
      "step:900/1750 train_time:141088ms step_avg:156.76ms tokens_processed:49152\n",
      "step:901/1750 train_time:141245ms step_avg:156.76ms tokens_processed:49152\n",
      "step:902/1750 train_time:141401ms step_avg:156.76ms tokens_processed:49152\n",
      "step:903/1750 train_time:141557ms step_avg:156.76ms tokens_processed:49152\n",
      "step:904/1750 train_time:141713ms step_avg:156.76ms tokens_processed:49152\n",
      "step:905/1750 train_time:141870ms step_avg:156.76ms tokens_processed:49152\n",
      "step:906/1750 train_time:142029ms step_avg:156.76ms tokens_processed:49152\n",
      "step:907/1750 train_time:142187ms step_avg:156.77ms tokens_processed:49152\n",
      "step:908/1750 train_time:142344ms step_avg:156.77ms tokens_processed:49152\n",
      "step:909/1750 train_time:142499ms step_avg:156.76ms tokens_processed:49152\n",
      "step:910/1750 train_time:142655ms step_avg:156.76ms tokens_processed:49152\n",
      "step:911/1750 train_time:142812ms step_avg:156.76ms tokens_processed:49152\n",
      "step:912/1750 train_time:142969ms step_avg:156.76ms tokens_processed:49152\n",
      "step:913/1750 train_time:143128ms step_avg:156.77ms tokens_processed:49152\n",
      "step:914/1750 train_time:143286ms step_avg:156.77ms tokens_processed:49152\n",
      "step:915/1750 train_time:143443ms step_avg:156.77ms tokens_processed:49152\n",
      "step:916/1750 train_time:143598ms step_avg:156.77ms tokens_processed:49152\n",
      "step:917/1750 train_time:143755ms step_avg:156.77ms tokens_processed:49152\n",
      "step:918/1750 train_time:143911ms step_avg:156.77ms tokens_processed:49152\n",
      "step:919/1750 train_time:144068ms step_avg:156.77ms tokens_processed:49152\n",
      "step:920/1750 train_time:144228ms step_avg:156.77ms tokens_processed:49152\n",
      "step:921/1750 train_time:144385ms step_avg:156.77ms tokens_processed:49152\n",
      "step:922/1750 train_time:144542ms step_avg:156.77ms tokens_processed:49152\n",
      "step:923/1750 train_time:144698ms step_avg:156.77ms tokens_processed:49152\n",
      "step:924/1750 train_time:144855ms step_avg:156.77ms tokens_processed:49152\n",
      "step:925/1750 train_time:145011ms step_avg:156.77ms tokens_processed:49152\n",
      "step:926/1750 train_time:145168ms step_avg:156.77ms tokens_processed:49152\n",
      "step:927/1750 train_time:145329ms step_avg:156.77ms tokens_processed:49152\n",
      "step:928/1750 train_time:145486ms step_avg:156.77ms tokens_processed:49152\n",
      "step:929/1750 train_time:145642ms step_avg:156.77ms tokens_processed:49152\n",
      "step:930/1750 train_time:145797ms step_avg:156.77ms tokens_processed:49152\n",
      "step:931/1750 train_time:145954ms step_avg:156.77ms tokens_processed:49152\n",
      "step:932/1750 train_time:146111ms step_avg:156.77ms tokens_processed:49152\n",
      "step:933/1750 train_time:146268ms step_avg:156.77ms tokens_processed:49152\n",
      "step:934/1750 train_time:146427ms step_avg:156.77ms tokens_processed:49152\n",
      "step:935/1750 train_time:146585ms step_avg:156.78ms tokens_processed:49152\n",
      "step:936/1750 train_time:146743ms step_avg:156.78ms tokens_processed:49152\n",
      "step:937/1750 train_time:146897ms step_avg:156.77ms tokens_processed:49152\n",
      "step:938/1750 train_time:147054ms step_avg:156.77ms tokens_processed:49152\n",
      "step:939/1750 train_time:147210ms step_avg:156.77ms tokens_processed:49152\n",
      "step:940/1750 train_time:147368ms step_avg:156.77ms tokens_processed:49152\n",
      "step:941/1750 train_time:147526ms step_avg:156.78ms tokens_processed:49152\n",
      "step:942/1750 train_time:147683ms step_avg:156.78ms tokens_processed:49152\n",
      "step:943/1750 train_time:147839ms step_avg:156.78ms tokens_processed:49152\n",
      "step:944/1750 train_time:147995ms step_avg:156.77ms tokens_processed:49152\n",
      "step:945/1750 train_time:148152ms step_avg:156.77ms tokens_processed:49152\n",
      "step:946/1750 train_time:148308ms step_avg:156.77ms tokens_processed:49152\n",
      "step:947/1750 train_time:148466ms step_avg:156.77ms tokens_processed:49152\n",
      "step:948/1750 train_time:148623ms step_avg:156.78ms tokens_processed:49152\n",
      "step:949/1750 train_time:148780ms step_avg:156.78ms tokens_processed:49152\n",
      "step:950/1750 train_time:148936ms step_avg:156.77ms tokens_processed:49152\n",
      "step:951/1750 train_time:149093ms step_avg:156.77ms tokens_processed:49152\n",
      "step:952/1750 train_time:149250ms step_avg:156.77ms tokens_processed:49152\n",
      "step:953/1750 train_time:149407ms step_avg:156.78ms tokens_processed:49152\n",
      "step:954/1750 train_time:149565ms step_avg:156.78ms tokens_processed:49152\n",
      "step:955/1750 train_time:149721ms step_avg:156.78ms tokens_processed:49152\n",
      "step:956/1750 train_time:149877ms step_avg:156.78ms tokens_processed:49152\n",
      "step:957/1750 train_time:150034ms step_avg:156.78ms tokens_processed:49152\n",
      "step:958/1750 train_time:150192ms step_avg:156.78ms tokens_processed:49152\n",
      "step:959/1750 train_time:150349ms step_avg:156.78ms tokens_processed:49152\n",
      "step:960/1750 train_time:150506ms step_avg:156.78ms tokens_processed:49152\n",
      "step:961/1750 train_time:150664ms step_avg:156.78ms tokens_processed:49152\n",
      "step:962/1750 train_time:150820ms step_avg:156.78ms tokens_processed:49152\n",
      "step:963/1750 train_time:150976ms step_avg:156.78ms tokens_processed:49152\n",
      "step:964/1750 train_time:151133ms step_avg:156.78ms tokens_processed:49152\n",
      "step:965/1750 train_time:151291ms step_avg:156.78ms tokens_processed:49152\n",
      "step:966/1750 train_time:151449ms step_avg:156.78ms tokens_processed:49152\n",
      "step:967/1750 train_time:151605ms step_avg:156.78ms tokens_processed:49152\n",
      "step:968/1750 train_time:151762ms step_avg:156.78ms tokens_processed:49152\n",
      "step:969/1750 train_time:151918ms step_avg:156.78ms tokens_processed:49152\n",
      "step:970/1750 train_time:152075ms step_avg:156.78ms tokens_processed:49152\n",
      "step:971/1750 train_time:152232ms step_avg:156.78ms tokens_processed:49152\n",
      "step:972/1750 train_time:152390ms step_avg:156.78ms tokens_processed:49152\n",
      "step:973/1750 train_time:152547ms step_avg:156.78ms tokens_processed:49152\n",
      "step:974/1750 train_time:152703ms step_avg:156.78ms tokens_processed:49152\n",
      "step:975/1750 train_time:152859ms step_avg:156.78ms tokens_processed:49152\n",
      "step:976/1750 train_time:153015ms step_avg:156.78ms tokens_processed:49152\n",
      "step:977/1750 train_time:153173ms step_avg:156.78ms tokens_processed:49152\n",
      "step:978/1750 train_time:153330ms step_avg:156.78ms tokens_processed:49152\n",
      "step:979/1750 train_time:153489ms step_avg:156.78ms tokens_processed:49152\n",
      "step:980/1750 train_time:153646ms step_avg:156.78ms tokens_processed:49152\n",
      "step:981/1750 train_time:153802ms step_avg:156.78ms tokens_processed:49152\n",
      "step:982/1750 train_time:153958ms step_avg:156.78ms tokens_processed:49152\n",
      "step:983/1750 train_time:154114ms step_avg:156.78ms tokens_processed:49152\n",
      "step:984/1750 train_time:154270ms step_avg:156.78ms tokens_processed:49152\n",
      "step:985/1750 train_time:154429ms step_avg:156.78ms tokens_processed:49152\n",
      "step:986/1750 train_time:154588ms step_avg:156.78ms tokens_processed:49152\n",
      "step:987/1750 train_time:154745ms step_avg:156.78ms tokens_processed:49152\n",
      "step:988/1750 train_time:154901ms step_avg:156.78ms tokens_processed:49152\n",
      "step:989/1750 train_time:155057ms step_avg:156.78ms tokens_processed:49152\n",
      "step:990/1750 train_time:155212ms step_avg:156.78ms tokens_processed:49152\n",
      "step:991/1750 train_time:155369ms step_avg:156.78ms tokens_processed:49152\n",
      "step:992/1750 train_time:155527ms step_avg:156.78ms tokens_processed:49152\n",
      "step:993/1750 train_time:155685ms step_avg:156.78ms tokens_processed:49152\n",
      "step:994/1750 train_time:155841ms step_avg:156.78ms tokens_processed:49152\n",
      "step:995/1750 train_time:155997ms step_avg:156.78ms tokens_processed:49152\n",
      "step:996/1750 train_time:156154ms step_avg:156.78ms tokens_processed:49152\n",
      "step:997/1750 train_time:156311ms step_avg:156.78ms tokens_processed:49152\n",
      "step:998/1750 train_time:156468ms step_avg:156.78ms tokens_processed:49152\n",
      "step:999/1750 train_time:156627ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1000/1750 train_time:156785ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1001/1750 train_time:156940ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1002/1750 train_time:157097ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1003/1750 train_time:157253ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1004/1750 train_time:157410ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1005/1750 train_time:157567ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1006/1750 train_time:157724ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1007/1750 train_time:157880ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1008/1750 train_time:158037ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1009/1750 train_time:158193ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1010/1750 train_time:158350ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1011/1750 train_time:158507ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1012/1750 train_time:158665ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1013/1750 train_time:158823ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1014/1750 train_time:158978ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1015/1750 train_time:159135ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1016/1750 train_time:159291ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1017/1750 train_time:159448ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1018/1750 train_time:159605ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1019/1750 train_time:159762ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1020/1750 train_time:159918ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1021/1750 train_time:160075ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1022/1750 train_time:160232ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1023/1750 train_time:160389ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1024/1750 train_time:160546ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1025/1750 train_time:160702ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1026/1750 train_time:160858ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1027/1750 train_time:161015ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1028/1750 train_time:161172ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1029/1750 train_time:161330ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1030/1750 train_time:161489ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1031/1750 train_time:161646ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1032/1750 train_time:161802ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1033/1750 train_time:161957ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1034/1750 train_time:162114ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1035/1750 train_time:162271ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1036/1750 train_time:162429ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1037/1750 train_time:162588ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1038/1750 train_time:162747ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1039/1750 train_time:162901ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1040/1750 train_time:163057ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1041/1750 train_time:163213ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1042/1750 train_time:163370ms step_avg:156.78ms tokens_processed:49152\n",
      "step:1043/1750 train_time:163529ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1044/1750 train_time:163688ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1045/1750 train_time:163845ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1046/1750 train_time:164000ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1047/1750 train_time:164157ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1048/1750 train_time:164312ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1049/1750 train_time:164469ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1050/1750 train_time:164628ms step_avg:156.79ms tokens_processed:49152\n",
      "---validation---\n",
      "step:1050/1750 val_loss:4.1638 train_time:164636ms step_avg:156.80ms\n",
      "---end of validation---\n",
      "step:1051/1750 train_time:164788ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1052/1750 train_time:164944ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1053/1750 train_time:165099ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1054/1750 train_time:165255ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1055/1750 train_time:165413ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1056/1750 train_time:165569ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1057/1750 train_time:165727ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1058/1750 train_time:165884ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1059/1750 train_time:166042ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1060/1750 train_time:166198ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1061/1750 train_time:166355ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1062/1750 train_time:166513ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1063/1750 train_time:166669ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1064/1750 train_time:166826ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1065/1750 train_time:166984ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1066/1750 train_time:167140ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1067/1750 train_time:167298ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1068/1750 train_time:167454ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1069/1750 train_time:167611ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1070/1750 train_time:167768ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1071/1750 train_time:167926ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1072/1750 train_time:168082ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1073/1750 train_time:168239ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1074/1750 train_time:168395ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1075/1750 train_time:168551ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1076/1750 train_time:168708ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1077/1750 train_time:168866ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1078/1750 train_time:169024ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1079/1750 train_time:169181ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1080/1750 train_time:169338ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1081/1750 train_time:169494ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1082/1750 train_time:169650ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1083/1750 train_time:169809ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1084/1750 train_time:169965ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1085/1750 train_time:170123ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1086/1750 train_time:170280ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1087/1750 train_time:170438ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1088/1750 train_time:170594ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1089/1750 train_time:170749ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1090/1750 train_time:170906ms step_avg:156.79ms tokens_processed:49152\n",
      "step:1091/1750 train_time:171064ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1092/1750 train_time:171221ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1093/1750 train_time:171379ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1094/1750 train_time:171537ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1095/1750 train_time:171693ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1096/1750 train_time:171849ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1097/1750 train_time:172005ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1098/1750 train_time:172163ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1099/1750 train_time:172321ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1100/1750 train_time:172479ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1101/1750 train_time:172635ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1102/1750 train_time:172790ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1103/1750 train_time:172947ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1104/1750 train_time:173103ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1105/1750 train_time:173261ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1106/1750 train_time:173420ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1107/1750 train_time:173578ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1108/1750 train_time:173733ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1109/1750 train_time:173888ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1110/1750 train_time:174045ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1111/1750 train_time:174202ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1112/1750 train_time:174361ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1113/1750 train_time:174520ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1114/1750 train_time:174677ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1115/1750 train_time:174832ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1116/1750 train_time:174987ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1117/1750 train_time:175144ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1118/1750 train_time:175300ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1119/1750 train_time:175460ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1120/1750 train_time:175619ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1121/1750 train_time:175777ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1122/1750 train_time:175931ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1123/1750 train_time:176087ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1124/1750 train_time:176243ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1125/1750 train_time:176400ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1126/1750 train_time:176558ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1127/1750 train_time:176718ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1128/1750 train_time:176873ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1129/1750 train_time:177029ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1130/1750 train_time:177186ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1131/1750 train_time:177342ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1132/1750 train_time:177499ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1133/1750 train_time:177659ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1134/1750 train_time:177816ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1135/1750 train_time:177973ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1136/1750 train_time:178128ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1137/1750 train_time:178285ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1138/1750 train_time:178442ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1139/1750 train_time:178600ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1140/1750 train_time:178758ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1141/1750 train_time:178915ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1142/1750 train_time:179071ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1143/1750 train_time:179227ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1144/1750 train_time:179384ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1145/1750 train_time:179540ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1146/1750 train_time:179698ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1147/1750 train_time:179857ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1148/1750 train_time:180014ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1149/1750 train_time:180170ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1150/1750 train_time:180325ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1151/1750 train_time:180482ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1152/1750 train_time:180639ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1153/1750 train_time:180797ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1154/1750 train_time:180954ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1155/1750 train_time:181111ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1156/1750 train_time:181267ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1157/1750 train_time:181423ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1158/1750 train_time:181580ms step_avg:156.80ms tokens_processed:49152\n",
      "step:1159/1750 train_time:181737ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1160/1750 train_time:181896ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1161/1750 train_time:182052ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1162/1750 train_time:182209ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1163/1750 train_time:182366ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1164/1750 train_time:182523ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1165/1750 train_time:182679ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1166/1750 train_time:182836ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1167/1750 train_time:182994ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1168/1750 train_time:183150ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1169/1750 train_time:183306ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1170/1750 train_time:183464ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1171/1750 train_time:183621ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1172/1750 train_time:183779ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1173/1750 train_time:183936ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1174/1750 train_time:184092ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1175/1750 train_time:184248ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1176/1750 train_time:184405ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1177/1750 train_time:184562ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1178/1750 train_time:184720ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1179/1750 train_time:184877ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1180/1750 train_time:185033ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1181/1750 train_time:185189ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1182/1750 train_time:185345ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1183/1750 train_time:185503ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1184/1750 train_time:185660ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1185/1750 train_time:185819ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1186/1750 train_time:185977ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1187/1750 train_time:186133ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1188/1750 train_time:186288ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1189/1750 train_time:186444ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1190/1750 train_time:186601ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1191/1750 train_time:186760ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1192/1750 train_time:186919ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1193/1750 train_time:187075ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1194/1750 train_time:187231ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1195/1750 train_time:187387ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1196/1750 train_time:187543ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1197/1750 train_time:187700ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1198/1750 train_time:187859ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1199/1750 train_time:188018ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1200/1750 train_time:188174ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1201/1750 train_time:188329ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1202/1750 train_time:188486ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1203/1750 train_time:188642ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1204/1750 train_time:188800ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1205/1750 train_time:188959ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1206/1750 train_time:189117ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1207/1750 train_time:189273ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1208/1750 train_time:189428ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1209/1750 train_time:189585ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1210/1750 train_time:189742ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1211/1750 train_time:189899ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1212/1750 train_time:190058ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1213/1750 train_time:190216ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1214/1750 train_time:190371ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1215/1750 train_time:190527ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1216/1750 train_time:190683ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1217/1750 train_time:190840ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1218/1750 train_time:190998ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1219/1750 train_time:191156ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1220/1750 train_time:191313ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1221/1750 train_time:191469ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1222/1750 train_time:191625ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1223/1750 train_time:191782ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1224/1750 train_time:191939ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1225/1750 train_time:192097ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1226/1750 train_time:192254ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1227/1750 train_time:192410ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1228/1750 train_time:192567ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1229/1750 train_time:192724ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1230/1750 train_time:192880ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1231/1750 train_time:193038ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1232/1750 train_time:193195ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1233/1750 train_time:193351ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1234/1750 train_time:193508ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1235/1750 train_time:193666ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1236/1750 train_time:193824ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1237/1750 train_time:193981ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1238/1750 train_time:194137ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1239/1750 train_time:194294ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1240/1750 train_time:194451ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1241/1750 train_time:194607ms step_avg:156.81ms tokens_processed:49152\n",
      "step:1242/1750 train_time:194765ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1243/1750 train_time:194923ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1244/1750 train_time:195081ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1245/1750 train_time:195237ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1246/1750 train_time:195394ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1247/1750 train_time:195551ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1248/1750 train_time:195707ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1249/1750 train_time:195865ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1250/1750 train_time:196023ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1251/1750 train_time:196180ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1252/1750 train_time:196337ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1253/1750 train_time:196493ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1254/1750 train_time:196650ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1255/1750 train_time:196806ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1256/1750 train_time:196964ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1257/1750 train_time:197124ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1258/1750 train_time:197281ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1259/1750 train_time:197438ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1260/1750 train_time:197594ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1261/1750 train_time:197750ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1262/1750 train_time:197906ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1263/1750 train_time:198065ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1264/1750 train_time:198223ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1265/1750 train_time:198380ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1266/1750 train_time:198537ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1267/1750 train_time:198693ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1268/1750 train_time:198849ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1269/1750 train_time:199006ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1270/1750 train_time:199164ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1271/1750 train_time:199323ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1272/1750 train_time:199480ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1273/1750 train_time:199636ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1274/1750 train_time:199793ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1275/1750 train_time:199949ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1276/1750 train_time:200108ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1277/1750 train_time:200264ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1278/1750 train_time:200422ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1279/1750 train_time:200579ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1280/1750 train_time:200735ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1281/1750 train_time:200891ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1282/1750 train_time:201047ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1283/1750 train_time:201204ms step_avg:156.82ms tokens_processed:49152\n",
      "step:1284/1750 train_time:201364ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1285/1750 train_time:201524ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1286/1750 train_time:201683ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1287/1750 train_time:201838ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1288/1750 train_time:201995ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1289/1750 train_time:202151ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1290/1750 train_time:202309ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1291/1750 train_time:202467ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1292/1750 train_time:202625ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1293/1750 train_time:202783ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1294/1750 train_time:202939ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1295/1750 train_time:203096ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1296/1750 train_time:203252ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1297/1750 train_time:203409ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1298/1750 train_time:203567ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1299/1750 train_time:203725ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1300/1750 train_time:203882ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1301/1750 train_time:204039ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1302/1750 train_time:204197ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1303/1750 train_time:204353ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1304/1750 train_time:204510ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1305/1750 train_time:204669ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1306/1750 train_time:204825ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1307/1750 train_time:204984ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1308/1750 train_time:205140ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1309/1750 train_time:205298ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1310/1750 train_time:205454ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1311/1750 train_time:205610ms step_avg:156.83ms tokens_processed:49152\n",
      "step:1312/1750 train_time:205768ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1313/1750 train_time:205926ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1314/1750 train_time:206083ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1315/1750 train_time:206240ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1316/1750 train_time:206397ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1317/1750 train_time:206552ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1318/1750 train_time:206709ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1319/1750 train_time:206867ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1320/1750 train_time:207024ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1321/1750 train_time:207183ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1322/1750 train_time:207339ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1323/1750 train_time:207496ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1324/1750 train_time:207653ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1325/1750 train_time:207810ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1326/1750 train_time:207967ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1327/1750 train_time:208125ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1328/1750 train_time:208284ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1329/1750 train_time:208440ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1330/1750 train_time:208597ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1331/1750 train_time:208754ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1332/1750 train_time:208911ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1333/1750 train_time:209069ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1334/1750 train_time:209226ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1335/1750 train_time:209385ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1336/1750 train_time:209541ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1337/1750 train_time:209698ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1338/1750 train_time:209855ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1339/1750 train_time:210011ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1340/1750 train_time:210168ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1341/1750 train_time:210325ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1342/1750 train_time:210482ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1343/1750 train_time:210638ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1344/1750 train_time:210795ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1345/1750 train_time:210952ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1346/1750 train_time:211108ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1347/1750 train_time:211267ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1348/1750 train_time:211424ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1349/1750 train_time:211582ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1350/1750 train_time:211738ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1351/1750 train_time:211895ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1352/1750 train_time:212052ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1353/1750 train_time:212208ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1354/1750 train_time:212366ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1355/1750 train_time:212524ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1356/1750 train_time:212682ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1357/1750 train_time:212839ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1358/1750 train_time:212995ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1359/1750 train_time:213152ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1360/1750 train_time:213308ms step_avg:156.84ms tokens_processed:49152\n",
      "step:1361/1750 train_time:213466ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1362/1750 train_time:213623ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1363/1750 train_time:213781ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1364/1750 train_time:213939ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1365/1750 train_time:214095ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1366/1750 train_time:214252ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1367/1750 train_time:214407ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1368/1750 train_time:214565ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1369/1750 train_time:214724ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1370/1750 train_time:214881ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1371/1750 train_time:215038ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1372/1750 train_time:215195ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1373/1750 train_time:215352ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1374/1750 train_time:215508ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1375/1750 train_time:215668ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1376/1750 train_time:215824ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1377/1750 train_time:215980ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1378/1750 train_time:216137ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1379/1750 train_time:216293ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1380/1750 train_time:216450ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1381/1750 train_time:216607ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1382/1750 train_time:216764ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1383/1750 train_time:216922ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1384/1750 train_time:217080ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1385/1750 train_time:217236ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1386/1750 train_time:217393ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1387/1750 train_time:217549ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1388/1750 train_time:217706ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1389/1750 train_time:217864ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1390/1750 train_time:218022ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1391/1750 train_time:218180ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1392/1750 train_time:218337ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1393/1750 train_time:218493ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1394/1750 train_time:218649ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1395/1750 train_time:218806ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1396/1750 train_time:218965ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1397/1750 train_time:219123ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1398/1750 train_time:219281ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1399/1750 train_time:219437ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1400/1750 train_time:219594ms step_avg:156.85ms tokens_processed:49152\n",
      "---validation---\n",
      "step:1400/1750 val_loss:3.9729 train_time:219600ms step_avg:156.86ms\n",
      "---end of validation---\n",
      "step:1401/1750 train_time:219752ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1402/1750 train_time:219908ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1403/1750 train_time:220067ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1404/1750 train_time:220224ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1405/1750 train_time:220380ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1406/1750 train_time:220537ms step_avg:156.85ms tokens_processed:49152\n",
      "step:1407/1750 train_time:220696ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1408/1750 train_time:220852ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1409/1750 train_time:221009ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1410/1750 train_time:221167ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1411/1750 train_time:221325ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1412/1750 train_time:221482ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1413/1750 train_time:221637ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1414/1750 train_time:221795ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1415/1750 train_time:221952ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1416/1750 train_time:222109ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1417/1750 train_time:222268ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1418/1750 train_time:222427ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1419/1750 train_time:222584ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1420/1750 train_time:222740ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1421/1750 train_time:222897ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1422/1750 train_time:223054ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1423/1750 train_time:223212ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1424/1750 train_time:223370ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1425/1750 train_time:223530ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1426/1750 train_time:223687ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1427/1750 train_time:223842ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1428/1750 train_time:223999ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1429/1750 train_time:224155ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1430/1750 train_time:224312ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1431/1750 train_time:224471ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1432/1750 train_time:224630ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1433/1750 train_time:224787ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1434/1750 train_time:224943ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1435/1750 train_time:225099ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1436/1750 train_time:225255ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1437/1750 train_time:225413ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1438/1750 train_time:225572ms step_avg:156.86ms tokens_processed:49152\n",
      "step:1439/1750 train_time:225731ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1440/1750 train_time:225889ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1441/1750 train_time:226046ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1442/1750 train_time:226202ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1443/1750 train_time:226358ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1444/1750 train_time:226515ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1445/1750 train_time:226675ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1446/1750 train_time:226832ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1447/1750 train_time:226991ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1448/1750 train_time:227148ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1449/1750 train_time:227305ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1450/1750 train_time:227461ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1451/1750 train_time:227619ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1452/1750 train_time:227777ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1453/1750 train_time:227935ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1454/1750 train_time:228093ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1455/1750 train_time:228249ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1456/1750 train_time:228406ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1457/1750 train_time:228562ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1458/1750 train_time:228719ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1459/1750 train_time:228878ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1460/1750 train_time:229036ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1461/1750 train_time:229194ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1462/1750 train_time:229351ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1463/1750 train_time:229508ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1464/1750 train_time:229665ms step_avg:156.87ms tokens_processed:49152\n",
      "step:1465/1750 train_time:229822ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1466/1750 train_time:229980ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1467/1750 train_time:230138ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1468/1750 train_time:230295ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1469/1750 train_time:230451ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1470/1750 train_time:230608ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1471/1750 train_time:230767ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1472/1750 train_time:230925ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1473/1750 train_time:231082ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1474/1750 train_time:231239ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1475/1750 train_time:231396ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1476/1750 train_time:231552ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1477/1750 train_time:231709ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1478/1750 train_time:231868ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1479/1750 train_time:232027ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1480/1750 train_time:232185ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1481/1750 train_time:232340ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1482/1750 train_time:232497ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1483/1750 train_time:232655ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1484/1750 train_time:232811ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1485/1750 train_time:232971ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1486/1750 train_time:233129ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1487/1750 train_time:233288ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1488/1750 train_time:233443ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1489/1750 train_time:233598ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1490/1750 train_time:233755ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1491/1750 train_time:233912ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1492/1750 train_time:234071ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1493/1750 train_time:234230ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1494/1750 train_time:234388ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1495/1750 train_time:234544ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1496/1750 train_time:234700ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1497/1750 train_time:234857ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1498/1750 train_time:235014ms step_avg:156.88ms tokens_processed:49152\n",
      "step:1499/1750 train_time:235173ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1500/1750 train_time:235331ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1501/1750 train_time:235489ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1502/1750 train_time:235645ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1503/1750 train_time:235802ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1504/1750 train_time:235958ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1505/1750 train_time:236116ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1506/1750 train_time:236274ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1507/1750 train_time:236433ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1508/1750 train_time:236591ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1509/1750 train_time:236748ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1510/1750 train_time:236905ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1511/1750 train_time:237061ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1512/1750 train_time:237218ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1513/1750 train_time:237377ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1514/1750 train_time:237534ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1515/1750 train_time:237693ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1516/1750 train_time:237851ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1517/1750 train_time:238008ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1518/1750 train_time:238165ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1519/1750 train_time:238322ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1520/1750 train_time:238480ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1521/1750 train_time:238637ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1522/1750 train_time:238794ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1523/1750 train_time:238951ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1524/1750 train_time:239108ms step_avg:156.89ms tokens_processed:49152\n",
      "step:1525/1750 train_time:239265ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1526/1750 train_time:239423ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1527/1750 train_time:239581ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1528/1750 train_time:239737ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1529/1750 train_time:239895ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1530/1750 train_time:240052ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1531/1750 train_time:240210ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1532/1750 train_time:240369ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1533/1750 train_time:240527ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1534/1750 train_time:240683ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1535/1750 train_time:240839ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1536/1750 train_time:240997ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1537/1750 train_time:241153ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1538/1750 train_time:241309ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1539/1750 train_time:241469ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1540/1750 train_time:241627ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1541/1750 train_time:241785ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1542/1750 train_time:241941ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1543/1750 train_time:242099ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1544/1750 train_time:242254ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1545/1750 train_time:242412ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1546/1750 train_time:242570ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1547/1750 train_time:242729ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1548/1750 train_time:242887ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1549/1750 train_time:243043ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1550/1750 train_time:243199ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1551/1750 train_time:243355ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1552/1750 train_time:243512ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1553/1750 train_time:243671ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1554/1750 train_time:243830ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1555/1750 train_time:243988ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1556/1750 train_time:244145ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1557/1750 train_time:244301ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1558/1750 train_time:244457ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1559/1750 train_time:244614ms step_avg:156.90ms tokens_processed:49152\n",
      "step:1560/1750 train_time:244773ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1561/1750 train_time:244932ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1562/1750 train_time:245089ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1563/1750 train_time:245246ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1564/1750 train_time:245402ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1565/1750 train_time:245558ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1566/1750 train_time:245716ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1567/1750 train_time:245874ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1568/1750 train_time:246033ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1569/1750 train_time:246192ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1570/1750 train_time:246348ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1571/1750 train_time:246505ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1572/1750 train_time:246662ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1573/1750 train_time:246819ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1574/1750 train_time:246977ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1575/1750 train_time:247134ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1576/1750 train_time:247293ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1577/1750 train_time:247451ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1578/1750 train_time:247607ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1579/1750 train_time:247764ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1580/1750 train_time:247921ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1581/1750 train_time:248079ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1582/1750 train_time:248236ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1583/1750 train_time:248394ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1584/1750 train_time:248551ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1585/1750 train_time:248708ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1586/1750 train_time:248865ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1587/1750 train_time:249022ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1588/1750 train_time:249179ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1589/1750 train_time:249336ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1590/1750 train_time:249495ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1591/1750 train_time:249652ms step_avg:156.91ms tokens_processed:49152\n",
      "step:1592/1750 train_time:249809ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1593/1750 train_time:249967ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1594/1750 train_time:250125ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1595/1750 train_time:250282ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1596/1750 train_time:250438ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1597/1750 train_time:250595ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1598/1750 train_time:250752ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1599/1750 train_time:250909ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1600/1750 train_time:251067ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1601/1750 train_time:251226ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1602/1750 train_time:251382ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1603/1750 train_time:251539ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1604/1750 train_time:251696ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1605/1750 train_time:251852ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1606/1750 train_time:252010ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1607/1750 train_time:252169ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1608/1750 train_time:252327ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1609/1750 train_time:252483ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1610/1750 train_time:252639ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1611/1750 train_time:252796ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1612/1750 train_time:252954ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1613/1750 train_time:253111ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1614/1750 train_time:253270ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1615/1750 train_time:253428ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1616/1750 train_time:253586ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1617/1750 train_time:253742ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1618/1750 train_time:253898ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1619/1750 train_time:254055ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1620/1750 train_time:254212ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1621/1750 train_time:254372ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1622/1750 train_time:254530ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1623/1750 train_time:254689ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1624/1750 train_time:254843ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1625/1750 train_time:254999ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1626/1750 train_time:255156ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1627/1750 train_time:255313ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1628/1750 train_time:255471ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1629/1750 train_time:255630ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1630/1750 train_time:255787ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1631/1750 train_time:255943ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1632/1750 train_time:256100ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1633/1750 train_time:256256ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1634/1750 train_time:256413ms step_avg:156.92ms tokens_processed:49152\n",
      "step:1635/1750 train_time:256572ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1636/1750 train_time:256732ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1637/1750 train_time:256890ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1638/1750 train_time:257047ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1639/1750 train_time:257203ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1640/1750 train_time:257359ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1641/1750 train_time:257516ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1642/1750 train_time:257675ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1643/1750 train_time:257833ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1644/1750 train_time:257991ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1645/1750 train_time:258148ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1646/1750 train_time:258306ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1647/1750 train_time:258462ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1648/1750 train_time:258619ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1649/1750 train_time:258778ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1650/1750 train_time:258938ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1651/1750 train_time:259094ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1652/1750 train_time:259250ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1653/1750 train_time:259407ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1654/1750 train_time:259563ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1655/1750 train_time:259719ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1656/1750 train_time:259878ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1657/1750 train_time:260035ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1658/1750 train_time:260193ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1659/1750 train_time:260350ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1660/1750 train_time:260507ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1661/1750 train_time:260663ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1662/1750 train_time:260820ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1663/1750 train_time:260978ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1664/1750 train_time:261136ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1665/1750 train_time:261294ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1666/1750 train_time:261451ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1667/1750 train_time:261607ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1668/1750 train_time:261764ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1669/1750 train_time:261922ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1670/1750 train_time:262080ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1671/1750 train_time:262238ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1672/1750 train_time:262396ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1673/1750 train_time:262553ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1674/1750 train_time:262709ms step_avg:156.93ms tokens_processed:49152\n",
      "step:1675/1750 train_time:262867ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1676/1750 train_time:263025ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1677/1750 train_time:263181ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1678/1750 train_time:263338ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1679/1750 train_time:263496ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1680/1750 train_time:263653ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1681/1750 train_time:263809ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1682/1750 train_time:263968ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1683/1750 train_time:264126ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1684/1750 train_time:264283ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1685/1750 train_time:264439ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1686/1750 train_time:264597ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1687/1750 train_time:264752ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1688/1750 train_time:264910ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1689/1750 train_time:265069ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1690/1750 train_time:265229ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1691/1750 train_time:265386ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1692/1750 train_time:265541ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1693/1750 train_time:265697ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1694/1750 train_time:265854ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1695/1750 train_time:266013ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1696/1750 train_time:266172ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1697/1750 train_time:266330ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1698/1750 train_time:266487ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1699/1750 train_time:266643ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1700/1750 train_time:266800ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1701/1750 train_time:266957ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1702/1750 train_time:267115ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1703/1750 train_time:267273ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1704/1750 train_time:267432ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1705/1750 train_time:267590ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1706/1750 train_time:267747ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1707/1750 train_time:267904ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1708/1750 train_time:268059ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1709/1750 train_time:268217ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1710/1750 train_time:268375ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1711/1750 train_time:268532ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1712/1750 train_time:268691ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1713/1750 train_time:268847ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1714/1750 train_time:269003ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1715/1750 train_time:269159ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1716/1750 train_time:269316ms step_avg:156.94ms tokens_processed:49152\n",
      "step:1717/1750 train_time:269475ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1718/1750 train_time:269634ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1719/1750 train_time:269792ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1720/1750 train_time:269948ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1721/1750 train_time:270105ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1722/1750 train_time:270262ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1723/1750 train_time:270418ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1724/1750 train_time:270577ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1725/1750 train_time:270735ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1726/1750 train_time:270893ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1727/1750 train_time:271050ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1728/1750 train_time:271208ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1729/1750 train_time:271364ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1730/1750 train_time:271521ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1731/1750 train_time:271679ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1732/1750 train_time:271836ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1733/1750 train_time:271994ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1734/1750 train_time:272151ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1735/1750 train_time:272308ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1736/1750 train_time:272465ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1737/1750 train_time:272623ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1738/1750 train_time:272780ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1739/1750 train_time:272938ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1740/1750 train_time:273095ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1741/1750 train_time:273252ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1742/1750 train_time:273409ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1743/1750 train_time:273568ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1744/1750 train_time:273726ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1745/1750 train_time:273883ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1746/1750 train_time:274039ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1747/1750 train_time:274196ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1748/1750 train_time:274353ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1749/1750 train_time:274511ms step_avg:156.95ms tokens_processed:49152\n",
      "step:1750/1750 train_time:274670ms step_avg:156.95ms tokens_processed:49152\n",
      "---validation---\n",
      "step:1750/1750 val_loss:3.8418 train_time:274678ms step_avg:156.96ms\n",
      "---end of validation---\n"
     ]
    }
   ],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        val_batch_size = world_size * args.val_seq_len\n",
    "\n",
    "        assert args.val_tokens % val_batch_size == 0\n",
    "        val_steps = args.val_tokens // val_batch_size\n",
    "        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):                \n",
    "                inputs, targets = next(val_loader)\n",
    "                loss = model(inputs, targets)\n",
    "                val_loss += loss\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader)\n",
    "    if step == 0:\n",
    "        print(\"First inputs retrieved\")\n",
    "    tokens_processed += len(inputs) # maybe times world size?\n",
    "\n",
    "    loss = model(inputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    # set optimization hyperparameters\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "    # step the optimizers\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    # null the gradients\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    # logging\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{len(inputs)}\",\n",
    "        console=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0f86-3515-4b2c-b6e7-ab96b5f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e07454-e46b-4ec7-8b97-06a505fc8698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
