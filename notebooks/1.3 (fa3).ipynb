{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b897866-7135-4029-afd5-1beea13cfb24",
   "metadata": {},
   "source": [
    "Do something like\n",
    "\n",
    "```\n",
    "pip install -U pip wheel setuptools ninja numpy packaging\n",
    "pip install --pre \"torch==2.9.0.dev20250713+cu126\" --index-url https://download.pytorch.org/whl/nightly/cu126\n",
    "\n",
    "\n",
    "git clone https://github.com/guilhermeleobas/flash-attention.git\n",
    "cd flash-attention/hopper\n",
    "git switch guilhermeleobas/fa3-compile\n",
    "\n",
    "\n",
    "export MAX_JOBS=24\n",
    "export FLASH_ATTENTION_FORCE_BUILD=TRUE        # skip prebuilt wheel fetch\n",
    "export FLASH_ATTENTION_DISABLE_SM80=TRUE       # Hopper-only\n",
    "export FLASH_ATTENTION_DISABLE_FP16=TRUE       # leave BF16, FP8\n",
    "export FLASH_ATTENTION_DISABLE_HDIM64=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM96=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM192=TRUE\n",
    "export FLASH_ATTENTION_DISABLE_HDIM256=TRUE\n",
    "\n",
    "python setup.py bdist_wheel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945e553f-dbff-48ba-8561-74f51bd11572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /workspace/flash-attention/hopper/dist/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (2.9.0.dev20250713+cu126)\n",
      "Collecting einops (from flash-attn-3==3.0.0b1)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (25.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn-3==3.0.0b1) (1.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.9 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.3.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.4.0+gitae848267 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn-3==3.0.0b1) (3.4.0+gitae848267)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-triton==3.4.0+gitae848267->torch->flash-attn-3==3.0.0b1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->flash-attn-3==3.0.0b1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn-3==3.0.0b1) (2.1.5)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn-3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn-3]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-3-3.0.0b1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /workspace/flash-attention/hopper/dist/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f894d5-2b23-46f6-8622-90ed61ff381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2025.8.3)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, hf-xet, huggingface-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [huggingface-hub] [huggingface-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.8 huggingface-hub-0.34.4 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f8d4b3-1323-4760-bd21-c4272c2fee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.9.0.dev20250713+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, pytorch-triton, sympy, typing-extensions\n",
      "Required-by: flash_attn_3, torchaudio, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ed5d43-2a1e-4cd9-811c-03137f276fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: flash_attn_3\n",
      "Version: 3.0.0b1\n",
      "Summary: FlashAttention-3\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: einops, ninja, packaging, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show flash_attn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2598ca10-f9da-4891-ae94-31c298b99fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fineweb_val_000000.bin: 100%|█████████████████| 200M/200M [00:00<00:00, 465MB/s]\n",
      "fineweb_train_000001.bin: 100%|███████████████| 200M/200M [00:00<00:00, 337MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../data/cached_fineweb10B.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ebe6a-7562-4d31-9c14-ac8b44269678",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62975aa6-f3b7-489a-9131-ea69089074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f91470-f6e0-431d-a490-23c80f3c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a879e98-a216-4626-a4ea-8db22d99df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432bad-bbd3-4853-9a8d-2b11d8feda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969746f-cc86-4121-a01e-19868c2a6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c63379b-7f98-4b29-aa53-31af47bc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ccb5-4d25-4918-aeb5-1b5a1f44a0fe",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1dd716-9d1b-4086-ae9c-3cf032ec04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile ## ns\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f0f206-d925-4920-8f53-681265a44628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # Efficient systems-wise implementation of step developed by @YouJiacheng,\n",
    "        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,\n",
    "        # @ryanyang0, and @vagrawal.\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                if base_i + rank < len(params):\n",
    "                    grad = params[base_i + rank].grad\n",
    "                # This gives strange dynamo warnings\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * world_size\n",
    "            momentum = group[\"momentum\"]\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                if base_i + rank < len(params):\n",
    "                    p = params[base_i + rank]\n",
    "                    grad = p.grad\n",
    "                    eff_lr = group[\"lr\"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, \"lr_mul\", 1.0)\n",
    "                    eff_weight_decay = group[\"lr\"] * group[\"weight_decay\"] * getattr(p, \"wd_mul\", 1.0)\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
    "                    momentum_buffer = state[\"momentum_buffer\"]\n",
    "                    p.mul_(1 - eff_weight_decay)\n",
    "                    momentum_buffer.lerp_(grad, 1 - momentum)\n",
    "                    grad = grad.lerp_(momentum_buffer, momentum)\n",
    "                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)\n",
    "                    p.add_(other=v, alpha=-eff_lr)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbc0f81-97ec-4ed7-99df-78f34bada062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "        # DistributedAdam implementation by @vagrawal\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78ffb1-753b-4a6f-a57f-81781873c084",
   "metadata": {},
   "source": [
    "## Custom Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e2360d-f65a-451b-89c9-9020d252e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x.new_tensor(x_s, dtype=torch.float32),\n",
    "            scale_b=x.new_tensor(w_s, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)\n",
    "        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)\n",
    "        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)\n",
    "        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=grad_inv_s,\n",
    "            scale_b=w_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=x_inv_s,\n",
    "            scale_b=grad_inv_s,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "        return grad_x, grad_w\n",
    "\n",
    "    return impl(g, x_f8, w_f8)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)\n",
    "\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    return grad_x, grad_w, None, None, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c129e66-be79-4122-857d-fe205176d0c6",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ec017e-065d-4f43-b092-9163c3aaf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43c37-4d2a-4fd6-bd58-63fcd1b5a54a",
   "metadata": {},
   "source": [
    "### CastedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4934dccb-7f32-442d-a7b8-4cc8d666d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastedLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.use_fp8 = use_fp8\n",
    "        self.x_s = x_s\n",
    "        self.w_s = w_s\n",
    "        self.grad_s = grad_s\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.use_fp8 and self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65922453-7b13-425f-a888-899f53d243ea",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c79cc0a-7a69-4d76-88a0-906ee65da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim // 4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim // 4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a536ae-14b2-4b5c-b0ee-f2ca0ad7045a",
   "metadata": {},
   "source": [
    "### Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2d179f-d588-4731-afc3-242e31ae487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn_interface\n",
    "from flash_attn_interface import flash_attn_qkvpacked_func, flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a21056-70ee-4b77-a2b8-8c33f427bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        self.attn_scale = 0.12\n",
    "            \n",
    "    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, bm_size: int):\n",
    "        B, T = x.shape[:2]  # batch, seqlen\n",
    "        H, D = self.num_heads, self.head_dim\n",
    "    \n",
    "        # (3, H*D, dim) -> (3*H*D, dim) for F.linear, then (B, T, 3, H, D)\n",
    "        qkv = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)) \\\n",
    "                .view(B, T, 3, H, D)\n",
    "    \n",
    "        # Get q, k, v as separate tensors: each (B, T, H, D)\n",
    "        # (Using unbind avoids the extra squeeze step; chunk(3, dim=2) + squeeze(2) is also fine.)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "    \n",
    "        # --- NO IN-PLACE WRITES ON VIEWS ---\n",
    "        # Apply norm + rotary (produce new tensors instead of copy_)\n",
    "        q = self.rotary(norm(q))\n",
    "        k = self.rotary(norm(k))\n",
    "    \n",
    "        # Mix ve into v without in-place ops\n",
    "        if ve is not None:\n",
    "            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)\n",
    "        else:\n",
    "            v = lambdas[0] * v\n",
    "    \n",
    "        # Call FlashAttention (ensure contiguous buffers, correct dtype)\n",
    "        y = flash_attn_func(\n",
    "            q.contiguous(),\n",
    "            k.contiguous(),\n",
    "            v.contiguous(),\n",
    "            softmax_scale=self.attn_scale,   # or None\n",
    "            causal=True,\n",
    "            window_size=(bm_size, 0),\n",
    "            deterministic=False,\n",
    "        )  # (B, T, H, D)\n",
    "    \n",
    "        y = y.reshape(B, T, H * D)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6828-b90a-47c6-9759-73bbab68521c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "992d3a44-eca9-4645-b9b9-b678cd76fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = CastedLinear(dim, hdim)\n",
    "        self.c_proj = CastedLinear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(\n",
    "            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa25d-96b7-4026-904d-e9b7f6e7ce7a",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c18407-3a67-4e22-9f5f-7717fb3cffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, bm_size: int):\n",
    "        x = lambdas[0] * x + lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, sa_lambdas, bm_size)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84352c5e-79c7-4a81-a683-e6a3b1561e95",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0bf666-e06a-4164-909b-4408c67b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897\n",
    "        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.\n",
    "        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.\n",
    "        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim ** 0.5) / 448, w_s=24 / 448,\n",
    "                                    grad_s=1 / 448)\n",
    "        self.lm_head.weight.detach().zero_()  # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        pad = (-num_layers * 5) % dist.get_world_size()\n",
    "        self.scalars = nn.Parameter(torch.cat([\n",
    "            torch.ones(num_layers),  # skip_weights\n",
    "            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],  # block lambdas\n",
    "            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],  # SA lambdas\n",
    "            torch.ones(pad),\n",
    "        ]))\n",
    "        # set learning rates\n",
    "        for param in self.embed.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        for param in self.value_embeds.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        self.lm_head.weight.lr_mul = 27.5\n",
    "        self.scalars.lr_mul = 5.0\n",
    "        \n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: int):\n",
    "        assert input_seq.ndim == 1\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        long_bm, short_bm = sliding_window_num_blocks, sliding_window_num_blocks // 2\n",
    "        bm_sizes = [long_bm, short_bm, short_bm, \n",
    "                       short_bm, long_bm, short_bm, \n",
    "                       short_bm, long_bm, short_bm, \n",
    "                       short_bm, short_bm, long_bm]\n",
    "        assert len(bm_sizes) == len(self.blocks)\n",
    "\n",
    "        x = x0 = norm(self.embed(input_seq)[None])  # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        skip_weights = self.scalars[:(len(self.blocks) // 2)]\n",
    "        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)\n",
    "        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)\n",
    "\n",
    "        n = len(self.blocks) // 2\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], bm_sizes[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "        logits = self.lm_head(x).float()\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1) ** 0.5))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq,\n",
    "                               reduction=\"sum\" if self.training else \"mean\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18b470-51e6-466a-a90b-20606dcac5dc",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e719acf6-7de5-4e4f-97db-20e6397ead89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d9c76-d429-4926-bf62-703b99fa056c",
   "metadata": {},
   "source": [
    "Previous approach:\n",
    "- builds boolean mask over max_batch_span tokens; linear in max_batch_span\n",
    "- new approach: build EOS index per shard. Log search on number of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe1f0904-4288-4559-b8ca-20f6084d5bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EOSBatchFinder:\n",
    "    def __init__(self, tokens: Tensor, eos_id: int = 50256):\n",
    "        # Precompute EOS positions once per shard \n",
    "        self.eos_idx = (tokens == eos_id).nonzero(as_tuple=True)[0].to(torch.int64).cpu()\n",
    "        self.i = 0      # pointer into eos_idx (start EOS for next step)\n",
    "        self.pos = 0    # logical stream position within this shard\n",
    "\n",
    "    def seek(self, pos: int):\n",
    "        # Set pointer to the first EOS >= pos\n",
    "        self.i = int(torch.searchsorted(self.eos_idx, pos))\n",
    "        if self.i >= int(self.eos_idx.numel()):\n",
    "            raise StopIteration(\"Seek past last EOS.\")\n",
    "        self.pos = pos\n",
    "\n",
    "    def next_batch(self, local_batch_size: int, world_size: int = 1):\n",
    "        n = int(self.eos_idx.numel())\n",
    "        if self.i >= n:\n",
    "            raise StopIteration(\"No more EOS in this shard.\")\n",
    "\n",
    "        starts = []\n",
    "        idx = self.i\n",
    "        cur = int(self.eos_idx[idx])  # absolute position (token index) of current start\n",
    "\n",
    "        # For each rank, find the first EOS >= cur + local_batch_size\n",
    "        for _ in range(world_size):\n",
    "            target = cur + local_batch_size\n",
    "            j = int(torch.searchsorted(self.eos_idx, target))\n",
    "            if j >= n:\n",
    "                raise StopIteration(\"Insufficient EOS ahead; hit tail of shard.\")\n",
    "            starts.append(cur)\n",
    "            idx = j\n",
    "            cur = int(self.eos_idx[idx])  # next segment starts at this end EOS\n",
    "\n",
    "        advance = int(self.eos_idx[idx] - self.pos)  # move stream to the last end\n",
    "        self.pos += advance\n",
    "        self.i = idx\n",
    "        return starts, advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f83a1c-a134-47ae-95e0-6386477a5c24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]\n",
    "    assert batch_size % world_size == 0\n",
    "    local_batch_size = batch_size // world_size\n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "\n",
    "    finder = EOSBatchFinder(tokens) if align_to_bos else None\n",
    "    if align_to_bos:\n",
    "        finder.seek(pos)\n",
    "    \n",
    "    max_batch_span = batch_size # only used when not align_to_bos\n",
    "    while True:\n",
    "        if pos + max_batch_span + 1 >= len(tokens):\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "            if align_to_bos:\n",
    "                finder = EOSBatchFinder(tokens)\n",
    "                finder.seek(pos)\n",
    "        if align_to_bos:\n",
    "            try:\n",
    "                batch_starts, batch_span = finder.next_batch(local_batch_size, world_size)\n",
    "                start_idx = batch_starts[rank]\n",
    "            except StopIteration:\n",
    "                # move to next shard on next loop\n",
    "                pos = len(tokens) + 1\n",
    "                continue\n",
    "        else:\n",
    "            batch_span = batch_size\n",
    "            start_idx = pos + rank * local_batch_size\n",
    "        buf = tokens[start_idx:][:local_batch_size + 1]\n",
    "        inputs = buf[:-1].to(device=\"cuda\", dtype=torch.int32, non_blocking=True) # no sync on host side;\n",
    "        targets = buf[1:].to(device=\"cuda\", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.\n",
    "        pos += batch_span\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6ff7-2376-4818-b0c6-63ea5c91a4c6",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d8c4d61-8438-4512-a4cd-476b74b5111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "031c79ec-83bf-4c7c-8bc1-d1f48488720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files = \"../data/fineweb10B/fineweb_train_*.bin\"  # input .bin to train on\n",
    "    val_files = \"../data/fineweb10B/fineweb_val_*.bin\"  # input .bin to eval validation loss on\n",
    "    val_tokens = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    train_seq_len = 48 * 1024  # FlexAttention sequence length\n",
    "    val_seq_len = 4 * 64 * 1024  # FlexAttention sequence length for validation\n",
    "    # optimization\n",
    "    num_iterations = 1750  # number of iterations to run\n",
    "    cooldown_frac = 0.45  # fraction of training spent cooling down the learning rate\n",
    "    # evaluation and logging\n",
    "    val_loss_every = 350  # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "\n",
    "    vocab_size = next_multiple_of_n(50257, n=128)\n",
    "    num_layers = 12\n",
    "    num_heads = 6\n",
    "    model_dim = 768\n",
    "\n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00850fb8-6c7a-47ae-8b3f-7f03570bc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['LOCAL_RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['GROUP_RANK'] = '0'\n",
    "os.environ['ROLE_RANK'] = '0'\n",
    "os.environ['ROLE_NAME'] = 'default'\n",
    "os.environ['ROLE_WORLD_SIZE'] = '1'\n",
    "os.environ['TORCHELASTIC_RESTART_COUNT'] = '0'\n",
    "os.environ['TORCHELASTIC_MAX_RESTARTS'] = '0'\n",
    "os.environ['TORCHELASTIC_RUN_ID'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "711588cc-0891-40ae-867f-d6aad225b656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "master_process = (rank == 0)  # this process will do logging, checkpointing etc.\n",
    "# assert world_size == 8  # this code is designed for 8xH100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41bdf92-3c1d-49f6-bf5b-1f40cf019fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "703f7239-29f7-4991-8539-df8c1b4eeb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "dist.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea574fff-3d48-4806-bc64-6cf7a9968b33",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c007ae54-89cc-448a-8380-634e61fe4ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/b204653b-b7c7-4df2-8564-05a13ffd5552.txt\n"
     ]
    }
   ],
   "source": [
    "# begin logging\n",
    "logfile = None\n",
    "if master_process:\n",
    "    run_id = uuid.uuid4()\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logfile = f\"logs/{run_id}.txt\"\n",
    "    print(logfile)\n",
    "\n",
    "\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddc10c5-9eed-45e3-9fd9-b4413a8d5994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 20 17:21:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                  Off |\n",
      "| N/A   43C    P0              70W / 350W |   1103MiB / 81559MiB |      4%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb0b9b-de7f-4052-a2fe-e6f9a69fc930",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "253fdb42-b56c-4a3b-bb0c-7ac5837d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = GPT(vocab_size=args.vocab_size, \n",
    "                       num_layers=args.num_layers, \n",
    "                       num_heads=args.num_heads, \n",
    "                       model_dim=args.model_dim,\n",
    "                       max_seq_len=max(args.train_seq_len, args.val_seq_len)\n",
    "                      ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c4760a4-1858-499b-b961-530cd04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a03b657a-b05f-4aa5-bf53-d07702385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12334b9f-193f-446b-a3e8-00a1479bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10,\n",
    "                      weight_decay=0.0)\n",
    "optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2672d18a-aad3-4706-822b-804d9ca90ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = torch.compile(model, dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809242-8dca-4f72-bc28-736aa58fc03c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0e811f2-8daa-4992-8271-1ca7f9410fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    if x < 1 - args.cooldown_frac:\n",
    "        return 1.0\n",
    "    else:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        return w * 1.0 + (1 - w) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29457d3f-2d28-4ae0-87e4-7c06cde338c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(1)\n",
    "def get_window_size_tokens(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x <= 1\n",
    "\n",
    "    return next_multiple_of_n(1728 * x, n=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e779f1-8889-4a9b-afcc-6a3172237e49",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fc40a3d-7ad0-4a8c-a035-e14357268cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 100\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state\n",
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1ffe97c-e49c-46f1-a9b1-49a81efc762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a71d51cc-f59c-4591-ae56-5a36fe2d912c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4efb5167e634f89a1dd88861dc6cf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: This API is going to be deprecated, please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:78.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(warmup_steps)):\n",
    "    inputs, targets = next(train_loader)\n",
    "    model(inputs, targets, get_window_size_tokens(1)).backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "model.load_state_dict(initial_state[\"model\"])\n",
    "for opt, opt_state in zip(optimizers, initial_state[\"optimizers\"]):\n",
    "    opt.load_state_dict(opt_state)\n",
    "del train_loader, initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42486d-5294-4c73-a903-2bacadc89567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be27d96a-9085-417b-b8e3-479220a3f2e1",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5496e90d-9c4e-43ce-92f6-b49362e624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)\n",
    "train_steps = args.num_iterations\n",
    "training_time_ms = 0\n",
    "tokens_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c30bd45-5c91-4e04-a04e-3fc4021306eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---validation---\n",
      "step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.33ms\n",
      "---end of validation---\n",
      "First inputs retrieved\n",
      "step:1/1750 train_time:222ms step_avg:221.65ms tokens_processed:49152\n",
      "step:2/1750 train_time:361ms step_avg:180.26ms tokens_processed:98304\n",
      "step:3/1750 train_time:511ms step_avg:170.18ms tokens_processed:147456\n",
      "step:4/1750 train_time:662ms step_avg:165.57ms tokens_processed:196608\n",
      "step:5/1750 train_time:815ms step_avg:163.06ms tokens_processed:245760\n",
      "step:6/1750 train_time:967ms step_avg:161.17ms tokens_processed:294912\n",
      "step:7/1750 train_time:1123ms step_avg:160.40ms tokens_processed:344064\n",
      "step:8/1750 train_time:1281ms step_avg:160.15ms tokens_processed:393216\n",
      "step:9/1750 train_time:1438ms step_avg:159.78ms tokens_processed:442368\n",
      "step:10/1750 train_time:1594ms step_avg:159.38ms tokens_processed:491520\n",
      "step:11/1750 train_time:1747ms step_avg:158.82ms tokens_processed:540672\n",
      "step:12/1750 train_time:1900ms step_avg:158.34ms tokens_processed:589824\n",
      "step:13/1750 train_time:2055ms step_avg:158.04ms tokens_processed:638976\n",
      "step:14/1750 train_time:2210ms step_avg:157.84ms tokens_processed:688128\n",
      "step:15/1750 train_time:2364ms step_avg:157.62ms tokens_processed:737280\n",
      "step:16/1750 train_time:2521ms step_avg:157.56ms tokens_processed:786432\n",
      "step:17/1750 train_time:2676ms step_avg:157.40ms tokens_processed:835584\n",
      "step:18/1750 train_time:2830ms step_avg:157.21ms tokens_processed:884736\n",
      "step:19/1750 train_time:2983ms step_avg:156.99ms tokens_processed:933888\n",
      "step:20/1750 train_time:3138ms step_avg:156.90ms tokens_processed:983040\n",
      "step:21/1750 train_time:3295ms step_avg:156.89ms tokens_processed:1032192\n",
      "step:22/1750 train_time:3450ms step_avg:156.82ms tokens_processed:1081344\n",
      "step:23/1750 train_time:3606ms step_avg:156.79ms tokens_processed:1130496\n",
      "step:24/1750 train_time:3760ms step_avg:156.68ms tokens_processed:1179648\n",
      "step:25/1750 train_time:3915ms step_avg:156.60ms tokens_processed:1228800\n",
      "step:26/1750 train_time:4069ms step_avg:156.50ms tokens_processed:1277952\n",
      "step:27/1750 train_time:4225ms step_avg:156.47ms tokens_processed:1327104\n",
      "step:28/1750 train_time:4379ms step_avg:156.40ms tokens_processed:1376256\n",
      "step:29/1750 train_time:4536ms step_avg:156.40ms tokens_processed:1425408\n",
      "step:30/1750 train_time:4690ms step_avg:156.34ms tokens_processed:1474560\n",
      "step:31/1750 train_time:4845ms step_avg:156.30ms tokens_processed:1523712\n",
      "step:32/1750 train_time:5001ms step_avg:156.27ms tokens_processed:1572864\n",
      "step:33/1750 train_time:5156ms step_avg:156.24ms tokens_processed:1622016\n",
      "step:34/1750 train_time:5311ms step_avg:156.22ms tokens_processed:1671168\n",
      "step:35/1750 train_time:5466ms step_avg:156.17ms tokens_processed:1720320\n",
      "step:36/1750 train_time:5621ms step_avg:156.13ms tokens_processed:1769472\n",
      "step:37/1750 train_time:5776ms step_avg:156.11ms tokens_processed:1818624\n",
      "step:38/1750 train_time:5931ms step_avg:156.09ms tokens_processed:1867776\n",
      "step:39/1750 train_time:6086ms step_avg:156.05ms tokens_processed:1916928\n",
      "step:40/1750 train_time:6242ms step_avg:156.05ms tokens_processed:1966080\n",
      "step:41/1750 train_time:6397ms step_avg:156.02ms tokens_processed:2015232\n",
      "step:42/1750 train_time:6552ms step_avg:156.01ms tokens_processed:2064384\n",
      "step:43/1750 train_time:6707ms step_avg:155.99ms tokens_processed:2113536\n",
      "step:44/1750 train_time:6862ms step_avg:155.96ms tokens_processed:2162688\n",
      "step:45/1750 train_time:7019ms step_avg:155.98ms tokens_processed:2211840\n",
      "step:46/1750 train_time:7174ms step_avg:155.96ms tokens_processed:2260992\n",
      "step:47/1750 train_time:7329ms step_avg:155.93ms tokens_processed:2310144\n",
      "step:48/1750 train_time:7483ms step_avg:155.90ms tokens_processed:2359296\n",
      "step:49/1750 train_time:7639ms step_avg:155.89ms tokens_processed:2408448\n",
      "step:50/1750 train_time:7796ms step_avg:155.91ms tokens_processed:2457600\n",
      "step:51/1750 train_time:7950ms step_avg:155.89ms tokens_processed:2506752\n",
      "step:52/1750 train_time:8105ms step_avg:155.86ms tokens_processed:2555904\n",
      "step:53/1750 train_time:8260ms step_avg:155.86ms tokens_processed:2605056\n",
      "step:54/1750 train_time:8416ms step_avg:155.85ms tokens_processed:2654208\n",
      "step:55/1750 train_time:8570ms step_avg:155.82ms tokens_processed:2703360\n",
      "step:56/1750 train_time:8725ms step_avg:155.80ms tokens_processed:2752512\n",
      "step:57/1750 train_time:8880ms step_avg:155.80ms tokens_processed:2801664\n",
      "step:58/1750 train_time:9036ms step_avg:155.79ms tokens_processed:2850816\n",
      "step:59/1750 train_time:9192ms step_avg:155.79ms tokens_processed:2899968\n",
      "step:60/1750 train_time:9346ms step_avg:155.77ms tokens_processed:2949120\n",
      "step:61/1750 train_time:9502ms step_avg:155.77ms tokens_processed:2998272\n",
      "step:62/1750 train_time:9659ms step_avg:155.80ms tokens_processed:3047424\n",
      "step:63/1750 train_time:9813ms step_avg:155.77ms tokens_processed:3096576\n",
      "step:64/1750 train_time:9968ms step_avg:155.75ms tokens_processed:3145728\n",
      "step:65/1750 train_time:10123ms step_avg:155.74ms tokens_processed:3194880\n",
      "step:66/1750 train_time:10279ms step_avg:155.74ms tokens_processed:3244032\n",
      "step:67/1750 train_time:10434ms step_avg:155.74ms tokens_processed:3293184\n",
      "step:68/1750 train_time:10590ms step_avg:155.73ms tokens_processed:3342336\n",
      "step:69/1750 train_time:10745ms step_avg:155.72ms tokens_processed:3391488\n",
      "step:70/1750 train_time:10901ms step_avg:155.73ms tokens_processed:3440640\n",
      "step:71/1750 train_time:11056ms step_avg:155.72ms tokens_processed:3489792\n",
      "step:72/1750 train_time:11212ms step_avg:155.72ms tokens_processed:3538944\n",
      "step:73/1750 train_time:11366ms step_avg:155.70ms tokens_processed:3588096\n",
      "step:74/1750 train_time:11523ms step_avg:155.71ms tokens_processed:3637248\n",
      "step:75/1750 train_time:11678ms step_avg:155.71ms tokens_processed:3686400\n",
      "step:76/1750 train_time:11833ms step_avg:155.70ms tokens_processed:3735552\n",
      "step:77/1750 train_time:11988ms step_avg:155.68ms tokens_processed:3784704\n",
      "step:78/1750 train_time:12143ms step_avg:155.68ms tokens_processed:3833856\n",
      "step:79/1750 train_time:12299ms step_avg:155.69ms tokens_processed:3883008\n",
      "step:80/1750 train_time:12456ms step_avg:155.69ms tokens_processed:3932160\n",
      "step:81/1750 train_time:12611ms step_avg:155.69ms tokens_processed:3981312\n",
      "step:82/1750 train_time:12765ms step_avg:155.67ms tokens_processed:4030464\n",
      "step:83/1750 train_time:12920ms step_avg:155.67ms tokens_processed:4079616\n",
      "step:84/1750 train_time:13076ms step_avg:155.66ms tokens_processed:4128768\n",
      "step:85/1750 train_time:13231ms step_avg:155.66ms tokens_processed:4177920\n",
      "step:86/1750 train_time:13388ms step_avg:155.68ms tokens_processed:4227072\n",
      "step:87/1750 train_time:13543ms step_avg:155.67ms tokens_processed:4276224\n",
      "step:88/1750 train_time:13699ms step_avg:155.67ms tokens_processed:4325376\n",
      "step:89/1750 train_time:13854ms step_avg:155.67ms tokens_processed:4374528\n",
      "step:90/1750 train_time:14009ms step_avg:155.65ms tokens_processed:4423680\n",
      "step:91/1750 train_time:14163ms step_avg:155.64ms tokens_processed:4472832\n",
      "step:92/1750 train_time:14320ms step_avg:155.65ms tokens_processed:4521984\n",
      "step:93/1750 train_time:14475ms step_avg:155.65ms tokens_processed:4571136\n",
      "step:94/1750 train_time:14631ms step_avg:155.65ms tokens_processed:4620288\n",
      "step:95/1750 train_time:14787ms step_avg:155.65ms tokens_processed:4669440\n",
      "step:96/1750 train_time:14942ms step_avg:155.65ms tokens_processed:4718592\n",
      "step:97/1750 train_time:15098ms step_avg:155.65ms tokens_processed:4767744\n",
      "step:98/1750 train_time:15254ms step_avg:155.66ms tokens_processed:4816896\n",
      "step:99/1750 train_time:15410ms step_avg:155.66ms tokens_processed:4866048\n",
      "step:100/1750 train_time:15564ms step_avg:155.64ms tokens_processed:4915200\n",
      "step:101/1750 train_time:15720ms step_avg:155.64ms tokens_processed:4964352\n",
      "step:102/1750 train_time:15875ms step_avg:155.64ms tokens_processed:5013504\n",
      "step:103/1750 train_time:16031ms step_avg:155.64ms tokens_processed:5062656\n",
      "step:104/1750 train_time:16186ms step_avg:155.63ms tokens_processed:5111808\n",
      "step:105/1750 train_time:16341ms step_avg:155.63ms tokens_processed:5160960\n",
      "step:106/1750 train_time:16497ms step_avg:155.63ms tokens_processed:5210112\n",
      "step:107/1750 train_time:16652ms step_avg:155.63ms tokens_processed:5259264\n",
      "step:108/1750 train_time:16807ms step_avg:155.62ms tokens_processed:5308416\n",
      "step:109/1750 train_time:16962ms step_avg:155.61ms tokens_processed:5357568\n",
      "step:110/1750 train_time:17118ms step_avg:155.62ms tokens_processed:5406720\n",
      "step:111/1750 train_time:17273ms step_avg:155.62ms tokens_processed:5455872\n",
      "step:112/1750 train_time:17429ms step_avg:155.62ms tokens_processed:5505024\n",
      "step:113/1750 train_time:17585ms step_avg:155.62ms tokens_processed:5554176\n",
      "step:114/1750 train_time:17741ms step_avg:155.62ms tokens_processed:5603328\n",
      "step:115/1750 train_time:17897ms step_avg:155.62ms tokens_processed:5652480\n",
      "step:116/1750 train_time:18053ms step_avg:155.63ms tokens_processed:5701632\n",
      "step:117/1750 train_time:18209ms step_avg:155.63ms tokens_processed:5750784\n",
      "step:118/1750 train_time:18363ms step_avg:155.62ms tokens_processed:5799936\n",
      "step:119/1750 train_time:18520ms step_avg:155.63ms tokens_processed:5849088\n",
      "step:120/1750 train_time:18675ms step_avg:155.62ms tokens_processed:5898240\n",
      "step:121/1750 train_time:18830ms step_avg:155.62ms tokens_processed:5947392\n",
      "step:122/1750 train_time:18987ms step_avg:155.63ms tokens_processed:5996544\n",
      "step:123/1750 train_time:19143ms step_avg:155.63ms tokens_processed:6045696\n",
      "step:124/1750 train_time:19299ms step_avg:155.64ms tokens_processed:6094848\n",
      "step:125/1750 train_time:19455ms step_avg:155.64ms tokens_processed:6144000\n",
      "step:126/1750 train_time:19610ms step_avg:155.64ms tokens_processed:6193152\n",
      "step:127/1750 train_time:19765ms step_avg:155.63ms tokens_processed:6242304\n",
      "step:128/1750 train_time:19921ms step_avg:155.63ms tokens_processed:6291456\n",
      "step:129/1750 train_time:20075ms step_avg:155.62ms tokens_processed:6340608\n",
      "step:130/1750 train_time:20232ms step_avg:155.63ms tokens_processed:6389760\n",
      "step:131/1750 train_time:35464ms step_avg:270.72ms tokens_processed:6438912\n",
      "step:132/1750 train_time:35617ms step_avg:269.82ms tokens_processed:6488064\n",
      "step:133/1750 train_time:35769ms step_avg:268.94ms tokens_processed:6537216\n",
      "step:134/1750 train_time:35923ms step_avg:268.08ms tokens_processed:6586368\n",
      "step:135/1750 train_time:36080ms step_avg:267.26ms tokens_processed:6635520\n",
      "step:136/1750 train_time:36237ms step_avg:266.45ms tokens_processed:6684672\n",
      "step:137/1750 train_time:36395ms step_avg:265.66ms tokens_processed:6733824\n",
      "step:138/1750 train_time:36549ms step_avg:264.85ms tokens_processed:6782976\n",
      "step:139/1750 train_time:36703ms step_avg:264.05ms tokens_processed:6832128\n",
      "step:140/1750 train_time:36858ms step_avg:263.27ms tokens_processed:6881280\n",
      "step:141/1750 train_time:37010ms step_avg:262.48ms tokens_processed:6930432\n",
      "step:142/1750 train_time:37167ms step_avg:261.74ms tokens_processed:6979584\n",
      "step:143/1750 train_time:37324ms step_avg:261.00ms tokens_processed:7028736\n",
      "step:144/1750 train_time:37480ms step_avg:260.27ms tokens_processed:7077888\n",
      "step:145/1750 train_time:37634ms step_avg:259.55ms tokens_processed:7127040\n",
      "step:146/1750 train_time:37789ms step_avg:258.83ms tokens_processed:7176192\n",
      "step:147/1750 train_time:37942ms step_avg:258.11ms tokens_processed:7225344\n",
      "step:148/1750 train_time:38098ms step_avg:257.42ms tokens_processed:7274496\n",
      "step:149/1750 train_time:38254ms step_avg:256.74ms tokens_processed:7323648\n",
      "step:150/1750 train_time:38410ms step_avg:256.07ms tokens_processed:7372800\n",
      "step:151/1750 train_time:38565ms step_avg:255.39ms tokens_processed:7421952\n",
      "step:152/1750 train_time:38720ms step_avg:254.73ms tokens_processed:7471104\n",
      "step:153/1750 train_time:38875ms step_avg:254.09ms tokens_processed:7520256\n",
      "step:154/1750 train_time:39030ms step_avg:253.44ms tokens_processed:7569408\n",
      "step:155/1750 train_time:39185ms step_avg:252.81ms tokens_processed:7618560\n",
      "step:156/1750 train_time:39341ms step_avg:252.19ms tokens_processed:7667712\n",
      "step:157/1750 train_time:39497ms step_avg:251.57ms tokens_processed:7716864\n",
      "step:158/1750 train_time:39653ms step_avg:250.97ms tokens_processed:7766016\n",
      "step:159/1750 train_time:39808ms step_avg:250.36ms tokens_processed:7815168\n",
      "step:160/1750 train_time:39961ms step_avg:249.76ms tokens_processed:7864320\n",
      "step:161/1750 train_time:40116ms step_avg:249.17ms tokens_processed:7913472\n",
      "step:162/1750 train_time:40269ms step_avg:248.58ms tokens_processed:7962624\n",
      "step:163/1750 train_time:40425ms step_avg:248.01ms tokens_processed:8011776\n",
      "step:164/1750 train_time:40581ms step_avg:247.45ms tokens_processed:8060928\n",
      "step:165/1750 train_time:40736ms step_avg:246.89ms tokens_processed:8110080\n",
      "step:166/1750 train_time:40891ms step_avg:246.33ms tokens_processed:8159232\n",
      "step:167/1750 train_time:41045ms step_avg:245.78ms tokens_processed:8208384\n",
      "step:168/1750 train_time:41201ms step_avg:245.24ms tokens_processed:8257536\n",
      "step:169/1750 train_time:41357ms step_avg:244.71ms tokens_processed:8306688\n",
      "step:170/1750 train_time:41514ms step_avg:244.20ms tokens_processed:8355840\n",
      "step:171/1750 train_time:41666ms step_avg:243.66ms tokens_processed:8404992\n",
      "step:172/1750 train_time:41822ms step_avg:243.15ms tokens_processed:8454144\n",
      "step:173/1750 train_time:41977ms step_avg:242.64ms tokens_processed:8503296\n",
      "step:174/1750 train_time:42133ms step_avg:242.14ms tokens_processed:8552448\n",
      "step:175/1750 train_time:42288ms step_avg:241.65ms tokens_processed:8601600\n",
      "step:176/1750 train_time:42443ms step_avg:241.15ms tokens_processed:8650752\n",
      "step:177/1750 train_time:42600ms step_avg:240.68ms tokens_processed:8699904\n",
      "step:178/1750 train_time:42756ms step_avg:240.20ms tokens_processed:8749056\n",
      "step:179/1750 train_time:42911ms step_avg:239.72ms tokens_processed:8798208\n",
      "step:180/1750 train_time:43065ms step_avg:239.25ms tokens_processed:8847360\n",
      "step:181/1750 train_time:43220ms step_avg:238.79ms tokens_processed:8896512\n",
      "step:182/1750 train_time:43376ms step_avg:238.33ms tokens_processed:8945664\n",
      "step:183/1750 train_time:43531ms step_avg:237.88ms tokens_processed:8994816\n",
      "step:184/1750 train_time:43688ms step_avg:237.44ms tokens_processed:9043968\n",
      "step:185/1750 train_time:43844ms step_avg:236.99ms tokens_processed:9093120\n",
      "step:186/1750 train_time:44001ms step_avg:236.56ms tokens_processed:9142272\n",
      "step:187/1750 train_time:44156ms step_avg:236.13ms tokens_processed:9191424\n",
      "step:188/1750 train_time:44312ms step_avg:235.70ms tokens_processed:9240576\n",
      "step:189/1750 train_time:44466ms step_avg:235.27ms tokens_processed:9289728\n",
      "step:190/1750 train_time:44622ms step_avg:234.85ms tokens_processed:9338880\n",
      "step:191/1750 train_time:44777ms step_avg:234.44ms tokens_processed:9388032\n",
      "step:192/1750 train_time:44934ms step_avg:234.03ms tokens_processed:9437184\n",
      "step:193/1750 train_time:45089ms step_avg:233.62ms tokens_processed:9486336\n",
      "step:194/1750 train_time:45244ms step_avg:233.22ms tokens_processed:9535488\n",
      "step:195/1750 train_time:45400ms step_avg:232.82ms tokens_processed:9584640\n",
      "step:196/1750 train_time:45556ms step_avg:232.43ms tokens_processed:9633792\n",
      "step:197/1750 train_time:45712ms step_avg:232.04ms tokens_processed:9682944\n",
      "step:198/1750 train_time:45866ms step_avg:231.65ms tokens_processed:9732096\n",
      "step:199/1750 train_time:46023ms step_avg:231.27ms tokens_processed:9781248\n",
      "step:200/1750 train_time:46177ms step_avg:230.89ms tokens_processed:9830400\n",
      "step:201/1750 train_time:46333ms step_avg:230.51ms tokens_processed:9879552\n",
      "step:202/1750 train_time:46487ms step_avg:230.14ms tokens_processed:9928704\n",
      "step:203/1750 train_time:46643ms step_avg:229.77ms tokens_processed:9977856\n",
      "step:204/1750 train_time:46800ms step_avg:229.41ms tokens_processed:10027008\n",
      "step:205/1750 train_time:46956ms step_avg:229.05ms tokens_processed:10076160\n",
      "step:206/1750 train_time:47112ms step_avg:228.70ms tokens_processed:10125312\n",
      "step:207/1750 train_time:47266ms step_avg:228.34ms tokens_processed:10174464\n",
      "step:208/1750 train_time:47421ms step_avg:227.99ms tokens_processed:10223616\n",
      "step:209/1750 train_time:47577ms step_avg:227.64ms tokens_processed:10272768\n",
      "step:210/1750 train_time:47735ms step_avg:227.31ms tokens_processed:10321920\n",
      "step:211/1750 train_time:47890ms step_avg:226.97ms tokens_processed:10371072\n",
      "step:212/1750 train_time:48045ms step_avg:226.63ms tokens_processed:10420224\n",
      "step:213/1750 train_time:48201ms step_avg:226.30ms tokens_processed:10469376\n",
      "step:214/1750 train_time:48357ms step_avg:225.97ms tokens_processed:10518528\n",
      "step:215/1750 train_time:48514ms step_avg:225.65ms tokens_processed:10567680\n",
      "step:216/1750 train_time:48667ms step_avg:225.31ms tokens_processed:10616832\n",
      "step:217/1750 train_time:48823ms step_avg:224.99ms tokens_processed:10665984\n",
      "step:218/1750 train_time:48979ms step_avg:224.67ms tokens_processed:10715136\n",
      "step:219/1750 train_time:49134ms step_avg:224.36ms tokens_processed:10764288\n",
      "step:220/1750 train_time:49290ms step_avg:224.05ms tokens_processed:10813440\n",
      "step:221/1750 train_time:49447ms step_avg:223.74ms tokens_processed:10862592\n",
      "step:222/1750 train_time:49603ms step_avg:223.44ms tokens_processed:10911744\n",
      "step:223/1750 train_time:49759ms step_avg:223.13ms tokens_processed:10960896\n",
      "step:224/1750 train_time:49915ms step_avg:222.84ms tokens_processed:11010048\n",
      "step:225/1750 train_time:50069ms step_avg:222.53ms tokens_processed:11059200\n",
      "step:226/1750 train_time:50225ms step_avg:222.23ms tokens_processed:11108352\n",
      "step:227/1750 train_time:50381ms step_avg:221.94ms tokens_processed:11157504\n",
      "step:228/1750 train_time:50537ms step_avg:221.65ms tokens_processed:11206656\n",
      "step:229/1750 train_time:50693ms step_avg:221.37ms tokens_processed:11255808\n",
      "step:230/1750 train_time:50849ms step_avg:221.08ms tokens_processed:11304960\n",
      "step:231/1750 train_time:51004ms step_avg:220.80ms tokens_processed:11354112\n",
      "step:232/1750 train_time:51163ms step_avg:220.53ms tokens_processed:11403264\n",
      "step:233/1750 train_time:51317ms step_avg:220.25ms tokens_processed:11452416\n",
      "step:234/1750 train_time:51472ms step_avg:219.97ms tokens_processed:11501568\n",
      "step:235/1750 train_time:51626ms step_avg:219.69ms tokens_processed:11550720\n",
      "step:236/1750 train_time:51783ms step_avg:219.42ms tokens_processed:11599872\n",
      "step:237/1750 train_time:51939ms step_avg:219.15ms tokens_processed:11649024\n",
      "step:238/1750 train_time:52097ms step_avg:218.89ms tokens_processed:11698176\n",
      "step:239/1750 train_time:52253ms step_avg:218.63ms tokens_processed:11747328\n",
      "step:240/1750 train_time:52408ms step_avg:218.37ms tokens_processed:11796480\n",
      "step:241/1750 train_time:52563ms step_avg:218.10ms tokens_processed:11845632\n",
      "step:242/1750 train_time:52718ms step_avg:217.84ms tokens_processed:11894784\n",
      "step:243/1750 train_time:52873ms step_avg:217.59ms tokens_processed:11943936\n",
      "step:244/1750 train_time:53029ms step_avg:217.33ms tokens_processed:11993088\n",
      "step:245/1750 train_time:53186ms step_avg:217.08ms tokens_processed:12042240\n",
      "step:246/1750 train_time:53342ms step_avg:216.84ms tokens_processed:12091392\n",
      "step:247/1750 train_time:53497ms step_avg:216.59ms tokens_processed:12140544\n",
      "step:248/1750 train_time:53654ms step_avg:216.35ms tokens_processed:12189696\n",
      "step:249/1750 train_time:53808ms step_avg:216.10ms tokens_processed:12238848\n",
      "step:250/1750 train_time:53964ms step_avg:215.86ms tokens_processed:12288000\n",
      "step:251/1750 train_time:54120ms step_avg:215.62ms tokens_processed:12337152\n",
      "step:252/1750 train_time:54277ms step_avg:215.39ms tokens_processed:12386304\n",
      "step:253/1750 train_time:54433ms step_avg:215.15ms tokens_processed:12435456\n",
      "step:254/1750 train_time:54589ms step_avg:214.92ms tokens_processed:12484608\n",
      "step:255/1750 train_time:54744ms step_avg:214.68ms tokens_processed:12533760\n",
      "step:256/1750 train_time:54900ms step_avg:214.45ms tokens_processed:12582912\n",
      "step:257/1750 train_time:55055ms step_avg:214.22ms tokens_processed:12632064\n",
      "step:258/1750 train_time:55212ms step_avg:214.00ms tokens_processed:12681216\n",
      "step:259/1750 train_time:55366ms step_avg:213.77ms tokens_processed:12730368\n",
      "step:260/1750 train_time:55522ms step_avg:213.55ms tokens_processed:12779520\n",
      "step:261/1750 train_time:70920ms step_avg:271.72ms tokens_processed:12828672\n",
      "step:262/1750 train_time:71072ms step_avg:271.27ms tokens_processed:12877824\n",
      "step:263/1750 train_time:71226ms step_avg:270.82ms tokens_processed:12926976\n",
      "step:264/1750 train_time:71379ms step_avg:270.38ms tokens_processed:12976128\n",
      "step:265/1750 train_time:71539ms step_avg:269.96ms tokens_processed:13025280\n",
      "step:266/1750 train_time:71698ms step_avg:269.54ms tokens_processed:13074432\n",
      "step:267/1750 train_time:71856ms step_avg:269.12ms tokens_processed:13123584\n",
      "step:268/1750 train_time:72011ms step_avg:268.70ms tokens_processed:13172736\n",
      "step:269/1750 train_time:72163ms step_avg:268.26ms tokens_processed:13221888\n",
      "step:270/1750 train_time:72319ms step_avg:267.85ms tokens_processed:13271040\n",
      "step:271/1750 train_time:72474ms step_avg:267.43ms tokens_processed:13320192\n",
      "step:272/1750 train_time:72630ms step_avg:267.02ms tokens_processed:13369344\n",
      "step:273/1750 train_time:72788ms step_avg:266.62ms tokens_processed:13418496\n",
      "step:274/1750 train_time:72944ms step_avg:266.22ms tokens_processed:13467648\n",
      "step:275/1750 train_time:73101ms step_avg:265.82ms tokens_processed:13516800\n",
      "step:276/1750 train_time:73256ms step_avg:265.42ms tokens_processed:13565952\n",
      "step:277/1750 train_time:73412ms step_avg:265.02ms tokens_processed:13615104\n",
      "step:278/1750 train_time:73567ms step_avg:264.63ms tokens_processed:13664256\n",
      "step:279/1750 train_time:73724ms step_avg:264.24ms tokens_processed:13713408\n",
      "step:280/1750 train_time:73880ms step_avg:263.86ms tokens_processed:13762560\n",
      "step:281/1750 train_time:74036ms step_avg:263.47ms tokens_processed:13811712\n",
      "step:282/1750 train_time:74192ms step_avg:263.09ms tokens_processed:13860864\n",
      "step:283/1750 train_time:74348ms step_avg:262.71ms tokens_processed:13910016\n",
      "step:284/1750 train_time:74504ms step_avg:262.34ms tokens_processed:13959168\n",
      "step:285/1750 train_time:74660ms step_avg:261.96ms tokens_processed:14008320\n",
      "step:286/1750 train_time:74816ms step_avg:261.60ms tokens_processed:14057472\n",
      "step:287/1750 train_time:74972ms step_avg:261.23ms tokens_processed:14106624\n",
      "step:288/1750 train_time:75129ms step_avg:260.86ms tokens_processed:14155776\n",
      "step:289/1750 train_time:75284ms step_avg:260.50ms tokens_processed:14204928\n",
      "step:290/1750 train_time:75439ms step_avg:260.13ms tokens_processed:14254080\n",
      "step:291/1750 train_time:75596ms step_avg:259.78ms tokens_processed:14303232\n",
      "step:292/1750 train_time:75753ms step_avg:259.43ms tokens_processed:14352384\n",
      "step:293/1750 train_time:75908ms step_avg:259.07ms tokens_processed:14401536\n",
      "step:294/1750 train_time:76063ms step_avg:258.72ms tokens_processed:14450688\n",
      "step:295/1750 train_time:76218ms step_avg:258.36ms tokens_processed:14499840\n",
      "step:296/1750 train_time:76372ms step_avg:258.01ms tokens_processed:14548992\n",
      "step:297/1750 train_time:76526ms step_avg:257.66ms tokens_processed:14598144\n",
      "step:298/1750 train_time:76682ms step_avg:257.32ms tokens_processed:14647296\n",
      "step:299/1750 train_time:76838ms step_avg:256.98ms tokens_processed:14696448\n",
      "step:300/1750 train_time:76993ms step_avg:256.64ms tokens_processed:14745600\n",
      "step:301/1750 train_time:77150ms step_avg:256.31ms tokens_processed:14794752\n",
      "step:302/1750 train_time:77304ms step_avg:255.97ms tokens_processed:14843904\n",
      "step:303/1750 train_time:77459ms step_avg:255.64ms tokens_processed:14893056\n",
      "step:304/1750 train_time:77614ms step_avg:255.31ms tokens_processed:14942208\n",
      "step:305/1750 train_time:77769ms step_avg:254.98ms tokens_processed:14991360\n",
      "step:306/1750 train_time:77925ms step_avg:254.66ms tokens_processed:15040512\n",
      "step:307/1750 train_time:78080ms step_avg:254.33ms tokens_processed:15089664\n",
      "step:308/1750 train_time:78236ms step_avg:254.01ms tokens_processed:15138816\n",
      "step:309/1750 train_time:78391ms step_avg:253.69ms tokens_processed:15187968\n",
      "step:310/1750 train_time:78547ms step_avg:253.38ms tokens_processed:15237120\n",
      "step:311/1750 train_time:78702ms step_avg:253.06ms tokens_processed:15286272\n",
      "step:312/1750 train_time:78858ms step_avg:252.75ms tokens_processed:15335424\n",
      "step:313/1750 train_time:79013ms step_avg:252.44ms tokens_processed:15384576\n",
      "step:314/1750 train_time:79168ms step_avg:252.13ms tokens_processed:15433728\n",
      "step:315/1750 train_time:79323ms step_avg:251.82ms tokens_processed:15482880\n",
      "step:316/1750 train_time:79478ms step_avg:251.51ms tokens_processed:15532032\n",
      "step:317/1750 train_time:79633ms step_avg:251.21ms tokens_processed:15581184\n",
      "step:318/1750 train_time:79789ms step_avg:250.91ms tokens_processed:15630336\n",
      "step:319/1750 train_time:79944ms step_avg:250.61ms tokens_processed:15679488\n",
      "step:320/1750 train_time:80100ms step_avg:250.31ms tokens_processed:15728640\n",
      "step:321/1750 train_time:80257ms step_avg:250.02ms tokens_processed:15777792\n",
      "step:322/1750 train_time:80412ms step_avg:249.73ms tokens_processed:15826944\n",
      "step:323/1750 train_time:80567ms step_avg:249.43ms tokens_processed:15876096\n",
      "step:324/1750 train_time:80722ms step_avg:249.14ms tokens_processed:15925248\n",
      "step:325/1750 train_time:80878ms step_avg:248.85ms tokens_processed:15974400\n",
      "step:326/1750 train_time:81033ms step_avg:248.57ms tokens_processed:16023552\n",
      "step:327/1750 train_time:81188ms step_avg:248.28ms tokens_processed:16072704\n",
      "step:328/1750 train_time:81344ms step_avg:248.00ms tokens_processed:16121856\n",
      "step:329/1750 train_time:81500ms step_avg:247.72ms tokens_processed:16171008\n",
      "step:330/1750 train_time:81656ms step_avg:247.44ms tokens_processed:16220160\n",
      "step:331/1750 train_time:81810ms step_avg:247.16ms tokens_processed:16269312\n",
      "step:332/1750 train_time:81965ms step_avg:246.88ms tokens_processed:16318464\n",
      "step:333/1750 train_time:82121ms step_avg:246.61ms tokens_processed:16367616\n",
      "step:334/1750 train_time:82277ms step_avg:246.34ms tokens_processed:16416768\n",
      "step:335/1750 train_time:82432ms step_avg:246.07ms tokens_processed:16465920\n",
      "step:336/1750 train_time:82588ms step_avg:245.80ms tokens_processed:16515072\n",
      "step:337/1750 train_time:82743ms step_avg:245.53ms tokens_processed:16564224\n",
      "step:338/1750 train_time:82899ms step_avg:245.26ms tokens_processed:16613376\n",
      "step:339/1750 train_time:83055ms step_avg:245.00ms tokens_processed:16662528\n",
      "step:340/1750 train_time:83210ms step_avg:244.73ms tokens_processed:16711680\n",
      "step:341/1750 train_time:83364ms step_avg:244.47ms tokens_processed:16760832\n",
      "step:342/1750 train_time:83520ms step_avg:244.21ms tokens_processed:16809984\n",
      "step:343/1750 train_time:83675ms step_avg:243.95ms tokens_processed:16859136\n",
      "step:344/1750 train_time:83830ms step_avg:243.69ms tokens_processed:16908288\n",
      "step:345/1750 train_time:83986ms step_avg:243.44ms tokens_processed:16957440\n",
      "step:346/1750 train_time:84142ms step_avg:243.18ms tokens_processed:17006592\n",
      "step:347/1750 train_time:84298ms step_avg:242.93ms tokens_processed:17055744\n",
      "step:348/1750 train_time:84454ms step_avg:242.68ms tokens_processed:17104896\n",
      "step:349/1750 train_time:84609ms step_avg:242.43ms tokens_processed:17154048\n",
      "step:350/1750 train_time:84764ms step_avg:242.18ms tokens_processed:17203200\n",
      "---validation---\n",
      "step:350/1750 val_loss:4.6921 train_time:84772ms step_avg:242.21ms\n",
      "---end of validation---\n",
      "step:351/1750 train_time:84921ms step_avg:241.94ms tokens_processed:17252352\n",
      "step:352/1750 train_time:85076ms step_avg:241.69ms tokens_processed:17301504\n",
      "step:353/1750 train_time:85231ms step_avg:241.45ms tokens_processed:17350656\n",
      "step:354/1750 train_time:85386ms step_avg:241.20ms tokens_processed:17399808\n",
      "step:355/1750 train_time:85541ms step_avg:240.96ms tokens_processed:17448960\n",
      "step:356/1750 train_time:85700ms step_avg:240.73ms tokens_processed:17498112\n",
      "step:357/1750 train_time:85856ms step_avg:240.49ms tokens_processed:17547264\n",
      "step:358/1750 train_time:86013ms step_avg:240.26ms tokens_processed:17596416\n",
      "step:359/1750 train_time:86168ms step_avg:240.02ms tokens_processed:17645568\n",
      "step:360/1750 train_time:86323ms step_avg:239.79ms tokens_processed:17694720\n",
      "step:361/1750 train_time:86478ms step_avg:239.55ms tokens_processed:17743872\n",
      "step:362/1750 train_time:86634ms step_avg:239.32ms tokens_processed:17793024\n",
      "step:363/1750 train_time:86791ms step_avg:239.09ms tokens_processed:17842176\n",
      "step:364/1750 train_time:86947ms step_avg:238.86ms tokens_processed:17891328\n",
      "step:365/1750 train_time:87103ms step_avg:238.64ms tokens_processed:17940480\n",
      "step:366/1750 train_time:87258ms step_avg:238.41ms tokens_processed:17989632\n",
      "step:367/1750 train_time:87415ms step_avg:238.19ms tokens_processed:18038784\n",
      "step:368/1750 train_time:87569ms step_avg:237.96ms tokens_processed:18087936\n",
      "step:369/1750 train_time:87725ms step_avg:237.74ms tokens_processed:18137088\n",
      "step:370/1750 train_time:87881ms step_avg:237.52ms tokens_processed:18186240\n",
      "step:371/1750 train_time:88038ms step_avg:237.30ms tokens_processed:18235392\n",
      "step:372/1750 train_time:88193ms step_avg:237.08ms tokens_processed:18284544\n",
      "step:373/1750 train_time:88348ms step_avg:236.86ms tokens_processed:18333696\n",
      "step:374/1750 train_time:88504ms step_avg:236.64ms tokens_processed:18382848\n",
      "step:375/1750 train_time:88660ms step_avg:236.43ms tokens_processed:18432000\n",
      "step:376/1750 train_time:88816ms step_avg:236.21ms tokens_processed:18481152\n",
      "step:377/1750 train_time:88973ms step_avg:236.00ms tokens_processed:18530304\n",
      "step:378/1750 train_time:89129ms step_avg:235.79ms tokens_processed:18579456\n",
      "step:379/1750 train_time:89285ms step_avg:235.58ms tokens_processed:18628608\n",
      "step:380/1750 train_time:89442ms step_avg:235.37ms tokens_processed:18677760\n",
      "step:381/1750 train_time:89598ms step_avg:235.16ms tokens_processed:18726912\n",
      "step:382/1750 train_time:89752ms step_avg:234.95ms tokens_processed:18776064\n",
      "step:383/1750 train_time:89908ms step_avg:234.75ms tokens_processed:18825216\n",
      "step:384/1750 train_time:90064ms step_avg:234.54ms tokens_processed:18874368\n",
      "step:385/1750 train_time:90220ms step_avg:234.34ms tokens_processed:18923520\n",
      "step:386/1750 train_time:90378ms step_avg:234.14ms tokens_processed:18972672\n",
      "step:387/1750 train_time:90534ms step_avg:233.94ms tokens_processed:19021824\n",
      "step:388/1750 train_time:90689ms step_avg:233.73ms tokens_processed:19070976\n",
      "step:389/1750 train_time:90845ms step_avg:233.54ms tokens_processed:19120128\n",
      "step:390/1750 train_time:105941ms step_avg:271.64ms tokens_processed:19169280\n",
      "step:391/1750 train_time:106096ms step_avg:271.34ms tokens_processed:19218432\n",
      "step:392/1750 train_time:106249ms step_avg:271.04ms tokens_processed:19267584\n",
      "step:393/1750 train_time:106404ms step_avg:270.75ms tokens_processed:19316736\n",
      "step:394/1750 train_time:106563ms step_avg:270.47ms tokens_processed:19365888\n",
      "step:395/1750 train_time:106722ms step_avg:270.18ms tokens_processed:19415040\n",
      "step:396/1750 train_time:106880ms step_avg:269.90ms tokens_processed:19464192\n",
      "step:397/1750 train_time:107037ms step_avg:269.61ms tokens_processed:19513344\n",
      "step:398/1750 train_time:107191ms step_avg:269.32ms tokens_processed:19562496\n",
      "step:399/1750 train_time:107345ms step_avg:269.03ms tokens_processed:19611648\n",
      "step:400/1750 train_time:107501ms step_avg:268.75ms tokens_processed:19660800\n",
      "step:401/1750 train_time:107659ms step_avg:268.48ms tokens_processed:19709952\n",
      "step:402/1750 train_time:107817ms step_avg:268.20ms tokens_processed:19759104\n",
      "step:403/1750 train_time:107975ms step_avg:267.93ms tokens_processed:19808256\n",
      "step:404/1750 train_time:108129ms step_avg:267.65ms tokens_processed:19857408\n",
      "step:405/1750 train_time:108287ms step_avg:267.37ms tokens_processed:19906560\n",
      "step:406/1750 train_time:108443ms step_avg:267.10ms tokens_processed:19955712\n",
      "step:407/1750 train_time:108600ms step_avg:266.83ms tokens_processed:20004864\n",
      "step:408/1750 train_time:108757ms step_avg:266.56ms tokens_processed:20054016\n",
      "step:409/1750 train_time:108914ms step_avg:266.29ms tokens_processed:20103168\n",
      "step:410/1750 train_time:109071ms step_avg:266.03ms tokens_processed:20152320\n",
      "step:411/1750 train_time:109227ms step_avg:265.76ms tokens_processed:20201472\n",
      "step:412/1750 train_time:109384ms step_avg:265.50ms tokens_processed:20250624\n",
      "step:413/1750 train_time:109541ms step_avg:265.23ms tokens_processed:20299776\n",
      "step:414/1750 train_time:109699ms step_avg:264.97ms tokens_processed:20348928\n",
      "step:415/1750 train_time:109855ms step_avg:264.71ms tokens_processed:20398080\n",
      "step:416/1750 train_time:110011ms step_avg:264.45ms tokens_processed:20447232\n",
      "step:417/1750 train_time:110167ms step_avg:264.19ms tokens_processed:20496384\n",
      "step:418/1750 train_time:110324ms step_avg:263.93ms tokens_processed:20545536\n",
      "step:419/1750 train_time:110483ms step_avg:263.68ms tokens_processed:20594688\n",
      "step:420/1750 train_time:110642ms step_avg:263.43ms tokens_processed:20643840\n",
      "step:421/1750 train_time:110799ms step_avg:263.18ms tokens_processed:20692992\n",
      "step:422/1750 train_time:110954ms step_avg:262.92ms tokens_processed:20742144\n",
      "step:423/1750 train_time:111110ms step_avg:262.67ms tokens_processed:20791296\n",
      "step:424/1750 train_time:111264ms step_avg:262.42ms tokens_processed:20840448\n",
      "step:425/1750 train_time:111421ms step_avg:262.17ms tokens_processed:20889600\n",
      "step:426/1750 train_time:111579ms step_avg:261.92ms tokens_processed:20938752\n",
      "step:427/1750 train_time:111736ms step_avg:261.68ms tokens_processed:20987904\n",
      "step:428/1750 train_time:111891ms step_avg:261.43ms tokens_processed:21037056\n",
      "step:429/1750 train_time:112046ms step_avg:261.18ms tokens_processed:21086208\n",
      "step:430/1750 train_time:112202ms step_avg:260.93ms tokens_processed:21135360\n",
      "step:431/1750 train_time:112356ms step_avg:260.69ms tokens_processed:21184512\n",
      "step:432/1750 train_time:112512ms step_avg:260.44ms tokens_processed:21233664\n",
      "step:433/1750 train_time:112669ms step_avg:260.21ms tokens_processed:21282816\n",
      "step:434/1750 train_time:112825ms step_avg:259.97ms tokens_processed:21331968\n",
      "step:435/1750 train_time:112982ms step_avg:259.73ms tokens_processed:21381120\n",
      "step:436/1750 train_time:113138ms step_avg:259.49ms tokens_processed:21430272\n",
      "step:437/1750 train_time:113294ms step_avg:259.25ms tokens_processed:21479424\n",
      "step:438/1750 train_time:113449ms step_avg:259.01ms tokens_processed:21528576\n",
      "step:439/1750 train_time:113605ms step_avg:258.78ms tokens_processed:21577728\n",
      "step:440/1750 train_time:113761ms step_avg:258.55ms tokens_processed:21626880\n",
      "step:441/1750 train_time:113918ms step_avg:258.32ms tokens_processed:21676032\n",
      "step:442/1750 train_time:114074ms step_avg:258.09ms tokens_processed:21725184\n",
      "step:443/1750 train_time:114230ms step_avg:257.85ms tokens_processed:21774336\n",
      "step:444/1750 train_time:114385ms step_avg:257.62ms tokens_processed:21823488\n",
      "step:445/1750 train_time:114541ms step_avg:257.40ms tokens_processed:21872640\n",
      "step:446/1750 train_time:114698ms step_avg:257.17ms tokens_processed:21921792\n",
      "step:447/1750 train_time:114853ms step_avg:256.94ms tokens_processed:21970944\n",
      "step:448/1750 train_time:115010ms step_avg:256.72ms tokens_processed:22020096\n",
      "step:449/1750 train_time:115166ms step_avg:256.50ms tokens_processed:22069248\n",
      "step:450/1750 train_time:115323ms step_avg:256.27ms tokens_processed:22118400\n",
      "step:451/1750 train_time:115480ms step_avg:256.05ms tokens_processed:22167552\n",
      "step:452/1750 train_time:115637ms step_avg:255.83ms tokens_processed:22216704\n",
      "step:453/1750 train_time:115793ms step_avg:255.61ms tokens_processed:22265856\n",
      "step:454/1750 train_time:115948ms step_avg:255.39ms tokens_processed:22315008\n",
      "step:455/1750 train_time:116105ms step_avg:255.18ms tokens_processed:22364160\n",
      "step:456/1750 train_time:116261ms step_avg:254.96ms tokens_processed:22413312\n",
      "step:457/1750 train_time:116417ms step_avg:254.74ms tokens_processed:22462464\n",
      "step:458/1750 train_time:116574ms step_avg:254.53ms tokens_processed:22511616\n",
      "step:459/1750 train_time:116731ms step_avg:254.32ms tokens_processed:22560768\n",
      "step:460/1750 train_time:116886ms step_avg:254.10ms tokens_processed:22609920\n",
      "step:461/1750 train_time:117043ms step_avg:253.89ms tokens_processed:22659072\n",
      "step:462/1750 train_time:117198ms step_avg:253.68ms tokens_processed:22708224\n",
      "step:463/1750 train_time:117353ms step_avg:253.46ms tokens_processed:22757376\n",
      "step:464/1750 train_time:117510ms step_avg:253.25ms tokens_processed:22806528\n",
      "step:465/1750 train_time:117666ms step_avg:253.05ms tokens_processed:22855680\n",
      "step:466/1750 train_time:117824ms step_avg:252.84ms tokens_processed:22904832\n",
      "step:467/1750 train_time:117981ms step_avg:252.64ms tokens_processed:22953984\n",
      "step:468/1750 train_time:118140ms step_avg:252.44ms tokens_processed:23003136\n",
      "step:469/1750 train_time:118295ms step_avg:252.23ms tokens_processed:23052288\n",
      "step:470/1750 train_time:118450ms step_avg:252.02ms tokens_processed:23101440\n",
      "step:471/1750 train_time:118606ms step_avg:251.82ms tokens_processed:23150592\n",
      "step:472/1750 train_time:118762ms step_avg:251.62ms tokens_processed:23199744\n",
      "step:473/1750 train_time:118919ms step_avg:251.41ms tokens_processed:23248896\n",
      "step:474/1750 train_time:119076ms step_avg:251.21ms tokens_processed:23298048\n",
      "step:475/1750 train_time:119232ms step_avg:251.02ms tokens_processed:23347200\n",
      "step:476/1750 train_time:119389ms step_avg:250.82ms tokens_processed:23396352\n",
      "step:477/1750 train_time:119546ms step_avg:250.62ms tokens_processed:23445504\n",
      "step:478/1750 train_time:119703ms step_avg:250.42ms tokens_processed:23494656\n",
      "step:479/1750 train_time:119859ms step_avg:250.23ms tokens_processed:23543808\n",
      "step:480/1750 train_time:120015ms step_avg:250.03ms tokens_processed:23592960\n",
      "step:481/1750 train_time:120172ms step_avg:249.84ms tokens_processed:23642112\n",
      "step:482/1750 train_time:120328ms step_avg:249.64ms tokens_processed:23691264\n",
      "step:483/1750 train_time:120485ms step_avg:249.45ms tokens_processed:23740416\n",
      "step:484/1750 train_time:120641ms step_avg:249.26ms tokens_processed:23789568\n",
      "step:485/1750 train_time:120798ms step_avg:249.07ms tokens_processed:23838720\n",
      "step:486/1750 train_time:120953ms step_avg:248.87ms tokens_processed:23887872\n",
      "step:487/1750 train_time:121109ms step_avg:248.68ms tokens_processed:23937024\n",
      "step:488/1750 train_time:121266ms step_avg:248.50ms tokens_processed:23986176\n",
      "step:489/1750 train_time:121421ms step_avg:248.31ms tokens_processed:24035328\n",
      "step:490/1750 train_time:121580ms step_avg:248.12ms tokens_processed:24084480\n",
      "step:491/1750 train_time:121737ms step_avg:247.94ms tokens_processed:24133632\n",
      "step:492/1750 train_time:121893ms step_avg:247.75ms tokens_processed:24182784\n",
      "step:493/1750 train_time:122048ms step_avg:247.56ms tokens_processed:24231936\n",
      "step:494/1750 train_time:122205ms step_avg:247.38ms tokens_processed:24281088\n",
      "step:495/1750 train_time:122361ms step_avg:247.19ms tokens_processed:24330240\n",
      "step:496/1750 train_time:122518ms step_avg:247.01ms tokens_processed:24379392\n",
      "step:497/1750 train_time:122676ms step_avg:246.83ms tokens_processed:24428544\n",
      "step:498/1750 train_time:122833ms step_avg:246.65ms tokens_processed:24477696\n",
      "step:499/1750 train_time:122989ms step_avg:246.47ms tokens_processed:24526848\n",
      "step:500/1750 train_time:123146ms step_avg:246.29ms tokens_processed:24576000\n",
      "step:501/1750 train_time:123303ms step_avg:246.11ms tokens_processed:24625152\n",
      "step:502/1750 train_time:123459ms step_avg:245.93ms tokens_processed:24674304\n",
      "step:503/1750 train_time:123616ms step_avg:245.76ms tokens_processed:24723456\n",
      "step:504/1750 train_time:123773ms step_avg:245.58ms tokens_processed:24772608\n",
      "step:505/1750 train_time:123929ms step_avg:245.40ms tokens_processed:24821760\n",
      "step:506/1750 train_time:124087ms step_avg:245.23ms tokens_processed:24870912\n",
      "step:507/1750 train_time:124243ms step_avg:245.06ms tokens_processed:24920064\n",
      "step:508/1750 train_time:124401ms step_avg:244.88ms tokens_processed:24969216\n",
      "step:509/1750 train_time:124557ms step_avg:244.71ms tokens_processed:25018368\n",
      "step:510/1750 train_time:124713ms step_avg:244.53ms tokens_processed:25067520\n",
      "step:511/1750 train_time:124869ms step_avg:244.36ms tokens_processed:25116672\n",
      "step:512/1750 train_time:125026ms step_avg:244.19ms tokens_processed:25165824\n",
      "step:513/1750 train_time:125185ms step_avg:244.02ms tokens_processed:25214976\n",
      "step:514/1750 train_time:125342ms step_avg:243.86ms tokens_processed:25264128\n",
      "step:515/1750 train_time:125500ms step_avg:243.69ms tokens_processed:25313280\n",
      "step:516/1750 train_time:125655ms step_avg:243.52ms tokens_processed:25362432\n",
      "step:517/1750 train_time:125811ms step_avg:243.35ms tokens_processed:25411584\n",
      "step:518/1750 train_time:125967ms step_avg:243.18ms tokens_processed:25460736\n",
      "step:519/1750 train_time:126124ms step_avg:243.01ms tokens_processed:25509888\n",
      "step:520/1750 train_time:142058ms step_avg:273.19ms tokens_processed:25559040\n",
      "step:521/1750 train_time:142215ms step_avg:272.97ms tokens_processed:25608192\n",
      "step:522/1750 train_time:142369ms step_avg:272.74ms tokens_processed:25657344\n",
      "step:523/1750 train_time:142525ms step_avg:272.52ms tokens_processed:25706496\n",
      "step:524/1750 train_time:142687ms step_avg:272.30ms tokens_processed:25755648\n",
      "step:525/1750 train_time:142848ms step_avg:272.09ms tokens_processed:25804800\n",
      "step:526/1750 train_time:143008ms step_avg:271.88ms tokens_processed:25853952\n",
      "step:527/1750 train_time:143164ms step_avg:271.66ms tokens_processed:25903104\n",
      "step:528/1750 train_time:143321ms step_avg:271.44ms tokens_processed:25952256\n",
      "step:529/1750 train_time:143475ms step_avg:271.22ms tokens_processed:26001408\n",
      "step:530/1750 train_time:143633ms step_avg:271.01ms tokens_processed:26050560\n",
      "step:531/1750 train_time:143793ms step_avg:270.80ms tokens_processed:26099712\n",
      "step:532/1750 train_time:143953ms step_avg:270.59ms tokens_processed:26148864\n",
      "step:533/1750 train_time:144111ms step_avg:270.38ms tokens_processed:26198016\n",
      "step:534/1750 train_time:144267ms step_avg:270.16ms tokens_processed:26247168\n",
      "step:535/1750 train_time:144424ms step_avg:269.95ms tokens_processed:26296320\n",
      "step:536/1750 train_time:144583ms step_avg:269.74ms tokens_processed:26345472\n",
      "step:537/1750 train_time:144743ms step_avg:269.54ms tokens_processed:26394624\n",
      "step:538/1750 train_time:144902ms step_avg:269.33ms tokens_processed:26443776\n",
      "step:539/1750 train_time:145059ms step_avg:269.13ms tokens_processed:26492928\n",
      "step:540/1750 train_time:145216ms step_avg:268.92ms tokens_processed:26542080\n",
      "step:541/1750 train_time:145372ms step_avg:268.71ms tokens_processed:26591232\n",
      "step:542/1750 train_time:145529ms step_avg:268.50ms tokens_processed:26640384\n",
      "step:543/1750 train_time:145690ms step_avg:268.31ms tokens_processed:26689536\n",
      "step:544/1750 train_time:145850ms step_avg:268.11ms tokens_processed:26738688\n",
      "step:545/1750 train_time:146010ms step_avg:267.91ms tokens_processed:26787840\n",
      "step:546/1750 train_time:146167ms step_avg:267.71ms tokens_processed:26836992\n",
      "step:547/1750 train_time:146324ms step_avg:267.50ms tokens_processed:26886144\n",
      "step:548/1750 train_time:146482ms step_avg:267.30ms tokens_processed:26935296\n",
      "step:549/1750 train_time:146642ms step_avg:267.11ms tokens_processed:26984448\n",
      "step:550/1750 train_time:146800ms step_avg:266.91ms tokens_processed:27033600\n",
      "step:551/1750 train_time:146957ms step_avg:266.71ms tokens_processed:27082752\n",
      "step:552/1750 train_time:147115ms step_avg:266.51ms tokens_processed:27131904\n",
      "step:553/1750 train_time:147273ms step_avg:266.32ms tokens_processed:27181056\n",
      "step:554/1750 train_time:147430ms step_avg:266.12ms tokens_processed:27230208\n",
      "step:555/1750 train_time:147589ms step_avg:265.93ms tokens_processed:27279360\n",
      "step:556/1750 train_time:147750ms step_avg:265.74ms tokens_processed:27328512\n",
      "step:557/1750 train_time:147909ms step_avg:265.55ms tokens_processed:27377664\n",
      "step:558/1750 train_time:148067ms step_avg:265.35ms tokens_processed:27426816\n",
      "step:559/1750 train_time:148224ms step_avg:265.16ms tokens_processed:27475968\n",
      "step:560/1750 train_time:148381ms step_avg:264.97ms tokens_processed:27525120\n",
      "step:561/1750 train_time:148538ms step_avg:264.77ms tokens_processed:27574272\n",
      "step:562/1750 train_time:148696ms step_avg:264.58ms tokens_processed:27623424\n",
      "step:563/1750 train_time:148853ms step_avg:264.39ms tokens_processed:27672576\n",
      "step:564/1750 train_time:149010ms step_avg:264.20ms tokens_processed:27721728\n",
      "step:565/1750 train_time:149168ms step_avg:264.01ms tokens_processed:27770880\n",
      "step:566/1750 train_time:149325ms step_avg:263.82ms tokens_processed:27820032\n",
      "step:567/1750 train_time:149482ms step_avg:263.64ms tokens_processed:27869184\n",
      "step:568/1750 train_time:149640ms step_avg:263.45ms tokens_processed:27918336\n",
      "step:569/1750 train_time:149797ms step_avg:263.26ms tokens_processed:27967488\n",
      "step:570/1750 train_time:149953ms step_avg:263.08ms tokens_processed:28016640\n",
      "step:571/1750 train_time:150110ms step_avg:262.89ms tokens_processed:28065792\n",
      "step:572/1750 train_time:150268ms step_avg:262.71ms tokens_processed:28114944\n",
      "step:573/1750 train_time:150425ms step_avg:262.52ms tokens_processed:28164096\n",
      "step:574/1750 train_time:150584ms step_avg:262.34ms tokens_processed:28213248\n",
      "step:575/1750 train_time:150742ms step_avg:262.16ms tokens_processed:28262400\n",
      "step:576/1750 train_time:150900ms step_avg:261.98ms tokens_processed:28311552\n",
      "step:577/1750 train_time:151055ms step_avg:261.79ms tokens_processed:28360704\n",
      "step:578/1750 train_time:151213ms step_avg:261.61ms tokens_processed:28409856\n",
      "step:579/1750 train_time:151370ms step_avg:261.43ms tokens_processed:28459008\n",
      "step:580/1750 train_time:151527ms step_avg:261.25ms tokens_processed:28508160\n",
      "step:581/1750 train_time:151687ms step_avg:261.08ms tokens_processed:28557312\n",
      "step:582/1750 train_time:151845ms step_avg:260.90ms tokens_processed:28606464\n",
      "step:583/1750 train_time:152003ms step_avg:260.73ms tokens_processed:28655616\n",
      "step:584/1750 train_time:152159ms step_avg:260.55ms tokens_processed:28704768\n",
      "step:585/1750 train_time:152315ms step_avg:260.37ms tokens_processed:28753920\n",
      "step:586/1750 train_time:152472ms step_avg:260.19ms tokens_processed:28803072\n",
      "step:587/1750 train_time:152629ms step_avg:260.02ms tokens_processed:28852224\n",
      "step:588/1750 train_time:152790ms step_avg:259.85ms tokens_processed:28901376\n",
      "step:589/1750 train_time:152947ms step_avg:259.67ms tokens_processed:28950528\n",
      "step:590/1750 train_time:153107ms step_avg:259.50ms tokens_processed:28999680\n",
      "step:591/1750 train_time:153263ms step_avg:259.33ms tokens_processed:29048832\n",
      "step:592/1750 train_time:153420ms step_avg:259.16ms tokens_processed:29097984\n",
      "step:593/1750 train_time:153577ms step_avg:258.98ms tokens_processed:29147136\n",
      "step:594/1750 train_time:153735ms step_avg:258.81ms tokens_processed:29196288\n",
      "step:595/1750 train_time:153894ms step_avg:258.64ms tokens_processed:29245440\n",
      "step:596/1750 train_time:154051ms step_avg:258.47ms tokens_processed:29294592\n",
      "step:597/1750 train_time:154209ms step_avg:258.31ms tokens_processed:29343744\n",
      "step:598/1750 train_time:154367ms step_avg:258.14ms tokens_processed:29392896\n",
      "step:599/1750 train_time:154524ms step_avg:257.97ms tokens_processed:29442048\n",
      "step:600/1750 train_time:154682ms step_avg:257.80ms tokens_processed:29491200\n",
      "step:601/1750 train_time:154840ms step_avg:257.64ms tokens_processed:29540352\n",
      "step:602/1750 train_time:154999ms step_avg:257.47ms tokens_processed:29589504\n",
      "step:603/1750 train_time:155154ms step_avg:257.30ms tokens_processed:29638656\n",
      "step:604/1750 train_time:155312ms step_avg:257.14ms tokens_processed:29687808\n",
      "step:605/1750 train_time:155468ms step_avg:256.97ms tokens_processed:29736960\n",
      "step:606/1750 train_time:155626ms step_avg:256.81ms tokens_processed:29786112\n",
      "step:607/1750 train_time:155785ms step_avg:256.65ms tokens_processed:29835264\n",
      "step:608/1750 train_time:155943ms step_avg:256.49ms tokens_processed:29884416\n",
      "step:609/1750 train_time:156101ms step_avg:256.32ms tokens_processed:29933568\n",
      "step:610/1750 train_time:156258ms step_avg:256.16ms tokens_processed:29982720\n",
      "step:611/1750 train_time:156414ms step_avg:256.00ms tokens_processed:30031872\n",
      "step:612/1750 train_time:156570ms step_avg:255.83ms tokens_processed:30081024\n",
      "step:613/1750 train_time:156728ms step_avg:255.67ms tokens_processed:30130176\n",
      "step:614/1750 train_time:156887ms step_avg:255.52ms tokens_processed:30179328\n",
      "step:615/1750 train_time:157046ms step_avg:255.36ms tokens_processed:30228480\n",
      "step:616/1750 train_time:157205ms step_avg:255.20ms tokens_processed:30277632\n",
      "step:617/1750 train_time:157361ms step_avg:255.04ms tokens_processed:30326784\n",
      "step:618/1750 train_time:157519ms step_avg:254.89ms tokens_processed:30375936\n",
      "step:619/1750 train_time:157676ms step_avg:254.73ms tokens_processed:30425088\n",
      "step:620/1750 train_time:157835ms step_avg:254.57ms tokens_processed:30474240\n",
      "step:621/1750 train_time:157993ms step_avg:254.42ms tokens_processed:30523392\n",
      "step:622/1750 train_time:158151ms step_avg:254.26ms tokens_processed:30572544\n",
      "step:623/1750 train_time:158309ms step_avg:254.11ms tokens_processed:30621696\n",
      "step:624/1750 train_time:158466ms step_avg:253.95ms tokens_processed:30670848\n",
      "step:625/1750 train_time:158624ms step_avg:253.80ms tokens_processed:30720000\n",
      "step:626/1750 train_time:158782ms step_avg:253.65ms tokens_processed:30769152\n",
      "step:627/1750 train_time:158941ms step_avg:253.49ms tokens_processed:30818304\n",
      "step:628/1750 train_time:159098ms step_avg:253.34ms tokens_processed:30867456\n",
      "step:629/1750 train_time:159255ms step_avg:253.19ms tokens_processed:30916608\n",
      "step:630/1750 train_time:159413ms step_avg:253.04ms tokens_processed:30965760\n",
      "step:631/1750 train_time:159570ms step_avg:252.88ms tokens_processed:31014912\n",
      "step:632/1750 train_time:159728ms step_avg:252.73ms tokens_processed:31064064\n",
      "step:633/1750 train_time:159887ms step_avg:252.59ms tokens_processed:31113216\n",
      "step:634/1750 train_time:160047ms step_avg:252.44ms tokens_processed:31162368\n",
      "step:635/1750 train_time:160206ms step_avg:252.29ms tokens_processed:31211520\n",
      "step:636/1750 train_time:160363ms step_avg:252.14ms tokens_processed:31260672\n",
      "step:637/1750 train_time:160520ms step_avg:251.99ms tokens_processed:31309824\n",
      "step:638/1750 train_time:160679ms step_avg:251.85ms tokens_processed:31358976\n",
      "step:639/1750 train_time:160837ms step_avg:251.70ms tokens_processed:31408128\n",
      "step:640/1750 train_time:160994ms step_avg:251.55ms tokens_processed:31457280\n",
      "step:641/1750 train_time:161152ms step_avg:251.41ms tokens_processed:31506432\n",
      "step:642/1750 train_time:161311ms step_avg:251.26ms tokens_processed:31555584\n",
      "step:643/1750 train_time:161469ms step_avg:251.12ms tokens_processed:31604736\n",
      "step:644/1750 train_time:161627ms step_avg:250.97ms tokens_processed:31653888\n",
      "step:645/1750 train_time:161786ms step_avg:250.83ms tokens_processed:31703040\n",
      "step:646/1750 train_time:161945ms step_avg:250.69ms tokens_processed:31752192\n",
      "step:647/1750 train_time:162103ms step_avg:250.54ms tokens_processed:31801344\n",
      "step:648/1750 train_time:162260ms step_avg:250.40ms tokens_processed:31850496\n",
      "step:649/1750 train_time:162416ms step_avg:250.26ms tokens_processed:31899648\n",
      "step:650/1750 train_time:177828ms step_avg:273.58ms tokens_processed:31948800\n",
      "step:651/1750 train_time:177985ms step_avg:273.40ms tokens_processed:31997952\n",
      "step:652/1750 train_time:178142ms step_avg:273.22ms tokens_processed:32047104\n",
      "step:653/1750 train_time:178301ms step_avg:273.05ms tokens_processed:32096256\n",
      "step:654/1750 train_time:178461ms step_avg:272.88ms tokens_processed:32145408\n",
      "step:655/1750 train_time:178622ms step_avg:272.71ms tokens_processed:32194560\n",
      "step:656/1750 train_time:178782ms step_avg:272.53ms tokens_processed:32243712\n",
      "step:657/1750 train_time:178939ms step_avg:272.36ms tokens_processed:32292864\n",
      "step:658/1750 train_time:179097ms step_avg:272.18ms tokens_processed:32342016\n",
      "step:659/1750 train_time:179253ms step_avg:272.01ms tokens_processed:32391168\n",
      "step:660/1750 train_time:179413ms step_avg:271.84ms tokens_processed:32440320\n",
      "step:661/1750 train_time:179574ms step_avg:271.67ms tokens_processed:32489472\n",
      "step:662/1750 train_time:179732ms step_avg:271.50ms tokens_processed:32538624\n",
      "step:663/1750 train_time:179891ms step_avg:271.33ms tokens_processed:32587776\n",
      "step:664/1750 train_time:180049ms step_avg:271.16ms tokens_processed:32636928\n",
      "step:665/1750 train_time:180208ms step_avg:270.99ms tokens_processed:32686080\n",
      "step:666/1750 train_time:180366ms step_avg:270.82ms tokens_processed:32735232\n",
      "step:667/1750 train_time:180525ms step_avg:270.65ms tokens_processed:32784384\n",
      "step:668/1750 train_time:180687ms step_avg:270.49ms tokens_processed:32833536\n",
      "step:669/1750 train_time:180844ms step_avg:270.32ms tokens_processed:32882688\n",
      "step:670/1750 train_time:181003ms step_avg:270.15ms tokens_processed:32931840\n",
      "step:671/1750 train_time:181160ms step_avg:269.99ms tokens_processed:32980992\n",
      "step:672/1750 train_time:181318ms step_avg:269.82ms tokens_processed:33030144\n",
      "step:673/1750 train_time:181476ms step_avg:269.65ms tokens_processed:33079296\n",
      "step:674/1750 train_time:181636ms step_avg:269.49ms tokens_processed:33128448\n",
      "step:675/1750 train_time:181798ms step_avg:269.33ms tokens_processed:33177600\n",
      "step:676/1750 train_time:181956ms step_avg:269.17ms tokens_processed:33226752\n",
      "step:677/1750 train_time:182115ms step_avg:269.00ms tokens_processed:33275904\n",
      "step:678/1750 train_time:182273ms step_avg:268.84ms tokens_processed:33325056\n",
      "step:679/1750 train_time:182431ms step_avg:268.68ms tokens_processed:33374208\n",
      "step:680/1750 train_time:182591ms step_avg:268.52ms tokens_processed:33423360\n",
      "step:681/1750 train_time:182750ms step_avg:268.36ms tokens_processed:33472512\n",
      "step:682/1750 train_time:182911ms step_avg:268.20ms tokens_processed:33521664\n",
      "step:683/1750 train_time:183070ms step_avg:268.04ms tokens_processed:33570816\n",
      "step:684/1750 train_time:183229ms step_avg:267.88ms tokens_processed:33619968\n",
      "step:685/1750 train_time:183388ms step_avg:267.72ms tokens_processed:33669120\n",
      "step:686/1750 train_time:183546ms step_avg:267.56ms tokens_processed:33718272\n",
      "step:687/1750 train_time:183706ms step_avg:267.40ms tokens_processed:33767424\n",
      "step:688/1750 train_time:183864ms step_avg:267.24ms tokens_processed:33816576\n",
      "step:689/1750 train_time:184022ms step_avg:267.09ms tokens_processed:33865728\n",
      "step:690/1750 train_time:184181ms step_avg:266.93ms tokens_processed:33914880\n",
      "step:691/1750 train_time:184338ms step_avg:266.77ms tokens_processed:33964032\n",
      "step:692/1750 train_time:184496ms step_avg:266.61ms tokens_processed:34013184\n",
      "step:693/1750 train_time:184654ms step_avg:266.46ms tokens_processed:34062336\n",
      "step:694/1750 train_time:184814ms step_avg:266.30ms tokens_processed:34111488\n",
      "step:695/1750 train_time:184972ms step_avg:266.15ms tokens_processed:34160640\n",
      "step:696/1750 train_time:185130ms step_avg:265.99ms tokens_processed:34209792\n",
      "step:697/1750 train_time:185289ms step_avg:265.84ms tokens_processed:34258944\n",
      "step:698/1750 train_time:185446ms step_avg:265.68ms tokens_processed:34308096\n",
      "step:699/1750 train_time:185605ms step_avg:265.53ms tokens_processed:34357248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0820 17:25:46.801000 9354 torch/_dynamo/convert_frame.py:1067] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "[rank0]:W0820 17:25:46.801000 9354 torch/_dynamo/convert_frame.py:1067] [0/8]    function: 'forward' (/tmp/ipykernel_9354/2160111820.py:33)\n",
      "[rank0]:W0820 17:25:46.801000 9354 torch/_dynamo/convert_frame.py:1067] [0/8]    last reason: 0/7: GLOBAL_STATE changed: grad_mode \n",
      "[rank0]:W0820 17:25:46.801000 9354 torch/_dynamo/convert_frame.py:1067] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W0820 17:25:46.801000 9354 torch/_dynamo/convert_frame.py:1067] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:700/1750 train_time:185763ms step_avg:265.38ms tokens_processed:34406400\n",
      "---validation---\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 49.12 GiB. GPU 0 has a total capacity of 79.11 GiB of which 42.59 GiB is free. Process 72665 has 36.51 GiB memory in use. Of the allocated memory 31.58 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(val_steps):\n\u001b[1;32m     23\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(val_loader)\n\u001b[0;32m---> 24\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_window_size_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m val_steps\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m val_loader\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:395\u001b[0m, in \u001b[0;36mOptimizedModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_has_any_global_hook():\n\u001b[1;32m    386\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:776\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 66\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, input_seq, target_seq, sliding_window_num_blocks)\u001b[0m\n\u001b[1;32m     63\u001b[0m         skip_connections\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[0;32m---> 66\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logits \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m7.5\u001b[39m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 49.12 GiB. GPU 0 has a total capacity of 79.11 GiB of which 42.59 GiB is free. Process 72665 has 36.51 GiB memory in use. Of the allocated memory 31.58 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        val_batch_size = world_size * args.val_seq_len\n",
    "\n",
    "        assert args.val_tokens % val_batch_size == 0\n",
    "        val_steps = args.val_tokens // val_batch_size\n",
    "        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, get_window_size_tokens(step))\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader)\n",
    "    if step == 0:\n",
    "        print(\"First inputs retrieved\")\n",
    "    tokens_processed += len(inputs) # maybe times world size?\n",
    "\n",
    "    model(inputs, targets, get_window_size_tokens(step)).backward()\n",
    "    # set optimization hyperparameters\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "    # step the optimizers\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    # null the gradients\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    # logging\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{len(inputs)}\",\n",
    "        console=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7da58-b098-4ad4-8c4c-667ab12e2ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0f86-3515-4b2c-b6e7-ab96b5f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e07454-e46b-4ec7-8b97-06a505fc8698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
