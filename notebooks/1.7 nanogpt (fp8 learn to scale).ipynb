{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f894d5-2b23-46f6-8622-90ed61ff381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vsrivastava/Documents/gpt2/.venv/bin/python: No module named pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip && pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275254b7-75b4-4f6c-b1db-ff78e2cf627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.9.0.dev20250713+cu126\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org\n",
      "Author: \n",
      "Author-email: PyTorch Team <packages@pytorch.org>\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvshmem-cu12, nvidia-nvtx-cu12, pytorch-triton, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd65b8-936d-4141-9651-403643d13021",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933abb-3b6b-4d70-b178-21a44ca35307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598ca10-f9da-4891-ae94-31c298b99fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../data/cached_fineweb10B.py 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a7be6-08d5-4711-87d3-ab7c35885b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2eecfb3-dcea-4c6a-b070-2f0b2b04356a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62975aa6-f3b7-489a-9131-ea69089074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f91470-f6e0-431d-a490-23c80f3c294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a879e98-a216-4626-a4ea-8db22d99df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d432bad-bbd3-4853-9a8d-2b11d8feda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0969746f-cc86-4121-a01e-19868c2a6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.attention.flex_attention import BlockMask, flex_attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c63379b-7f98-4b29-aa53-31af47bc0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ccb5-4d25-4918-aeb5-1b5a1f44a0fe",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1dd716-9d1b-4086-ae9c-3cf032ec04a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.compile ## ns\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f0f206-d925-4920-8f53-681265a44628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        # Efficient systems-wise implementation of step developed by @YouJiacheng,\n",
    "        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,\n",
    "        # @ryanyang0, and @vagrawal.\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                if base_i + rank < len(params):\n",
    "                    grad = params[base_i + rank].grad\n",
    "                # This gives strange dynamo warnings\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * world_size\n",
    "            momentum = group[\"momentum\"]\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                if base_i + rank < len(params):\n",
    "                    p = params[base_i + rank]\n",
    "                    grad = p.grad\n",
    "                    eff_lr = group[\"lr\"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, \"lr_mul\", 1.0)\n",
    "                    eff_weight_decay = group[\"lr\"] * group[\"weight_decay\"] * getattr(p, \"wd_mul\", 1.0)\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(grad)\n",
    "                    momentum_buffer = state[\"momentum_buffer\"]\n",
    "                    p.mul_(1 - eff_weight_decay)\n",
    "                    momentum_buffer.lerp_(grad, 1 - momentum)\n",
    "                    grad = grad.lerp_(momentum_buffer, momentum)\n",
    "                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)\n",
    "                    p.add_(other=v, alpha=-eff_lr)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbc0f81-97ec-4ed7-99df-78f34bada062",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DistAdam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        params = list(params)\n",
    "        sizes = {p.shape for p in params}\n",
    "        # create one buffer per unique parameter-size\n",
    "        param_groups = []\n",
    "        for size in sizes:\n",
    "            group_params = [p for p in params if p.shape == size]\n",
    "            param_groups.append(dict(params=group_params))\n",
    "        super().__init__(param_groups, defaults)\n",
    "        # DistributedAdam implementation by @vagrawal\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            grad = torch.empty_like(params[-1])\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78ffb1-753b-4a6f-a57f-81781873c084",
   "metadata": {},
   "source": [
    "## Custom matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66e2360d-f65a-451b-89c9-9020d252e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward: x:[M,K], w:[N,K]; scales are 0-D tensors\n",
    "@torch.library.custom_op(\"nanogpt::mm\", mutates_args=())\n",
    "def mm_op(x: Tensor, w: Tensor, x_s: Tensor, w_s: Tensor, grad_s: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(x: Tensor, w: Tensor, x_s: Tensor, w_s: Tensor):\n",
    "        assert x.is_contiguous() and w.is_contiguous()\n",
    "        # quantize to fp8 with per-tensor scales\n",
    "        x_f8 = (x / x_s).to(torch.float8_e4m3fn)\n",
    "        w_f8 = (w / w_s).to(torch.float8_e4m3fn)\n",
    "        # scales to fp32 (on the right device) before calling _scaled_mm\n",
    "        out = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=x_s.to(device=x.device, dtype=torch.float32),\n",
    "            scale_b=w_s.to(device=x.device, dtype=torch.float32),\n",
    "            use_fast_accum=True,\n",
    "        )\n",
    "        return out, x_f8, w_f8\n",
    "\n",
    "    return impl(x, w, x_s, w_s)\n",
    "\n",
    "@mm_op.register_fake\n",
    "def _(x: Tensor, w: Tensor, *_):\n",
    "    assert x.ndim == w.ndim == 2\n",
    "    assert x.shape[1] == w.shape[1]\n",
    "    assert x.device == w.device\n",
    "    assert x.is_contiguous() and w.is_contiguous()\n",
    "    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)\n",
    "\n",
    "@torch.library.custom_op(\"nanogpt::mm_backward\", mutates_args=())\n",
    "def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: Tensor, w_s: Tensor, grad_s: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    @torch.compile\n",
    "    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: Tensor, w_s: Tensor, grad_s: Tensor):\n",
    "        assert grad.is_contiguous()\n",
    "\n",
    "        xs_f32  = x_s.to(device=grad.device, dtype=torch.float32)\n",
    "        ws_f32  = w_s.to(device=grad.device, dtype=torch.float32)\n",
    "        gs_f32  = grad_s.to(device=grad.device, dtype=torch.float32)\n",
    "        one_f32 = torch.ones((), device=grad.device, dtype=torch.float32)\n",
    "\n",
    "        grad_f8 = (grad / gs_f32).to(torch.float8_e5m2)\n",
    "\n",
    "        # dL/dx (keep mat2 column-major via .T.contiguous().T)\n",
    "        grad_x = torch._scaled_mm(\n",
    "            grad_f8,\n",
    "            w_f8.T.contiguous().T,\n",
    "            out_dtype=torch.bfloat16,\n",
    "            scale_a=gs_f32,\n",
    "            scale_b=ws_f32,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "\n",
    "        # dL/dw\n",
    "        grad_w = torch._scaled_mm(\n",
    "            x_f8.T.contiguous(),\n",
    "            grad_f8.T.contiguous().T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=xs_f32,\n",
    "            scale_b=gs_f32,\n",
    "            use_fast_accum=False,\n",
    "        ).T\n",
    "\n",
    "        # === scale grads ===\n",
    "        # IMPORTANT: keep mat2 column-major here -> DO NOT call .contiguous()\n",
    "        xw_no_scale = torch._scaled_mm(\n",
    "            x_f8,\n",
    "            w_f8.T,\n",
    "            out_dtype=torch.float32,\n",
    "            scale_a=one_f32,\n",
    "            scale_b=one_f32,\n",
    "            use_fast_accum=False,\n",
    "        )\n",
    "        inner = (grad.to(torch.float32) * xw_no_scale).sum()\n",
    "\n",
    "        grad_x_s = (ws_f32 * inner).to(device=x_s.device, dtype=torch.float32)\n",
    "        grad_w_s = (xs_f32 * inner).to(device=w_s.device, dtype=torch.float32)\n",
    "\n",
    "        return grad_x, grad_w, grad_x_s, grad_w_s\n",
    "\n",
    "    return impl(g, x_f8, w_f8, x_s, w_s, grad_s)\n",
    "\n",
    "@mm_backward_op.register_fake\n",
    "def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):\n",
    "    # meta shapes/dtypes only\n",
    "    grad_x_meta = x_f8.to(torch.bfloat16)\n",
    "    grad_w_meta = w_f8.T.contiguous().T.to(torch.float32)\n",
    "    grad_xs_meta = g.new_empty((), dtype=torch.float32)  # 0-D\n",
    "    grad_ws_meta = g.new_empty((), dtype=torch.float32)  # 0-D\n",
    "    return grad_x_meta, grad_w_meta, grad_xs_meta, grad_ws_meta\n",
    "\n",
    "# Autograd glue\n",
    "def backward(ctx, grad_out: Tensor, *_):\n",
    "    x_f8, w_f8 = ctx.saved_tensors\n",
    "    x_s, w_s, grad_s = ctx.scales\n",
    "    grad_x, grad_w, grad_x_s, grad_w_s = torch.ops.nanogpt.mm_backward(\n",
    "        grad_out, x_f8, w_f8, x_s, w_s, grad_s\n",
    "    )\n",
    "    # no gradient for grad_s here\n",
    "    return grad_x, grad_w, grad_x_s, grad_w_s, None\n",
    "\n",
    "def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):\n",
    "    *_, x_s, w_s, grad_s = inputs\n",
    "    _, x_f8, w_f8 = output\n",
    "    ctx.save_for_backward(x_f8, w_f8)\n",
    "    ctx.scales = x_s, w_s, grad_s\n",
    "    ctx.set_materialize_grads(False)\n",
    "\n",
    "mm_op.register_autograd(backward, setup_context=setup_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c129e66-be79-4122-857d-fe205176d0c6",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ec017e-065d-4f43-b092-9163c3aaf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x: Tensor):\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43c37-4d2a-4fd6-bd58-63fcd1b5a54a",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5deb021d-7bc0-4dd5-a413-8c35e954419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP8Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, grad_s: float=1.):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        self.scales = nn.Parameter(torch.ones(2))\n",
    "        self.grad_s = grad_s # computing grad_s is more annoying -- double gradient\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.training:\n",
    "            _x = x.flatten(0, -2)\n",
    "            x_s, w_s = self.scales.unbind()  # 0-d tensors\n",
    "            grad_s_t = _x.new_tensor(self.grad_s) # 0-D tensor, no grad\n",
    "            out, _, _ = torch.ops.nanogpt.mm(_x, self.weight, x_s, w_s, grad_s_t)\n",
    "            return out.reshape(*x.shape[:-1], -1)\n",
    "        else:\n",
    "            return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4934dccb-7f32-442d-a7b8-4cc8d666d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP32Linear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        \n",
    "    def reset_parameters(self) -> None:\n",
    "        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)\n",
    "        bound = (3 ** 0.5) * std\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return F.linear(x, self.weight.type_as(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65922453-7b13-425f-a888-899f53d243ea",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c79cc0a-7a69-4d76-88a0-906ee65da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)\n",
    "        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim // 4, dtype=torch.float32)\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim // 4)])\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        self.cos = nn.Buffer(theta.cos(), persistent=False)\n",
    "        self.sin = nn.Buffer(theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: Tensor):\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        return torch.cat((y1, y2), 3).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a536ae-14b2-4b5c-b0ee-f2ca0ad7045a",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a21056-70ee-4b77-a2b8-8c33f427bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        hdim = num_heads * head_dim\n",
    "        std = 0.5 * (dim ** -0.5)\n",
    "        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng\n",
    "        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng\n",
    "        # https://x.com/hi_tysam/status/1879699187107033311\n",
    "        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))\n",
    "        self.rotary = Rotary(head_dim, max_seq_len)\n",
    "        self.c_proj = FP32Linear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun\n",
    "        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283\n",
    "        self.attn_scale = 0.12\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):\n",
    "        B, T = x.size(0), x.size(1)  # batch size, sequence length\n",
    "        assert B == 1, \"Must use batch size = 1 for FlexAttention\"\n",
    "        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads,\n",
    "                                                                             self.head_dim).chunk(3, dim=-2)\n",
    "        q, k = norm(q), norm(k)  # QK norm @Grad62304977\n",
    "        q, k = self.rotary(q), self.rotary(k)\n",
    "        if ve is not None:\n",
    "            v = lambdas[0] * v + lambdas[1] * ve.view_as(v)  # @KoszarskyB & @Grad62304977\n",
    "        else:  # skip mid-layers token value embeddings by @YouJiacheng\n",
    "            v = lambdas[0] * v\n",
    "        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask,\n",
    "                           scale=self.attn_scale).transpose(1, 2)\n",
    "        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6828-b90a-47c6-9759-73bbab68521c",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992d3a44-eca9-4645-b9b9-b678cd76fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        hdim = 4 * dim\n",
    "        self.c_fc = FP32Linear(dim, hdim)\n",
    "        self.c_proj = FP32Linear(hdim, dim)\n",
    "        self.c_proj.weight.detach().zero_()  # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(\n",
    "            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa25d-96b7-4026-904d-e9b7f6e7ce7a",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c18407-3a67-4e22-9f5f-7717fb3cffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):\n",
    "        super().__init__()\n",
    "        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng\n",
    "        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor,\n",
    "                block_mask: BlockMask):\n",
    "        x = lambdas[0] * x + lambdas[1] * x0\n",
    "        if self.attn is not None:\n",
    "            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84352c5e-79c7-4a81-a683-e6a3b1561e95",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e0bf666-e06a-4164-909b-4408c67b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, model_dim)\n",
    "\n",
    "        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])\n",
    "        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])\n",
    "\n",
    "        self.lm_head = FP32Linear(model_dim, vocab_size)#FP8Linear(model_dim, vocab_size, grad_s=1 / 448)\n",
    "        self.lm_head.weight.detach().zero_()  # @Grad62304977\n",
    "        # Add learnable skip connection weights for decoder layers\n",
    "        assert num_layers % 2 == 0\n",
    "        pad = (-num_layers * 5) % dist.get_world_size()\n",
    "        self.scalars = nn.Parameter(torch.cat([\n",
    "            torch.ones(num_layers),  # skip_weights\n",
    "            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)],  # block lambdas\n",
    "            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)],  # SA lambdas\n",
    "            torch.ones(pad),\n",
    "        ]))\n",
    "        # set learning rates\n",
    "        for param in self.embed.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        for param in self.value_embeds.parameters():\n",
    "            param.lr_mul = 75.\n",
    "        self.lm_head.weight.lr_mul = 27.5\n",
    "        self.scalars.lr_mul = 5.0\n",
    "\n",
    "    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):\n",
    "        BLOCK_SIZE = 128\n",
    "        docs = (input_seq == 50256).cumsum(0)\n",
    "\n",
    "        def document_causal(b, h, q_idx, kv_idx):\n",
    "            causal_mask = q_idx >= kv_idx\n",
    "            document_mask = docs[q_idx] == docs[kv_idx]\n",
    "            return causal_mask & document_mask\n",
    "\n",
    "        def dense_to_ordered(dense_blockmask: Tensor):\n",
    "            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)\n",
    "            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)\n",
    "            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()\n",
    "\n",
    "        # manual block mask creation by @YouJiacheng\n",
    "        assert len(input_seq) % BLOCK_SIZE == 0\n",
    "        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE\n",
    "        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device=\"cuda\")\n",
    "        causal_blockmask_any = block_idx[:, None] >= block_idx\n",
    "        causal_blockmask_all = block_idx[:, None] > block_idx\n",
    "        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()\n",
    "        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()\n",
    "        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)\n",
    "        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)\n",
    "        blockmask_any = causal_blockmask_any & document_blockmask_any\n",
    "        blockmask_all = causal_blockmask_all & document_blockmask_all\n",
    "        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)\n",
    "        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)\n",
    "\n",
    "        def build_bm(window_size_blocks: Tensor) -> BlockMask:\n",
    "            return BlockMask.from_kv_blocks(\n",
    "                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),\n",
    "                partial_kv_indices,\n",
    "                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),\n",
    "                full_kv_indices,\n",
    "                BLOCK_SIZE=BLOCK_SIZE,\n",
    "                mask_mod=document_causal,\n",
    "            )\n",
    "\n",
    "        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper\n",
    "        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)\n",
    "\n",
    "    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):\n",
    "        assert input_seq.ndim == 1\n",
    "\n",
    "        ve = [value_embed(input_seq) for value_embed in self.value_embeds]\n",
    "        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure\n",
    "        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]\n",
    "        assert len(ve) == len(self.blocks)\n",
    "\n",
    "        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)\n",
    "        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm,\n",
    "                       short_bm, long_bm]\n",
    "        assert len(block_masks) == len(self.blocks)\n",
    "\n",
    "        x = x0 = norm(self.embed(input_seq)[None])  # use of norm here by @Grad62304977\n",
    "\n",
    "        # U-net design by @brendanh0gan\n",
    "        skip_connections = []\n",
    "        skip_weights = self.scalars[:(len(self.blocks) // 2)]\n",
    "        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)\n",
    "        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)\n",
    "\n",
    "        n = len(self.blocks) // 2\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            if i >= n:\n",
    "                x = x + skip_weights[i - n] * skip_connections.pop()\n",
    "            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])\n",
    "            if i < n:\n",
    "                skip_connections.append(x)\n",
    "\n",
    "        x = norm(x)\n",
    "        logits = self.lm_head(x).float()\n",
    "        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)\n",
    "        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1) ** 0.5))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq,\n",
    "                               reduction=\"sum\" if self.training else \"mean\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18b470-51e6-466a-a90b-20606dcac5dc",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e719acf6-7de5-4e4f-97db-20e6397ead89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d9c76-d429-4926-bf62-703b99fa056c",
   "metadata": {},
   "source": [
    "Previous approach:\n",
    "- builds boolean mask over max_batch_span tokens; linear in max_batch_span\n",
    "- new approach: build EOS index per shard. Log search on number of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1f0904-4288-4559-b8ca-20f6084d5bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EOSBatchFinder:\n",
    "    def __init__(self, tokens: Tensor, eos_id: int = 50256):\n",
    "        # Precompute EOS positions once per shard \n",
    "        self.eos_idx = (tokens == eos_id).nonzero(as_tuple=True)[0].to(torch.int64).cpu()\n",
    "        self.i = 0      # pointer into eos_idx (start EOS for next step)\n",
    "        self.pos = 0    # logical stream position within this shard\n",
    "\n",
    "    def seek(self, pos: int):\n",
    "        # Set pointer to the first EOS >= pos\n",
    "        self.i = int(torch.searchsorted(self.eos_idx, pos))\n",
    "        if self.i >= int(self.eos_idx.numel()):\n",
    "            raise StopIteration(\"Seek past last EOS.\")\n",
    "        self.pos = pos\n",
    "\n",
    "    def next_batch(self, local_batch_size: int, world_size: int = 1):\n",
    "        n = int(self.eos_idx.numel())\n",
    "        if self.i >= n:\n",
    "            raise StopIteration(\"No more EOS in this shard.\")\n",
    "\n",
    "        starts = []\n",
    "        idx = self.i\n",
    "        cur = int(self.eos_idx[idx])  # absolute position (token index) of current start\n",
    "\n",
    "        # For each rank, find the first EOS >= cur + local_batch_size\n",
    "        for _ in range(world_size):\n",
    "            target = cur + local_batch_size\n",
    "            j = int(torch.searchsorted(self.eos_idx, target))\n",
    "            if j >= n:\n",
    "                raise StopIteration(\"Insufficient EOS ahead; hit tail of shard.\")\n",
    "            starts.append(cur)\n",
    "            idx = j\n",
    "            cur = int(self.eos_idx[idx])  # next segment starts at this end EOS\n",
    "\n",
    "        advance = int(self.eos_idx[idx] - self.pos)  # move stream to the last end\n",
    "        self.pos += advance\n",
    "        self.i = idx\n",
    "        return starts, advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f83a1c-a134-47ae-95e0-6386477a5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]\n",
    "    assert batch_size % world_size == 0\n",
    "    local_batch_size = batch_size // world_size\n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "\n",
    "    finder = EOSBatchFinder(tokens) if align_to_bos else None\n",
    "    if align_to_bos:\n",
    "        finder.seek(pos)\n",
    "    \n",
    "    max_batch_span = batch_size # only used when not align_to_bos\n",
    "    while True:\n",
    "        if pos + max_batch_span + 1 >= len(tokens):\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "            if align_to_bos:\n",
    "                finder = EOSBatchFinder(tokens)\n",
    "                finder.seek(pos)\n",
    "        if align_to_bos:\n",
    "            try:\n",
    "                batch_starts, batch_span = finder.next_batch(local_batch_size, world_size)\n",
    "                start_idx = batch_starts[rank]\n",
    "            except StopIteration:\n",
    "                # move to next shard on next loop\n",
    "                pos = len(tokens) + 1\n",
    "                continue\n",
    "        else:\n",
    "            batch_span = batch_size\n",
    "            start_idx = pos + rank * local_batch_size\n",
    "        buf = tokens[start_idx:][:local_batch_size + 1]\n",
    "        inputs = buf[:-1].to(device=\"cuda\", dtype=torch.int32, non_blocking=True) # no sync on host side;\n",
    "        targets = buf[1:].to(device=\"cuda\", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.\n",
    "        pos += batch_span\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6ff7-2376-4818-b0c6-63ea5c91a4c6",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8c4d61-8438-4512-a4cd-476b74b5111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_multiple_of_n(v: float | int, *, n: int):\n",
    "    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "031c79ec-83bf-4c7c-8bc1-d1f48488720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # data\n",
    "    train_files = \"../data/fineweb10B/fineweb_train_*.bin\"  # input .bin to train on\n",
    "    val_files = \"../data/fineweb10B/fineweb_val_*.bin\"  # input .bin to eval validation loss on\n",
    "    val_tokens = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n",
    "    train_seq_len = 48 * 1024  # FlexAttention sequence length\n",
    "    val_seq_len = 4 * 64 * 1024  # FlexAttention sequence length for validation\n",
    "    # optimization\n",
    "    num_iterations = 1750  # number of iterations to run\n",
    "    cooldown_frac = 0.45  # fraction of training spent cooling down the learning rate\n",
    "    # evaluation and logging\n",
    "    val_loss_every = 350  # every how many steps to evaluate val loss? 0 for only at the end\n",
    "    save_checkpoint = False\n",
    "\n",
    "    vocab_size = next_multiple_of_n(50257, n=128)\n",
    "    num_layers = 12\n",
    "    num_heads = 6\n",
    "    model_dim = 768\n",
    "\n",
    "\n",
    "args = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00850fb8-6c7a-47ae-8b3f-7f03570bc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_WORLD_SIZE'] = '1'\n",
    "os.environ['RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['LOCAL_RANK'] = '0'  # This would be 0-7 for each process\n",
    "os.environ['GROUP_RANK'] = '0'\n",
    "os.environ['ROLE_RANK'] = '0'\n",
    "os.environ['ROLE_NAME'] = 'default'\n",
    "os.environ['ROLE_WORLD_SIZE'] = '1'\n",
    "os.environ['TORCHELASTIC_RESTART_COUNT'] = '0'\n",
    "os.environ['TORCHELASTIC_MAX_RESTARTS'] = '0'\n",
    "os.environ['TORCHELASTIC_RUN_ID'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "711588cc-0891-40ae-867f-d6aad225b656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "master_process = (rank == 0)  # this process will do logging, checkpointing etc.\n",
    "# assert world_size == 8  # this code is designed for 8xH100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e41bdf92-3c1d-49f6-bf5b-1f40cf019fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\", int(os.environ[\"LOCAL_RANK\"]))\n",
    "torch.cuda.set_device(device)\n",
    "run = False  \n",
    "if not run:\n",
    "    dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "    dist.barrier()\n",
    "    run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea574fff-3d48-4806-bc64-6cf7a9968b33",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bad3bb5-c567-490d-a86d-d876aba3676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/d1398c8b-5195-43c1-a926-cda39506695a.txt\n"
     ]
    }
   ],
   "source": [
    "# begin logging\n",
    "logfile = None\n",
    "if master_process:\n",
    "    run_id = uuid.uuid4()\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logfile = f\"logs/{run_id}.txt\"\n",
    "    print(logfile)\n",
    "\n",
    "\n",
    "def print0(s, console=False):\n",
    "    if master_process:\n",
    "        with open(logfile, \"a\") as f:\n",
    "            if console:\n",
    "                print(s)\n",
    "            print(s, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bedf9028-88de-452c-a252-542dfe432812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]',\n",
       " '2.9.0.dev20250713+cu126')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version, torch.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37f8c6ac-472b-40b6-9cc5-eb634ecab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"Running Python {sys.version}\")\n",
    "print0(f\"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f66bc63-d530-4537-a09c-e79378abdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 22 19:34:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                  Off |\n",
      "| N/A   42C    P0              78W / 350W |   1103MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb0b9b-de7f-4052-a2fe-e6f9a69fc930",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "253fdb42-b56c-4a3b-bb0c-7ac5837d732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = GPT(vocab_size=args.vocab_size, \n",
    "                       num_layers=args.num_layers, \n",
    "                       num_heads=args.num_heads, \n",
    "                       model_dim=args.model_dim,\n",
    "                       max_seq_len=max(args.train_seq_len, args.val_seq_len)\n",
    "                      ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c4760a4-1858-499b-b961-530cd04f8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        m.bfloat16()\n",
    "for param in model.parameters():\n",
    "    dist.broadcast(param.detach(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a03b657a-b05f-4aa5-bf53-d07702385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the parameters to optimize\n",
    "hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "head_params = [model.lm_head.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12334b9f-193f-446b-a3e8-00a1479bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer(s)\n",
    "# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence\n",
    "# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094\n",
    "optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10,\n",
    "                      weight_decay=0.0)\n",
    "optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)\n",
    "optimizers = [optimizer1, optimizer2]\n",
    "for opt in optimizers:\n",
    "    for group in opt.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2672d18a-aad3-4706-822b-804d9ca90ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Module = torch.compile(model, dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8809242-8dca-4f72-bc28-736aa58fc03c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0e811f2-8daa-4992-8271-1ca7f9410fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule: stable then decay\n",
    "def get_lr(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x < 1\n",
    "    if x < 1 - args.cooldown_frac:\n",
    "        return 1.0\n",
    "    else:\n",
    "        w = (1 - x) / args.cooldown_frac\n",
    "        return w * 1.0 + (1 - w) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29457d3f-2d28-4ae0-87e4-7c06cde338c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention window size schedule: linearly increase\n",
    "@lru_cache(1)\n",
    "def get_window_size_blocks_helper(window_size: int):\n",
    "    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "def get_window_size_blocks(step: int):\n",
    "    x = step / args.num_iterations  # progress in training\n",
    "    assert 0 <= x <= 1\n",
    "    # Linearly increase the block-wise sliding window size over training 128 -> 1792\n",
    "    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng\n",
    "    window_size = next_multiple_of_n(1728 * x, n=128)\n",
    "    return get_window_size_blocks_helper(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e779f1-8889-4a9b-afcc-6a3172237e49",
   "metadata": {},
   "source": [
    "### Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5aed5ee-412b-496f-8305-4425b29cb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fc40a3d-7ad0-4a8c-a035-e14357268cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup the training kernels, then re-initialize the state so we aren't cheating\n",
    "warmup_steps = 100\n",
    "initial_state = dict(model=copy.deepcopy(model.state_dict()),\n",
    "                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state\n",
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1800c7f8-1b22-4bfd-81da-1eb9fec9a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4f52b848fa424494dedaccfdf23d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm(range(warmup_steps)):\n",
    "    inputs, targets = next(train_loader)\n",
    "    model(inputs, targets, get_window_size_blocks(1)).backward()\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d30dfc-260c-40e5-808a-b388c37541c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a71d51cc-f59c-4591-ae56-5a36fe2d912c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb0526a32c541d5b5e03d1c401994d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(initial_state[\"model\"])\n",
    "for opt, opt_state in zip(optimizers, initial_state[\"optimizers\"]):\n",
    "    opt.load_state_dict(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bf3eafa-77e2-452a-942c-c45c2c9aae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_loader, initial_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27d96a-9085-417b-b8e3-479220a3f2e1",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496e90d-9c4e-43ce-92f6-b49362e624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)\n",
    "train_steps = args.num_iterations\n",
    "training_time_ms = 0\n",
    "tokens_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30bd45-5c91-4e04-a04e-3fc4021306eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the clock\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "# begin training\n",
    "for step in range(train_steps + 1):\n",
    "    last_step = (step == train_steps)\n",
    "\n",
    "    # --------------- VALIDATION SECTION -----------------\n",
    "    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):\n",
    "        print('---validation---')\n",
    "        # stop the clock\n",
    "        torch.cuda.synchronize()\n",
    "        training_time_ms += 1000 * (time.perf_counter() - t0)\n",
    "        model.eval()\n",
    "        val_batch_size = world_size * args.val_seq_len\n",
    "\n",
    "        assert args.val_tokens % val_batch_size == 0\n",
    "        val_steps = args.val_tokens // val_batch_size\n",
    "        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(val_steps):\n",
    "                inputs, targets = next(val_loader)\n",
    "                val_loss += model(inputs, targets, get_window_size_blocks(step))\n",
    "        val_loss /= val_steps\n",
    "        del val_loader\n",
    "        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n",
    "        print0(\n",
    "            f\"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms\",\n",
    "            console=True)\n",
    "        model.train()\n",
    "        # start the clock again\n",
    "        print('---end of validation---')\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "    if last_step:\n",
    "        if master_process and args.save_checkpoint:\n",
    "            log = dict(step=step, model=model.state_dict(),\n",
    "                       optimizers=[opt.state_dict() for opt in optimizers])\n",
    "            os.makedirs(f\"logs/{run_id}\", exist_ok=True)\n",
    "            torch.save(log, f\"logs/{run_id}/state_step{step:06d}.pt\")\n",
    "        # the last step only has the validation loop, so break to avoid training\n",
    "        break\n",
    "\n",
    "    # --------------- TRAINING SECTION -----------------\n",
    "    inputs, targets = next(train_loader)\n",
    "    if step == 0:\n",
    "        print(\"First inputs retrieved\")\n",
    "    tokens_processed += len(inputs) # maybe times world size?\n",
    "\n",
    "    model(inputs, targets, get_window_size_blocks(step)).backward()\n",
    "    # set optimization hyperparameters\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * get_lr(step)\n",
    "\n",
    "    for group in optimizer2.param_groups:\n",
    "        frac = min(step / 300, 1)  # momentum warmup for muon\n",
    "        group[\"momentum\"] = (1 - frac) * 0.85 + frac * 0.95\n",
    "    # step the optimizers\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    # null the gradients\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    # logging\n",
    "    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)\n",
    "    print0(\n",
    "        f\"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms tokens_processed:{tokens_processed}\",\n",
    "        console=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0f86-3515-4b2c-b6e7-ab96b5f58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print0(f\"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB \"\n",
    "       f\"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB\", console=True)\n",
    "dist.destroy_process_group()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
